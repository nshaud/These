
@article{stewart_local_2012,
  title = {Local {{Climate Zones}} for {{Urban Temperature Studies}}},
  volume = {93},
  issn = {0003-0007},
  doi = {10.1175/BAMS-D-11-00019.1},
  abstract = {The effect of urban development on local thermal climate is widely documented in scientific literature. Observations of urban\textendash{}rural air temperature differences\textemdash{}or urban heat islands (UHIs)\textemdash{}have been reported for cities and regions worldwide, often with local field sites that are extremely diverse in their physical and climatological characteristics. These sites are usually described only as ``urban'' or ``rural,'' leaving much uncertainty about the actual exposure and land cover of the sites. To address the inadequacies of urban\textendash{}rural description, the ``local climate zone'' (LCZ) classification system has been developed. The LCZ system comprises 17 zone types at the local scale (102 to 104 m). Each type is unique in its combination of surface structure, cover, and human activity. Classification of sites into appropriate LCZs requires basic metadata and surface characterization. The zone definitions provide a standard framework for reporting and comparing field sites and their temperature observations. The LCZ system is designed primarily for urban heat island researchers, but it has derivative uses for city planners, landscape ecologists, and global climate change investigators.},
  number = {12},
  journal = {Bulletin of the American Meteorological Society},
  author = {Stewart, I. D. and Oke, T. R.},
  month = may,
  year = {2012},
  pages = {1879-1900},
  file = {/home/naudeber/Bibliographie//Bulletin of the American Meteorological Society/2012/Stewart Oke 2012 - Local Climate Zones for Urban Temperature Studies.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/AEEBUUNM/BAMS-D-11-00019.html}
}

@article{rottensteiner_isprs_2012,
  title = {The {{ISPRS}} Benchmark on Urban Object Classification and {{3D}} Building Reconstruction},
  volume = {1},
  journal = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  author = {Rottensteiner, Franz and Sohn, Gunho and Jung, Jaewook and Gerke, Markus and Baillard, Caroline and Benitez, Sebastien and Breitkopf, Uwe},
  year = {2012},
  pages = {3},
  file = {/home/naudeber/Bibliographie//ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci/2012/Rottensteiner et al 2012 - The ISPRS benchmark on urban object classification and 3D building.pdf}
}

@techreport{gerke_use_2015,
  title = {Use of the {{Stair Vision Library}} within the {{ISPRS 2D Semantic Labeling Benchmark}} ({{Vaihingen}})},
  institution = {{International Institute for Geo-Information Science and Earth Observation}},
  author = {Gerke, Markus},
  year = {2015},
  file = {/home/naudeber/Bibliographie//International Institute for Geo-Information Science and Earth Observation/2015/Gerke 2015 - Use of the Stair Vision Library within the ISPRS 2D Semantic Labeling Benchmark.pdf}
}

@article{everingham_pascal_2014,
  title = {The {{Pascal Visual Object Classes Challenge}}: {{A Retrospective}}},
  volume = {111},
  issn = {0920-5691, 1573-1405},
  shorttitle = {The {{Pascal Visual Object Classes Challenge}}},
  doi = {10.1007/s11263-014-0733-5},
  abstract = {The Pascal Visual Object Classes (VOC) challenge consists of two components: (i) a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii) an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008\textendash{}2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community's progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges.},
  language = {en},
  number = {1},
  journal = {International Journal of Computer Vision},
  author = {Everingham, Mark and Eslami, S. M. Ali and Gool, Luc Van and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
  month = jun,
  year = {2014},
  keywords = {Image Processing and Computer Vision,Computer Imaging; Vision; Pattern Recognition and Graphics,Artificial Intelligence (incl. Robotics),Pattern Recognition,segmentation,object detection,object recognition,Database,Benchmark},
  pages = {98-136},
  file = {/home/naudeber/Bibliographie//International Journal of Computer Vision/2014/Everingham et al 2014 - The Pascal Visual Object Classes Challenge.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/4CFZXJMT/10.html}
}

@article{razakarivony_vehicle_2016,
  title = {Vehicle {{Detection}} in {{Aerial Imagery}}: {{A}} Small Target Detection Benchmark},
  volume = {34},
  shorttitle = {Vehicle {{Detection}} in {{Aerial Imagery}}},
  doi = {10.1016/j.jvcir.2015.11.002},
  journal = {Journal of Visual Communication and Image Representation},
  author = {Razakarivony, S\'ebastien and Jurie, Fr\'ed\'eric},
  year = {2016},
  pages = {187--203},
  file = {/home/naudeber/Bibliographie//Journal of Visual Communication and Image Representation/2016/Razakarivony Jurie 2016 - Vehicle Detection in Aerial Imagery.pdf}
}

@incollection{lin_microsoft_2014,
  series = {Lecture Notes in Computer Science},
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  copyright = {\textcopyright{}2014 Springer International Publishing Switzerland},
  isbn = {978-3-319-10601-4 978-3-319-10602-1},
  shorttitle = {Microsoft {{COCO}}},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  language = {en},
  number = {8693},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  publisher = {{Springer International Publishing}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll\'ar, Piotr and Zitnick, C. Lawrence},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  month = sep,
  year = {2014},
  keywords = {Artificial Intelligence (incl. Robotics),Computer Graphics,Image Processing and Computer Vision,Pattern Recognition},
  pages = {740-755},
  file = {/home/naudeber/Bibliographie//Springer International Publishing/2014/Lin et al 2014 - Microsoft COCO.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/27T2J7FZ/978-3-319-10602-1_48.html},
  doi = {10.1007/978-3-319-10602-1_48}
}

@article{cramer_dgpf_2010,
  title = {The {{DGPF}} Test on Digital Aerial Camera Evaluation \textendash{} Overview and Test Design},
  volume = {2},
  journal = {Photogrammetrie \textendash{} Fernerkundung \textendash{} Geoinformation},
  author = {Cramer, M.},
  year = {2010},
  pages = {73--82}
}

@article{campos-taberner_processing_2016,
  title = {Processing of {{Extremely High}}-{{Resolution LiDAR}} and {{RGB Data}}: {{Outcome}} of the 2015 {{IEEE GRSS Data Fusion Contest Part A}}: 2-{{D Contest}}},
  volume = {9},
  issn = {1939-1404},
  shorttitle = {Processing of {{Extremely High}}-{{Resolution LiDAR}} and {{RGB Data}}},
  doi = {10.1109/JSTARS.2016.2569162},
  abstract = {In this paper, we discuss the scientific outcomes of the 2015 data fusion contest organized by the Image Analysis and Data Fusion Technical Committee (IADF TC) of the IEEE Geoscience and Remote Sensing Society (IEEE GRSS). As for previous years, the IADF TC organized a data fusion contest aiming at fostering new ideas and solutions for multisource studies. The 2015 edition of the contest proposed a multiresolution and multisensorial challenge involving extremely high-resolution RGB images and a three-dimensional (3-D) LiDAR point cloud. The competition was framed in two parallel tracks, considering 2-D and 3-D products, respectively. In this paper, we discuss the scientific results obtained by the winners of the 2-D contest, which studied either the complementarity of RGB and LiDAR with deep neural networks (winning team) or provided a comprehensive benchmarking evaluation of new classification strategies for extremely high-resolution multimodal data (runner-up team). The data and the previously undisclosed ground truth will remain available for the community and can be obtained at http://www.grss-ieee.org/community/technical-committees/data-fusion/2015-ieee-grss-data-fusion-contest/. The 3-D part of the contest is discussed in the Part-B paper [1].},
  number = {12},
  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  author = {{Campos-Taberner}, Manuel and {Romero-Soriano}, Adriana and Gatta, Carlo and {Camps-Valls}, Gustau and Lagrange, Adrien and Le Saux, Bertrand and Beaup\`ere, Anne and Boulch, Alexandre and {Chan-Hon-Tong}, Adrien and Herbin, St\'ephane and Randrianarivo, Hicham and Ferecatu, Marin and Shimoni, Michal and Moser, Gabriele and Tuia, Devis},
  month = dec,
  year = {2016},
  keywords = {deep neural networks,Laser radar,Data integration,Earth,Spatial resolution,Three-dimensional displays,LiDAR,extremely high spatial resolution,image analysis and data fusion (IADF),landcover classification,multimodal-data fusion,multiresolution-,multisource-,remote sensing},
  pages = {5547-5559},
  file = {/home/naudeber/Bibliographie//IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing/2016/Campos-Taberner et al 2016 - Processing of Extremely High-Resolution LiDAR and RGB Data.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FC7U9666/articleDetails.html}
}

@inproceedings{randrianarivo_urban_2013,
  title = {Urban Structure Detection with Deformable Part-Based Models},
  doi = {10.1109/IGARSS.2013.6721126},
  abstract = {In this paper we apply the deformable part model by Felzenszwalb et al., which is at this moment the state of the art in many computer vision related tasks, to detect different types of man made structures in very high resolution aerial images - a reputedly difficult problem in our field. We test the framework on a database of crops of aerial images at a definition of 10 cm/pixel and investigate how the model performs on several classes of objects. The results show that the model can achieve reasonable performance in this context. However, depending on the type of object, there are specific issues which will have to be taken into account to build an effective semi-supervised annotation tool based on this model.},
  booktitle = {2013 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}} - {{IGARSS}}},
  author = {Randrianarivo, Hicham and Le Saux, Bertrand and Ferecatu, Marin},
  month = jul,
  year = {2013},
  keywords = {object detection,support vector machines,geophysical image processing,object recognition,Buildings,computer vision,Image resolution,computer vision related tasks,deformable part-based models,urban structure detection,very high resolution aerial images,Computational modeling,Deformable models,image analysis,very high resolution,feature extraction,remote sensing},
  pages = {200-203},
  file = {/home/naudeber/Bibliographie//undefined/2013/Randrianarivo et al 2013 - Urban structure detection with deformable part-based models.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/QP3KRFWC/6721126.html}
}

@inproceedings{penatti_deep_2015,
  title = {Do Deep Features Generalize from Everyday Objects to Remote Sensing and Aerial Scenes Domains?},
  doi = {10.1109/CVPRW.2015.7301382},
  abstract = {In this paper, we evaluate the generalization power of deep features (ConvNets) in two new scenarios: aerial and remote sensing image classification. We evaluate experimentally ConvNets trained for recognizing everyday objects for the classification of aerial and remote sensing images. ConvNets obtained the best results for aerial images, while for remote sensing, they performed well but were outperformed by low-level color descriptors, such as BIC. We also present a correlation analysis, showing the potential for combining/fusing different ConvNets with other descriptors or even for combining multiple ConvNets. A preliminary set of experiments fusing ConvNets obtains state-of-the-art results for the well-known UCMerced dataset.},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Penatti, Ot\'avio and Nogueira, Keiller and {dos Santos}, Jefersson A.},
  month = jun,
  year = {2015},
  keywords = {Accuracy,aerial images,aerial scenes domains,BIC,Correlation,correlation analysis,correlation methods,deep features,feature extraction,generalization power,geophysical image processing,Histograms,image classification,Image color analysis,image colour analysis,low-level color descriptors,multiple ConvNets,object recognition,remote sensing,remote sensing image classification,UCMerced dataset,Visualization},
  pages = {44-51},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/AHDUQXD8/7301382.html}
}

@inproceedings{lagrange_benchmarking_2015,
  title = {Benchmarking Classification of Earth-Observation Data: {{From}} Learning Explicit Features to Convolutional Networks},
  shorttitle = {Benchmarking Classification of Earth-Observation Data},
  doi = {10.1109/IGARSS.2015.7326745},
  abstract = {In this paper, we address the task of semantic labeling of multisource earth-observation (EO) data. Precisely, we benchmark several concurrent methods of the last 15 years, from expert classifiers, spectral support-vector classification and high-level features to deep neural networks. We establish that (1) combining multisensor features is essential for retrieving some specific classes, (2) in the image domain, deep convolutional networks obtain significantly better overall performances and (3) transfer of learning from large generic-purpose image sets is highly effective to build EO data classifiers.},
  booktitle = {2015 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}} ({{IGARSS}})},
  author = {Lagrange, Adrien and Le Saux, Bertrand and Beaup\`ere, Anne and Boulch, Alexandre and {Chan-Hon-Tong}, Adrien and Herbin, St\'ephane and Randrianarivo, Hicham and Ferecatu, Marin},
  month = jul,
  year = {2015},
  keywords = {neural nets,support vector machines,geophysical image processing,terrain mapping,deep convolutional networks,deep neural networks,expert classifiers,generic-purpose image sets,high-level features,image domain,learning explicit features,multisensor features,multisource earth-observation data benchmarking classification,semantic labeling,spectral support-vector classification,Buildings,Laser radar,Neural networks,Pattern analysis,Semantics,feature extraction,image classification,remote sensing},
  pages = {4173-4176},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WXE5EVSM/7326745.html}
}

@inproceedings{maggiori_can_2017,
  title = {Can {{Semantic Labeling Methods Generalize}} to {{Any City}}? {{The Inria Aerial Image Labeling Benchmark}}},
  shorttitle = {Can {{Semantic Labeling Methods Generalize}} to {{Any City}}?},
  doi = {10.1109/IGARSS.2017.8127684},
  abstract = {New challenges in remote sensing impose the necessity of designing pixel classification methods that, once trained on a certain dataset, generalize to other areas of the earth. This may include regions where the appearance of the same type of objects is significantly different. In the literature it is common to use a single image and split it into training and test sets to train a classifier and assess its performance, respectively. However, this does not prove the generalization capabilities to other inputs. In this paper, we propose an aerial image labeling dataset that covers a wide range of urban settlement appearances, from different geographic locations. Moreover, the cities included in the test set are different from those of the training set. We also experiment with convolutional neural networks on our dataset.},
  language = {en},
  booktitle = {Proceedings of the {{IEEE International Symposium}} on {{Geoscience}} and {{Remote Sensing}} ({{IGARSS}})},
  author = {Maggiori, Emmanuel and Tarabalka, Yuliya and Charpiat, Guillaume and Alliez, Pierre},
  month = jul,
  year = {2017},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/UV49DI3R/Maggiori et al_2017_Can Semantic Labeling Methods Generalize to Any City.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/K7CHRHV4/hal-01468452.html}
}

@article{brostow_semantic_2009,
  series = {Video-based Object and Event Analysis},
  title = {Semantic Object Classes in Video: {{A}} High-Definition Ground Truth Database},
  volume = {30},
  issn = {0167-8655},
  shorttitle = {Semantic Object Classes in Video},
  doi = {10.1016/j.patrec.2008.04.005},
  number = {2},
  journal = {Pattern Recognition Letters},
  author = {Brostow, Gabriel J. and Fauqueur, Julien and Cipolla, Roberto},
  month = jan,
  year = {2009},
  keywords = {object recognition,semantic segmentation,Video database,Video understanding,Label propagation},
  pages = {88-97},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/I8FJM7EM/Brostow et al_2009_Semantic object classes in video.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FTC9XV9V/S0167865508001220.html}
}

@inproceedings{song_sun_2015,
  title = {{{SUN RGB}}-{{D}}: {{A RGB}}-{{D}} Scene Understanding Benchmark Suite},
  shorttitle = {{{SUN RGB}}-{{D}}},
  doi = {10.1109/CVPR.2015.7298655},
  abstract = {Although RGB-D sensors have enabled major break-throughs for several vision tasks, such as 3D reconstruction, we have not attained the same level of success in high-level scene understanding. Perhaps one of the main reasons is the lack of a large-scale benchmark with 3D annotations and 3D evaluation metrics. In this paper, we introduce an RGB-D benchmark suite for the goal of advancing the state-of-the-arts in all major scene understanding tasks. Our dataset is captured by four different sensors and contains 10,335 RGB-D images, at a similar scale as PASCAL VOC. The whole dataset is densely annotated and includes 146,617 2D polygons and 64,595 3D bounding boxes with accurate object orientations, as well as a 3D room layout and scene category for each image. This dataset enables us to train data-hungry algorithms for scene-understanding tasks, evaluate them using meaningful 3D metrics, avoid overfitting to a small testing set, and study cross-sensor bias.},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Song, S. and Lichtenberg, S. P. and Xiao, J.},
  month = jun,
  year = {2015},
  keywords = {Pascal VOC,image colour analysis,Three-dimensional displays,Estimation,Benchmark testing,image sensors,2D polygons,3D annotations,3D bounding boxes,3D evaluation metrics,3D reconstruction,3D room layout,RGB-D images,RGB-D scene understanding benchmark suite,RGB-D sensors,SUN RGB-D,cross-sensor bias,data-hungry algorithms,high-level scene understanding,object orientations,scene category,vision tasks,Cameras,Iterative closest point algorithm,Layout,Sensors},
  pages = {567-576},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WSU485C5/7298655.html}
}

@inproceedings{deng_imagenet_2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  doi = {10.1109/CVPR.2009.5206848},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  month = jun,
  year = {2009},
  keywords = {computer vision,Explosions,Image databases,Image resolution,image retrieval,ImageNet database,Information retrieval,Internet,large-scale hierarchical image database,large-scale ontology,Large-scale systems,multimedia computing,multimedia data,Multimedia databases,Ontologies,ontologies (artificial intelligence),Robustness,Spine,subtree,trees (mathematics),very large databases,visual databases,wordNet structure},
  pages = {248-255},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FFXN3NBD/5206848.html}
}

@article{le_saux_2018_2018,
  title = {2018 {{IEEE GRSS Data Fusion Contest}}: {{Multimodal Land Use Classification}} [{{Technical Committees}}]},
  volume = {6},
  issn = {2473-2397},
  shorttitle = {2018 {{IEEE GRSS Data Fusion Contest}}},
  doi = {10.1109/MGRS.2018.2798161},
  abstract = {Presents information on the 2018 IEEE GRSS Data Fusion Contest.},
  number = {1},
  journal = {IEEE Geoscience and Remote Sensing Magazine},
  author = {Le Saux, Bertrand and Yokoya, Naoto and Hansch, Ronny and Prasad, Saurabh},
  month = mar,
  year = {2018},
  pages = {52-54},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/UTBE7ZSW/cookiedetectresponse.html}
}

@article{tuia_2017_2017,
  title = {The 2017 {{IEEE Geoscience}} and {{Remote Sensing Society Data Fusion Contest}}: {{Open Data}} for {{Global Multimodal Land Use Classification}} [{{Technical Committees}}]},
  volume = {5},
  issn = {2473-2397},
  shorttitle = {The 2017 {{IEEE Geoscience}} and {{Remote Sensing Society Data Fusion Contest}}},
  doi = {10.1109/MGRS.2017.2760346},
  abstract = {Presents information on the 2017 IEEE Geoscience and Remote Sensing Society Data Fusion Contest.},
  number = {4},
  journal = {IEEE Geoscience and Remote Sensing Magazine},
  author = {Tuia, Devis and Moser, Gabriele and Le Saux, Bertrand and Bechtel, Benjamin and See, Linda},
  month = dec,
  year = {2017},
  pages = {110-114},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/S5KYRQTF/cookiedetectresponse.html}
}

@inproceedings{basu_deepsat_2015,
  address = {New York, NY, USA},
  series = {SIGSPATIAL '15},
  title = {{{DeepSat}}: {{A Learning Framework}} for {{Satellite Imagery}}},
  isbn = {978-1-4503-3967-4},
  shorttitle = {{{DeepSat}}},
  doi = {10.1145/2820783.2820816},
  abstract = {Satellite image classification is a challenging problem that lies at the crossroads of remote sensing, computer vision, and machine learning. Due to the high variability inherent in satellite data, most of the current object classification approaches are not suitable for handling satellite datasets. The progress of satellite image analytics has also been inhibited by the lack of a single labeled high-resolution dataset with multiple class labels. The contributions of this paper are twofold -- (1) first, we present two new satellite datasets called SAT-4 and SAT-6, and (2) then, we propose a classification framework that extracts features from an input image, normalizes them and feeds the normalized feature vectors to a Deep Belief Network for classification. On the SAT-4 dataset, our best network produces a classification accuracy of 97.95\% and outperforms three state-of-the-art object recognition algorithms, namely - Deep Belief Networks, Convolutional Neural Networks and Stacked Denoising Autoencoders by \textasciitilde{}11\%. On SAT-6, it produces a classification accuracy of 93.9\% and outperforms the other algorithms by \textasciitilde{}15\%. Comparative studies with a Random Forest classifier show the advantage of an unsupervised learning approach over traditional supervised learning techniques. A statistical analysis based on Distribution Separability Criterion and Intrinsic Dimensionality Estimation substantiates the effectiveness of our approach in learning better representations for satellite imagery.},
  booktitle = {Proceedings of the 23rd {{SIGSPATIAL International Conference}} on {{Advances}} in {{Geographic Information Systems}}},
  publisher = {{ACM}},
  author = {Basu, Saikat and Ganguly, Sangram and Mukhopadhyay, Supratik and DiBiano, Robert and Karki, Manohar and Nemani, Ramakrishna},
  year = {2015},
  keywords = {deep learning,high resolution,satellite imagery},
  pages = {37:1--37:10},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/GY9QWZDV/Basu et al. - 2015 - DeepSat A Learning Framework for Satellite Imager.pdf}
}

@inproceedings{yang_bag--visual-words_2010,
  address = {New York, NY, USA},
  series = {GIS '10},
  title = {Bag-of-Visual-Words and {{Spatial Extensions}} for {{Land}}-Use {{Classification}}},
  isbn = {978-1-4503-0428-3},
  doi = {10.1145/1869790.1869829},
  abstract = {We investigate bag-of-visual-words (BOVW) approaches to land-use classification in high-resolution overhead imagery. We consider a standard non-spatial representation in which the frequencies but not the locations of quantized image features are used to discriminate between classes analogous to how words are used for text document classification without regard to their order of occurrence. We also consider two spatial extensions, the established spatial pyramid match kernel which considers the absolute spatial arrangement of the image features, as well as a novel method which we term the spatial co-occurrence kernel that considers the relative arrangement. These extensions are motivated by the importance of spatial structure in geographic data. The methods are evaluated using a large ground truth image dataset of 21 land-use classes. In addition to comparisons with standard approaches, we perform extensive evaluation of different configurations such as the size of the visual dictionaries used to derive the BOVW representations and the scale at which the spatial relationships are considered. We show that even though BOVW approaches do not necessarily perform better than the best standard approaches overall, they represent a robust alternative that is more effective for certain land-use classes. We also show that extending the BOVW approach with our proposed spatial co-occurrence kernel consistently improves performance.},
  booktitle = {Proceedings of the 18th {{SIGSPATIAL International Conference}} on {{Advances}} in {{Geographic Information Systems}}},
  publisher = {{ACM}},
  author = {Yang, Yi and Newsam, Shawn},
  year = {2010},
  keywords = {land-use classification,bag-of-visual-words,local invariant features},
  pages = {270--279},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/GFUS52MJ/Yang et Newsam - 2010 - Bag-of-visual-words and Spatial Extensions for Lan.pdf}
}

@inproceedings{zhou_scene_2017,
  address = {Honolulu, United States},
  title = {Scene {{Parsing}} through {{ADE20K Dataset}}},
  doi = {10.1109/CVPR.2017.544},
  abstract = {Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the communitys efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the ADE20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A scene parsing benchmark is built upon the ADE20K with 150 object and stuff classes included. Several segmentation baseline models are evaluated on the benchmark. A novel network design called Cascade Segmentation Module is proposed to parse a scene into stuff, objects, and object parts in a cascade and improve over the baselines. We further show that the trained scene parsing networks can lead to applications such as image content removal and scene synthesis1.},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
  month = jul,
  year = {2017},
  keywords = {ADE20K dataset,computer vision,Computer vision,dense annotations,detailed annotations,image datasets,image representation,image segmentation,Image segmentation,Labeling,learning (artificial intelligence),Neural networks,object parts,object recognition,object segmentation,scene parsing benchmark,Semantics,Sun,trained scene parsing networks,Visualization},
  pages = {5122-5130},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/LIFS3QTA/8100027.html}
}


