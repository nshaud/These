%!TEX root = Manuscrit.tex
\chapter{Extension aux capteurs non-conventionnels}
\label{chap:extension}
%\citationChap{I suppose it is tempting, if the only tool you have is a hammer, to treat everything as if it were a nail.}{Abraham Maslow}
\minitoc

\chapsummary{%
\lettrine{L}{es} images multispectrales sont couramment utilisées en télédétection, mais leur nombre de bandes élevé complique l'application directe de \glssymbol{FCN} pré-entraînés sur des données \gls{RVB}. Dans ce chapitre, nous montrons qu'il est en pratique tout de même possible d'étendre les résultats obtenus en imagerie couleur pour le multispectral. Si l'infrarouge n'apporte pas d'information supplémentaire en imagerie aérienne, nous adaptons des \glssymbol{FCN} classiques afin de générer des cartes sémantiques fines à partir d'images multispectrales Sentinel-2, pour lesquelles la prise en compte des bandes non-visibles est largement bénéfique.

Nous nous intéressons ensuite au cas particulier de l'imagerie hyperspectrale, pour laquelle la résolution spectrale élevée nécessite des précautions additionnelles. Nous réalisons un comparatif des méthodes de l'état de l'art en apprentissage profond pour l'imagerie hyperspectrale et montrons la pertinence des modèles convolutifs à trois dimensions, en dépit du faible nombre d'exemples.

Enfin, nous étudions les possibilités de traitement des modèles numériques de terrain à partir de réseaux convolutifs. Bien qu'il ne s'agisse pas de données optiques, les modèles de terrain contiennent une information de hauteur particulièrement discriminante pour la végétation et les objets artificiels. Nous montrons que les \glssymbol{FCN} en niveaux de gris permettent d'obtenir des cartographies à partir de ces modèles, mais moins précises qu'en utilisant les images couleur, justifiant ainsi le besoin d'approches multi-modales.
}

\newpage

\section{Images multispectrales}

Dans le chapitre précédent, nous avons montré que les \glspl{FCN} pouvaient être mis en \oe{}uvre pour la segmentation sémantique d'images aériennes à trois canaux, \gls{RVB} et \gls{IRRV}. Toutefois, la majorité des satellites d'observation de la Terre, qu'ils soient institutionnels comme \gls{Landsat} et \gls{SPOT}, ou commerciaux comme \emph{WorldView}, embarquent des capteurs \glslink{multispectral}{multispectraux}. Ces instruments permettent d'extraire de l'information invisible à l'\oe{}il humain qu'il est souhaitable de pouvoir exploiter pour la cartographie automatisée.

\subsection{Prise en compte du proche infrarouge}

Les images multispectrales les plus simples sont des images à trois ou quatre canaux comprenant un canal infrarouge. En l'occurrence, les jeux de données \glssymbol{ISPRS} Vaihingen et Potsdam comportent de telles données multispectrales, respectivement \gls{IRRV} et \gls{IRRVB}.

Nous avons montré dans le~\cref{chap:cartographie} qu'il était possible et de traiter les images \gls{IRRV} de la même façon que les images \gls{RVB} traditionnelles. En particulier, cela nous a permis de transférer les poids de réseaux de pré-entraînés sur ImageNet à des images de télédétection \gls{IRRV}. Toutefois, dans le cas des images multispectrales \gls{IRRVB}, il est impossible de réaliser ce transfert en conservant les quatre canaux. Notre approche précédente a consisté à utiliser seulement les trois canaux \gls{RVB}, en omettant le canal infrarouge.

\begin{figure}[h]
  \includegraphics[width=\textwidth]{histogrammes_potsdam}
  \caption{Distributions des intensités pour les canaux rouge, vert, bleu et infrarouge du jeu de données \glssymbol{ISPRS} Potsdam.}
  \label{fig:potsdam_histograms}
\end{figure}

Dans un premier temps, nous pouvons examiner les distributions des intensités pour chaque canal au sein du jeu de données. Les histogrammes sont présentés dans la~\cref{fig:potsdam_histograms}. Les distributions correspondent à une loi gamma dont les paramètres sont très similaires pour les canaux \gls{RVB}. Les statistiques du canal infrarouge diffèrent cependant de celles des canaux visibles. Ceci est d'autant plus visible dans les cartes de corrélation inter-canaux de la~\cref{fig:potsdam_correlations}. Les canaux visibles sont fortement corrélés (c\oe{}fficient de Pearson supérieur à 0,87). À l'inverse, le canal infrarouge n'est que modérément corrélé avec les autres canaux, notamment lorsque l'écart en longueur d'onde s'accroît. Ainsi, le c\oe{}ifficient de Pearson entre les distributions rouge et infrarouge est de 0,80, mais tombe à 0,69 entre vert et infrarouge et 0,57 entre bleu et infrarouge.

\begin{figure}[h]
  \includegraphics[width=0.33\textwidth]{hexbin_potsdam_rg}
  \includegraphics[width=0.33\textwidth]{hexbin_potsdam_rb}
  \includegraphics[width=0.33\textwidth]{hexbin_potsdam_gb}\\
  \includegraphics[width=0.33\textwidth]{hexbin_potsdam_rir}
  \includegraphics[width=0.33\textwidth]{hexbin_potsdam_gir}
  \includegraphics[width=0.33\textwidth]{hexbin_potsdam_bir}
  \caption{Cartes de corrélation entre canaux du jeu de données \glssymbol{ISPRS} Potsdam.}
  \label{fig:potsdam_correlations}
\end{figure}


Nous réalisons une étude préliminaire sur le jeu de données \glssymbol{ISPRS} Potsdam en reprenant les modèles et hyperparamètres obtenus dans le~\cref{chap:cartographie}. En particulier, nous étudions différentes variantes de SegNet, avec et sans initialisation de l'encodeur à partir des poids de VGG-16 pré-entraîné sur ImageNet, sur différentes combinaisons de canaux. Les résultats sont détaillés dans le~\cref{tab:comparaison_bandes}. Les tuiles 2\_11, 7\_10, 4\_10, 7\_9, 7\_12, 6\_8, 6\_7, 6\_9, 5\_11, 2\_10, 2\_12, 3\_10, 6\_10, 6\_11, 3\_12, 3\_11, 5\_12 et 4\_12 sont utilisées pour l'entraînement tandis que la validation s'effectue sur les tuiles 4\_11, 6\_12, 7\_11, 5\_10, 7\_8 et 7\_7.

\begin{table}
  \setlength\tabcolsep{3pt}
  \caption{Comparaison des résultats obtenus par SegNet sur Potsdam à partir de plusieurs combinaisons de canaux.}
  \label{tab:comparaison_bandes}
  \begin{tabularx}{\textwidth}{Y Y c c c c c c c}
    \toprule
    Canaux & Pré-entraînement & Routes & Bâtiments & Vég. basse & Arbres & Véhicules & Autres & Exactitude\\
    \midrule
    IR+B & n & 72.79 & 87.22 & 57.61 & 71.74 & 87.05 & 13.15 & 70.69\\
    R+G & n & 89.92 & 95.80 & 81.49 & 84.30 & 95.21 & 40.42 & 88.48\\
    IR+R & n & 90.75 & 95.89 & 82.77 & 84.97 & 95.50 & 42.47 & 89.25\\
    \glsname{IRRV} & n & 90.83 & 95.91 & 83.31 & 84.26 & 94.99 & 43.74 & 89.29\\
    \glsname{RVB} & n & 90.40 & 96.32 & 82.38 & 83.78 & 95.42 & 39.97 & 89.02\\
    \glsname{IRRVB} & n & 89.67 & 95.57 & 82.35 & 83.82 & 95.17 & 42.89 & 88.51\\
    \glsname{RVB} & o & 92.35 & \textbf{97.62} & 85.18 & 87.19 & \textbf{96.11} & \textbf{52.15} & 91.22\\
    \glsname{IRRV} & o & \textbf{92.51} & 97.34 & \textbf{85.68} & \textbf{87.54} & \textbf{96.11} & 50.24 & \textbf{91.28}\\
    \bottomrule
  \end{tabularx}
\end{table}

Ces expériences préliminaires montrent que les résultats obtenus à l'aide des trois canaux \gls{RVB} et \gls{IRRV} sont très similaires sur Potsdam. Cependant, les modèles n'utilisant que trois canaux sont légèrement plus exacts que les modèles à quatre canaux \gls{IRRVB}, même lorsqu'ils sont initialisés aléatoirement, c'est-à-dire sans pré-entraînement. En particulier, les résultats à partir des combinaisons de 2 canaux semblent indiquer que l'interaction entre les canaux infrarouge et bleu entraîne une baisse conséquente des performances du modèle à cause d'une confusion considérable entre la classe "autres" et les autres classes. Cette chute des performances apparaît y compris lorsque le réseau n'apprend pas sur les pixels appartenant à la classe "autres", c'est-à-dire que l'interaction des canaux infrarouge et bleu introduit bien une confusion que SegNet ne parvient pas à contourner. Il semble y avoir un lien entre la cohérence radiométrique des canaux utilisés en entrée du réseau et les performances finales de classification. Ce lien est toutefois encore hypothétique et n'est pas trivial à mettre en évidence.

\subsection{Images multispectrales}
\label{sec:multispectral}

La plupart des capteurs optiques embarqués dans des satellites d'observation de la Terre réalisent des acquisitions multispectrales. En effet, l'information intéressante ne se situe pas toujours dans le domaine visible. La réponse lumineuse de la chlorophylle dans l'infrarouge proche est par exemple un indicateur caractéristique de la végétation. La présence de longueurs d'onde d'acquisitions supplémentaires permet souvent d'identifier des matériaux spécifiques indétectables autrement. Ainsi, l'instrumentation de \gls{Sentinel}-2 permet de détecter les aérosols côtiers (bande 1 à \SI{443}{\nano\meter}), le \emph{red edge} de la chlorophylle (bandes 5 à 7 entre \SI{705}{\nano\meter} et \SI{783}{\nano\meter}) ainsi que la vapeur d'eau (bande 9 à \SI{945}{\nano\meter}) et les cirrus (bande 10 à \SI{1,375}{\micro\meter}). Ces bandes d'acquisition viennent en complément du domaine visible. Dans ce cas, les bandes ne sont pas nécessairement toutes de même largeur.

Cette section s'intéresse donc à l'utilisation des \glspl{FCN} pour la segmentation sémantique d'images satellites multispectrales. En particulier, nous bénéficions d'un jeu de données d'images Sentinel-2 accompagnées de la carte d'occupation du sol \emph{GlobeCover} 2009~\cite{arino_global_2012}. Les images Sentinel-2 considérées couvrent une région se situant à la frontière de la France, de la Suisse et de l'Italie. Les images ont été acquises entre mai et octobre 2016. Nous faisons l'hypothèse que les changements d'occupation des sols durant les 7 années séparant la création de la carte et l'acquisition des données sont relativement peu nombreux.

Un premier jeu de données est composé des images ne contenant aucune couverture nuageuse (nuages opaques ou cirrus). Un second jeu de données est constitué uniquement sur la période d'été mais inclut également les tuiles comprenant une couverture nuageuse, dont la détection est désormais prise en compte dans la tâche de classification. Les deux jeux de données sont ainsi constituées d'acquisitions sur les mêmes zones à plusieurs dates. Les images Sentinel-2 sont interpolées à une résolution de \SI{20}{\meter/\px} pour toutes les bandes tandis que les annotations issues de\emph{GlobeCover} sont à une résolution de \SI{300}{\meter/\px}. Les détails des jeux de données sont indiqués dans le~\cref{tab:datasets} et les classes considérées sont listées dans le~\cref{tab:globecover_legende}.

\begin{table}[h]
\caption{Deux jeux de données d'images Sentinel-2 sont considérés. Le premier s'étend sur une longue période mais exclut les images contenant une couverture nuageuse. Le second est restreint à une période temporelle plus courte mais inclut les images en présence de nuages. Les deux jeux de données contiennent environ 150 millions de pixels chacun.}
\label{tab:datasets}
\begin{tabular}{lccc}
\toprule
Jeux de données & \multicolumn{3}{c}{nombre d'images}\\
(période)  & entraînement & validation  & nombre de classes \\
\midrule
D1, période longue sans nuages & & & \\
 (mai--oct. 2016) & 140 & 54 & 16 \\
\midrule
D2, période courte avec nuages & & & \\
(juin--août 2016) & 158 & 39 & 17 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
  \caption[Liste des classes des jeux de données D1 et D2 dérivés de \emph{GlobeCover} 2009.]{Liste des classes des jeux de données D1 et D2 dérivés de \emph{GlobeCover} 2009.\\{\small * La classe nuage est ajoutée \emph{a posteriori} à partir du masque fourni avec les données Sentinel-2 par Copernicus.}}
  \label{tab:globecover_legende}
  \begin{tabularx}{\textwidth}{cY}
    \toprule
    Valeur & Légende \emph{GlobeCover} 2009\\
    \midrule
    1 & Cultures non irriguées\\ % Rainfeld croplands
    2 & Mosaïque culture (50-70\%)/végétation (pelouse, fruticée, forêt) (20-50\%)\\ % Mosaic Cropland (50-70%) / Vegetation (grassland, shrubland, forest) (20-50%)
    3 & Mosaïque végétation (pelouse, fruticée, forêt) (50-70\%)/cultures (20-50\%)\\ % Mosaic Vegetation (grassland, shrubland, forest) (50-70%) / Cropland (20-50%)
    4 & Forêt décidue à feuilles larges dense (>40\%, >5m)\\ %Closed (>40%) broadleaved deciduous forest (>5m)
    5 & Forêt épineuse sempervirent dense (>40\%, >5m)\\ %Closed (>40%) needleleaved evergreen forest (>5m)
    6 & Forêt peu dense (15-40\%) décidue ou sempervirent (>5m)\\%Open (15-40%) needleleaved deciduous or evergreen forest
    7 & Forêt mixe décidue et sempervirent peu dense (>15\%, >5m)\\%Closed to open (>15%) mixed broadleaved and needleleaved forest (>5m)
    8 & Mosaïque forêt et fruticée (50-70\%)/pelouse (20-50\%)\\%Mosaic Forest/Shrubland (50-70%) / Grassland (20-50%)
    9 & Mosaïque pelouse (50-70\%)/forêt et fruticée (20-50\%)\\%Mosaic Grassland (50-70%) / Forest/Shrubland (20-50%)
    10 & Fruticée peu dense (>15\%, <5m)\\%Closed to open (>15%) shrubland (<5m)
    11 & Pelouse peu dense (>15\%)\\%Closed to open (>15%) grassland
    12 & Végétation éparse (boisée, fruticées, pelouse, >15\%)\\%Sparse (>15%) vegetation (woody vegetation, shrubs, grassland)
    13 & Végétation peu dense (boisée, fruticées, pelouse) sur sol régulièrement inondé ou détrempé (eau douce, saumâtre ou salée)\\%Closed to open (>15%) vegetation (grassland, shrubland, woody vegetation) on regularly flooded or waterlogged soil - Fresh, brackish or saline water
    14 & Surface artificielle ou assimilée (urbanisation>50\%)\\%Artificial surfaces and associated areas (urban areas >50%)
    15 & Zone de terre nue\\%Bare areas
    16 & Étendue d'eau\\%Water bodies
    17 & Nuage*\\%Clouds
    \bottomrule
  \end{tabularx}
\end{table}

Nous considérons une architecture SegNet réduite, dont le décodeur est coupé après le deuxième bloc. En effet, il ne nous est pas utile d'obtenir des cartes à résolution $1:1$. Nous utilisons l'approche multi-échelle de la~\cref{sec:deep_multiscale} pour générer des cartes à échelle $1:8$ (\SI{160}{\meter/\px}), $1:16$ et $1:32$ ce qui diminue le nombre de paramètres à optimiser et le temps de calcul. Ces cartes sont interpolées à résolution \SI{300}{\meter/\px} puis moyennées avant le calcul du \emph{softmax}. Nous considérons une variante SegNet \gls{RVB} entraîné sur les bandes Sentinel-2 (2,3,4) correspondant aux vraies couleurs et une variante SegNet \glslink{multispectral}{MSI} entraîné sur l'ensemble des 12 bandes. Suivant les hyperparamètres identifiés au~\cref{chap:cartographie}, nous implémentons les modèles à l'aide de la bibliothèque \gls{PyTorch}~\cite{noauthor_pytorch_2016}. Les réseaux sont optimisés par descente de gradient stochastique avec moment, avec un taux d'apprentissage fixe de \num{0,001} et un moment de \num{0,9} pendant \num{150000} itérations. Les variantes \gls{RVB} et \glslink{multispectral}{MSI} utilisent respectivement une de taille de \emph{batch} de 20 et 10, pour chacun occuper environ 6 Go de mémoire \gls{GPU}. L'entraînement prend environ 18 heures sur un \gls{GPU} NVIDIA Titan X (génération Pascal).

Dans l'ensemble, le modèle entraîné sur les 12 bandes atteint 66,5\% d'exactitude sur le jeu de données D1 (sans nuages) et 86,4\% sur D2 (avec nuages). L'écart important entre les jeux de données est une combinaison de deux facteurs. Tout d'abord, la présence des nuages nombreux et faciles à détecter (score $F_1 > 97\%$) augmente statistiquement le nombre de pixels bien classifiés. Néanmoins, comme détaillé dans le~\cref{tab:s2_results}, les performances sur l'ensemble des classes de D1 sont inférieures à celles des mêmes classes sur D2. Ceci s'explique par la faible variabilité des images de D1. Ce jeu de données ayant été constitué sous contrainte d'une couverture nuageuse faible, son étendue spatiale est plus faible et les modèles appris sur D1 généralisent moins bien sur les acquisitions utilisées pour la validation. À l'inverse, D2 couvre une période temporelle plus faible mais inclut une plus grande variabilité d'images. En particulier, D2 comporte plusieurs zones imagées sous divers conditions climatiques, avec une couverture nuageuse plus ou moins importante. Cela permet au modèle d'apprendre les invariants radiométriques nécessaires à une bonne généralisation.
Enfin, la prise en compte de l'ensemble des 12 bandes multispectrales plutôt que seulement les 3 bandes visibles \gls{RVB} permet de gagner 2\% d'exactitude sur D1 et 2,5\% sur D2. Ce résultat se vérifie sur la majorité des classes indiquées dans le~\cref{tab:s2_results}. Ceci conforte notre intuition initiale\,: l'information multispectrale semble plus riche que l'image couleur.

Cette étude nous permet d'aboutir à deux conclusions. D'une part, elle montre la pertinence des modèles entièrement convolutif pour le traitement des images multispectrales. En effet, l'architecture SegNet s'étend avec succès des données aériennes aux données Sentinel-2. S'il est nécessaire d'arbitrer certains choix techniques, notamment concernant l'interpolation des bandes et le choix d'une résolution de référence pour la vérité terrain, il ne semble pas y avoir de verrou majeur à l'utilisation des \glspl{FCN} pour le multispectral. D'autre part, l'inclusion des bandes hors du domaine visible permet d'augmenter l'expressivité du modèle, qui bénéficie de la richesse de l'information multispectrale. Cela permet notamment d'améliorer le pouvoir discriminant du SegNet sur des classes pouvant être ambiguës en \gls{RVB}.

\begin{table}[h]
  \caption[Résultats de classification de SegNet sur les jeux de données D1 et D2 Sentinel-2.]{Résultats de classification de SegNet sur les jeux de données D1 et D2 Sentinel-2 (cf.~\cref{tab:globecover_legende} pour le détail des classes).}
  \label{tab:s2_results}
  \setlength\tabcolsep{2pt}
  \scalebox{0.75}[0.75]{
  \begin{tabularx}{1.33\textwidth}{Yccccccccccccccccccc}
    \toprule
    Jeu de données      & Modèle                      & Exactitude & 1    & 2    & 3    & 4    & 5    & 6    & 7    & 8    & 9    & 10   & 11   & 12   & 13   & 14   & 15   & 16   & 17  \\
    \midrule
    \multirow{2}{*}{D1} &  SegNet \glsname{RVB}             & 55,0 & 39.0 & 35.5 & 1.8 & 71.0 & 0.9 & 0.0 & 0.0 & 0.0 & 0.0 & 2.1 & 0.0 & 5.3 & 0.0 & 36.0 & 0.0 & 74.6 & --\\
                        &  SegNet \glssymbol{multispectral} & 66,5 & 36.2 & 38.1 & 1.4 & 85.1 & 6.3 & 0.0 & 1.4 & 0.0 & 0.0 & 2.2 & 0.0 & 3.4 & 0.0 & 34.3 & 0.0 & 97.8 & --\\
    \midrule
    \multirow{2}{*}{D2} &  SegNet \glsname{RVB}             & 84,9 & 74,7 & 64,1 & 38,0 & 89,9 & 68,4 & 58,4 & 51,4 & 36,7 & 39,8 & 61,0 & 45,7 & 55,2 & 47,9 & 76,9 & 66,9 & 98,7 & 96,7\\
                        &  SegNet \glssymbol{multispectral} & 86,4 & 76,0 & 66,8 & 43,4 & 92,4 & 72,3 & 51,0 & 59,5 & 22,9 & 50,0 & 67,4 & 48,0 & 53,5 & 41,0 & 77,0 & 66,7 & 98,8 & 97,7\\
    \bottomrule
  \end{tabularx}}
\end{table}

\begin{figure}[h]
  \foreach\idx in {3,2}{
  \begin{subfigure}{\textwidth}
    \foreach\picname\picpath in {Couleur naturelle/sentinel_rgbclouds_\idx_tci,
                                 Prédiction/sentinel_rgbclouds_\idx_pred,
                                 Vérité terrain/sentinel_rgbclouds_\idx_gt}{
    \begin{subfigure}{0.33\textwidth}
      \includegraphics[width=\textwidth]{\picpath}
      \caption*{\picname}
    \end{subfigure}}%
  \end{subfigure}}
  \caption{Exemples de prédictions du modèle SegNet \glslink{multispectral}{MSI} entraîné sur D2 (avec nuages).}
  \label{fig:segnet_msi_d2}
\end{figure}

\begin{figure}[h]
  \foreach\picname\picpath in {Couleur naturelle/sentinel_rgbnoclouds_tci,
                               Prédiction/sentinel_rgbnoclouds_pred,
                               Vérité terrain/sentinel_rgbnoclouds_gt}{
  \begin{subfigure}{0.33\textwidth}
    \includegraphics[width=\textwidth]{\picpath}
    \caption*{\picname}
  \end{subfigure}}%
  \caption{Exemples de prédictions du modèle SegNet \glslink{multispectral}{MSI} entraîné sur D1 (sans nuages).}
  \label{fig:segnet_msi_d1}
\end{figure}

\section{Imagerie hyperspectrale}

\begin{figure}
  \begin{minipage}[t]{0.485\textwidth}
      \includegraphics[width=\textwidth]{hyperspectral_cube_pavia}
      \caption{Exemple de cube hyperspectral sur le jeu de données \emph{Pavia University}.}
      \label{fig:cube_hyperspectral}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.485\textwidth}
      \includegraphics[width=\textwidth]{reflectances}
      \caption{Exemple de réflectances caractéristiques de diverses surfaces terrestres.}
      \small{Crédits image\,: \href{https://commons.wikimedia.org/wiki/File:R\%C3\%A9flectance_surfaces_terrestres.png}{Arbeck (Wikimedia Commons, CC-BY-SA 3.0)}}
      \label{fig:reflectances}
  \end{minipage}
\end{figure}

Jusqu'ici, nous avons pu constater que l'interprétation d'images couleurs \gls{RVB} et infrarouge classiques bénéficie considérablement de l'introduction des \glslink{CNN}{réseaux de neurones convolutifs profonds}. Comme présenté dans la~\cref{sec:multispectral}, les \glspl{CNN} permettent également de tirer partie des acquisitions multispectrales, y compris hors du domaine visible. Cette information supplémentaire permet de caractériser des objets d'intérêt et des matériaux indétectables autrement. En réalité, chaque matériau possède une signature spectrale caractéristique définie par la façon dont il réfléchit la lumière. Il paraît donc naturel de chercher à acquérir simultanément les réponses lumineuses sur plusieurs bandes spectrales afin de réaliser une analyse fine des sols~\cite{cubero-castan_physics-based_2015,fabre_estimation_2015}.

Cette logique est à l'origine des capteurs \glslink{hyperspectral}{hyperspectraux}. Ceux-ci possèdent une faible résolution spatiale, mais une très grande résolution spectrale et sont capables de mesurer la réponse lumineuse d'un objet sur plusieurs centaines de bandes spectrales. Les méthodes de \emph{deep learning} purement orientées vision ne se transposent pas directement aux données hyperspectrales. En effet, la dimension spectrale prédomine devant les dimensions spatiales caractéristiques des objets d'intérêt. À titre d'exemple, une acquisition hyperspectrale aérienne typique présentera une résolution au sol au mieux de \SI{1}{\meter/\px} et une résolution spectrale d'environ \SI{10}{\nano\meter/bande}, pour 200 bandes spectrales entre \SI{0,4}{\micro\meter} et \SI{2,5}{\micro\meter}. Si l'on considère une maison individuelle de $12m\times10m$ \footnote{D'après le ministère de l'environnement, la surface de plancher moyenne des maisons françaises était de \SI{121}{\meter\squared} en 2015 (\href{http://www.cohesion-territoires.gouv.fr/IMG/pdf/datalab-essentiel-51-le-prix-des-terrains-a-batir-en-2015-oct2016.pdf}{\og \emph{Le prix des terrains à bâtir en 2015\fg}}).}, cet objet en hyperspectral sera décrit par un tenseur de taille $12\times10\times200$.
En comparaison, les acquisitions aériennes \gls{RVB} en \gls{EHR} présentent une résolution spatiale de l'ordre de \SI{10}{\centi\meter/\px} sur 3 bandes entre \SI{0,4}{\micro\meter} et \SI{0,7}{\micro\meter}. La même maison serait alors caractérisée par un tenseur de dimensions $120\times100\times3$. Si le volume de données est similaire (24 000 scalaires contre 36 000), leur répartition est nettement différente. En pratique, on parle ainsi de cube hyperspectral, ou \emph{hypercube} (cf.~\cref{fig:cube_hyperspectral}). Enfin, compte-tenu de la faible résolution spatiale des capteurs hyperspectraux, un hypercube couvre une même zone avec moins de pixels qu'une image \gls{RVB} classique. Le nombre d'échantillons annotés pour l'entraînement de modèles supervisés sera donc plus faible que précédemment. Ces deux éléments sont les principaux obstacles auxquels nous allons faire face pour mettre en \oe{}uvre l'apprentissage profond sur des hypercubes.

\subsection{Principes physiques de l'imagerie hyperspectrale}
%\textit{Paragraphe rédigé à partir d'éléments d'un mail de Xavier Ceamanos, ONERA.}

Physiquement, un capteur hyperspectral mesure\footnote{Les caméras multispectrales et hyperspectrales fonctionnent généralement sur un mode \emph{push broom} réalisant une acquisition ligne par ligne, différent des appareils photographiques habituels. Les détails techniques liés aux capteurs et à leur étalonnage ne seront pas abordés dans ce manuscrit.} l'intensité (en unité de luminance spectrale) du flux lumineux $\phi$ par unité de surface et par unité d'angle solide. Il s'agit d'une grandeur physique qui a pour unité le \si{\watt\per\square\meter\per\steradian}. Ce flux lumineux est mesuré par le capteur pour différentes bandes radiométriques, dont la largeur est aux alentours de \SI{10}{\nano\meter}. Pour chaque pixel, c'est-à-dire pour chaque unité de surface, le capteur discrétise sur plusieurs centaines de bandes le spectre lumineux renvoyé par la surface. Chacun de ces spectres peut se représenter sous la forme d'une courbe de réponse spectrale comme celles de la~\cref{{fig:reflectances}}. Ce flux lumineux intègre aussi bien la lumière émise et réfléchie par l'objet que celle provenant de l'environnement, qui s'ajoute à la mesure.

Dans le cadre de la télédétection pour l'observation de la Terre, les acquisitions sont réalisées depuis le sommet de l'atmosphère (en satellite) ou depuis l'atmosphère même (en aéroporté). Le signal provenant de la surface est ainsi altéré par les perturbations atmosphériques. Comme nous l'avons vu précédemment, les images satellitaires sont régulièrement perturbées par les nuages, les effets de brumes et les aérosols en suspensions dans l'air. Compte-tenu de la finesse des bandes d'acquisition, les acquisitions hyperspectrales sont particulièrement sensibles à ces phénomènes. Néanmoins, les applications en observation de la Terre qui nous concernent s'intéressent aux surfaces et des matériaux au sol. On s'intéressera ainsi de préférence à la réflectance du matériau, définie comme le rapport entre le flux émis par une surface et le flux incident~:
\begin{equation}
  \rho = \frac{\phi_{\mathit{r\acute{e}fl\acute{e}chi}}}{\phi_\mathit{incident}}
\end{equation}

La réflectance est un indicateur du pouvoir réfléchissant d'un objet pour longueur d'onde donnée (on parle également d'albédo du matériau). C'est une grandeur sans unité comprise entre 0 (surface complètement absorbante) et 1 (surface totalement réfléchissante). En règle générale, un matériau renvoyant plus de 80\% de la lumière blanche apparaît blanc, tandis qu'un matériau réfléchissant moins de 3\% apparaît noir. Dans le cas de la réflectance radiométrique, cet indice est mesuré pour chaque bande de fréquences du capteur, ce qui permet de dessiner une courbe de réflectance en fonction de la longueur d'onde. La réflectance est préférée à la luminance car il s'agit d'une propriété intrinsèque au matériau, indépendante de l'environnement extérieur. La courbe de réflectance d'un matériau correspond de fait à une signature spectrale dotée d'un fort pouvoir discriminant~\cref{fig:reflectances}. Lorsque c'est possible, on cherchera donc à travailler sur des données en réflectance.

\paragraph{Corrections environnementales}
Passer de la luminance à la réflectance nécessite d'éliminer l'influence de l'environnement sur l'intensité lumineuse mesurée par le capteur. La compensation des phénomènes perturbatoires impliquent des techniques dites de correction atmosphérique~\cite{deschamps_atmospheric_1980, rahman_smac_1994, chavez_image-based_1996}. Celles-ci permettent de réduire l'influence de l'atmosphère sur la mesure~\cite{gao_atmospheric_2009}. Les méthodes de correction atmosphérique transforment ainsi les images de luminance en images de réflectance. Pour ce faire, les spécialistes conçoivent des modèles d'atmosphère qu'ils inversent afin de rendre négligeable l'influence de la diffusion lumineuse et des phénomènes radiatifs. Généralement, ces modèles nécessitent de connaître divers paramètres physiques, comme l'ensoleillement. Certaines de ces informations peuvent être obtenues \emph{a posteriori} grâce aux éphémérides. Dans certains cas, il est possible d'embarquer un capteur d'ensoleillement sur le dos de l'appareil embarquant pour mesurer en temps réel les conditions d'illumination. Les capteurs hyperspectraux ``à main'' éclairent directement la cible et permettent ainsi de s'affranchir des conditions environnementales.

Par ailleurs, soulignons que le calcul de la réflectance se fait généralement sous hypothèse de planarité du sol. En effet, le relief natural du terrain et la présence d'objets en surface introduit des réflexions et des occlusions. Les premières peuvent conduire à des sur-illuminations lorsque plusieurs réflexions convergent vers le même point, tandis que les secondes provoquent une sous-illumination, c'est-à-dire des ombres. La prise en compte du \gls{MNE} est parfois nécessaire pour corriger efficacement l'hypercube, notamment en milieu urbain~\cite{ceamanos_using_2017}.

Enfin, il faut garder à l'esprit que bien qu'il s'agisse d'une pratique courante et maîtrisée, l'application d'une correction atmosphérique ou de terrain sur l'hypercube est susceptible d'introduire des erreurs et des incertitudes dans les données.

\paragraph{Visualisation}
Contrairement à la vision humaine, la plage de fonctionnement d'une caméra hyeprspectrale s'étend bien au-delà du domaine visible. La plupart des capteurs balaient les longueurs d'onde de l'ultra-violet (\SI{300}{\nano\meter}) jusqu'à la limite de l'infrarouge moyen (\SI{3 000}{\nano\meter}) par tranches de \SI{10}{\nano\meter}. En comparaison, le spectre visible ne couvre que les longueurs d'onde de \SI{300}{\nano\meter} à $\simeq$ \SI{700}{\nano\meter}. Les écrans utilisés pour la visualisation utilisent traditionnellement le mode \gls{RVB} et diffusent une image comme l'agrégation de trois cartes d'intensité en rouge, vert et bleu. Cette approche correspond aux trois types de récepteurs situés dans l'\oe{}il humain. Néanmoins, une image hyperspectrale correspond à un cube de données au sein duquel chaque pixel contient une réponse spectrale complète. Ces signatures spectrales caractérisent les surfaces et les matériaux lorsqu'ils sont purs. En pratique, la faible résolution spatiale implique que le spectre mesuré pour un pixel soit un mélange de différents matériaux, d'autant plus dans le cas de la végétation.

Compte-tenu de la différence des résolutions spectrales entre l'hyperspectral (très petite devant celle des yeux humains) et de l'imagerie \gls{RVB} classique, il n'y a pas d'équivalence entre les deux représentations. Une image hyperspectrale contient nettement plus d'informations que l'image \gls{RVB} à résolution spatiale identique. En outre, s'il est possible de reconstruire une image \gls{RVB} composite à partir de bandes spectrales bien choisies dans l'image hyperspectrale, les différences de résolution font qu'il ne s'agit que d'une pseudo-image, qui n'aurait pas été vue de cette façon par des yeux humains. En effet, un appareil photo fonctionne en captant séparément la lumière rouge, verte et bleue grâce à un filtre, de façon à simuler le fonctionnement de l'\oe{}il humain. Un capteur hyperspectral fera généralement une acquisition ligne par ligne du spectre complet, décomposé par un prisme (capteur dit ``\textit{pushbroom}''). Les deux modes d'acquisition ne sont ainsi pas comparables.

\subsection{Jeux de données}
\label{sec:hyperspectral_datasets}

Plusieurs acquisitions hyperspectrales annotées ont été rendues publiques afin d'évaluer les performances de classification de différentes méthodes\footnote{\url{http://www.ehu.eus/ccwintco/index.php?title=Hyperspectral_Remote_Sensing_Scenes}}. Nous présentons ici les jeux de données publics les plus utilisés par ordre de popularité.

Comme nous allons le voir, une des difficultés majeures en apprentissage statistique pour le traitement d'images hyperspectrales est le faible nombre d'échantillons disponibles. Compte-tenuu des différentes de capteurs, de conditions d'exposition et d'étalonnage, il est difficile d'exploiter conjointement plusieurs jeux de données. Or, individuellement, les images hyperspectrales annotées mises à disposition de la communauté scientifique sont de taille très faible en comparaison des banques d'images \gls{RVB} habituelles. Cela complique l'évaluation des méthodes d'apprentissage supervisées. Il existe bien une base de données à grande échelle d'acquisitions hyperspectrales\footnote{\url{https://\gls{AVIRIS}.jpl.nasa.gov/alt_locator/}}, réalisées avec le capteur \gls{AVIRIS} sur le territoire américain, mais ces images ne sont toutefois pas annotées.

\paragraph{Pavia}

Pavia est un jeu de données acquis via le capteur ROSIS avec une résolution au sol de \SI{1,3}{\meter} sur la ville de Pavie, en Italie. Il est divisé en deux scènes\,: Pavia University (103 bandes, \SI{610x340}{\px}) et Pavia Centre (102 bandes, \SI{1096x715}{\px}). 9 classes d'intérêt sont annotées, couvrant différents matériaux urbains (brique, asphalte, métaux), l'eau et la végétation sur 50\% de l'image.

Il s'agit d'un des principaux jeux de données de référence dans la communauté, notamment car les deux scènes se trouvent parmi les plus grandes images hyperspectrales annotées disponibles. En outre, l'utilisation du même capteur sur les deux images permet de tester des méthodes de transfert de connaissances en hyperspectral.

\paragraph{Indian Pines}

Indian Pines est un jeu de données acquis en utilisant le capteur américain \gls{AVIRIS}. La scène couvre une surface agricole sur 224 bandes spectrales pour $145\times145$px, avec une résolution au sol de \SI{3,7}{\meter/\px}. La majorité de l'image consiste en des champs d'une dizaine de cultures différentes, le reste étant occupé par de la végétation dense. 16 classes sont annotées, dont certaines très rares (moins de 100 échantillons). Les bandes d'absorption de l'eau (108$\rightarrow$112, 154$\rightarrow$167 et 224) sont généralement enlevées.

En dépit de sa faible taille, il s'agit d'un des principaux jeux de données de référence dans la communauté. Les classes les plus rares ne sont parfois pas prises en compte pour évaluer les algorithmes de classification.

\paragraph{Salinas}
Salinas est un jeu de données utilisant également le capteur \gls{AVIRIS}. La scène comporte $512\times217$ échantillons à \SI{3,7}{\meter/\px}. Les bandes d'absorption de l'eau (108$\rightarrow$112, 154$\rightarrow$167 et 224) sont généralement enlevées. 16 classes sont annotées, majoritairement concernant les différentes cultures observées, la végétation et le type de sol.

\paragraph{Kennedy Space Center (KSC)}
Le jeu de données KSC utilise le capteur \gls{AVIRIS} avec une résolution au sol de \SI{18}{\meter/\px}. Les bandes d'absorption de l'eau et celles avec un rapport signal/bruit faibles sont retirées pour ne conserver que les 176 bandes les plus informatives. 13 classes concernant divers types d'occupation du terrain sont annotées.

\paragraph{Botswana}
Botswana est un jeu de données acquis sur le delta du Okavango à l'aide du senseur Hyperion embarqué par le satellite EO-1 de la NASA, à une résolution de \SI{30}{\meter/\px} sur 242 bandes. Seules les 145 bandes 10$\rightarrow$55, 82$\rightarrow$97, 102$\rightarrow$119, 134$\rightarrow$164 et 187$\rightarrow$220 sont conservées, les autres correspondant aux bandes d'absorption de l'eau et à des bandes mal calibrées. 14 classes d'intérêt sont annotées concernant différents types de végétation et de marécages.

% \subsubsection{NAOMI Mandji}
% Le jeu de données NAOMI Mandji est une acquisition réalisée par le DOTA (ONERA Toulouse) en collaboration avec Total dans le cadre du projet de recherche NAOMI. Cette acquisition couvre les environs de Port-Gentil, sur l'île de Mandji au Gabon. La zone 2, correspondant à une image de $1944\times1196$px a été annotée sur 9 classes d'intérêt à une résolution spatiale de 1,3m/px. Au total, 416 bandes ont été acquises, 158 dans le VNIR (\textit{Visible and Near InfraRed}) et 258 dans le SWIR (\textit{Short-Wave InfraRed}). La couverture du SWIR est incluse que celle du VNIR, certains pixels n'ayant que les bandes correspondant au second.

\paragraph{DFC 2018}
Le jeu de données \glssymbol{DFC} 2018 correspond à une acquisition sur le centre-ville de Houston, Texas (États-Unis) à l'aide d'une caméra hyperspectrale aérienne. L'acquisition couvre le domaine \SIrange{380}{1050}{\nano\meter} à l'aide de 48 bandes à une résolution de \SI{1}{\meter/\px}. 20 classes d'intérêt sont définies, incluant des objets urbains (bâtiments, routes de différents types, rails, voitures, trains\dots) mais aussi la végétation (saine, stressée, décidue, sempervirent). Ce jeu de données est issu de la compétition \glsdesc{DFC} 2018, détaillée dans l'\cref{annexe:dfc2018}.

\paragraph{Récapitulatif}

Les caractéristiques des différents jeux de données publics identifiés sont listées dans le~\cref{tab:hyperx_datasets}. Le principal élément qui en ressort est le faible nombre d'échantillons annotés disponibles sur chacun des datasets. Le capteur \gls{AVIRIS} est utilisé sur plusieurs scènes, mais les classes identifiées ne sont pas cohérentes d'une acquisition à l'autre, limitant le potentiel de réutilisation des modèles.

\begin{table}[h]
\setlength{\tabcolsep}{3pt}
\begin{tabularx}{\textwidth}{ c Y c c c c Y c }
\toprule
Jeu de données & Pixels & Bandes & Domaine & Résolution & Annotations & Classes & Acquisition\\
\midrule
Pavia & \num{991040} & 103 & \SIrange{0,4}{0,9}{\micro\meter} & \SI{1,3}{\meter/\px} & \num{50232} & 9 & Aérienne\\
Indian Pines & \num{21025} & 224 & \SIrange{0,4}{2,5}{\micro\meter} & \SI{3,7}{\meter/\px} & \num{10249} & 16 & Aérienne\\
Salinas & \num{111104} & 227 & \SIrange{0,4}{2,5}{\micro\meter} & \SI{3,7}{\meter/\px} & \num{54129} & 16 & Aérienne\\
KSC & \num{314368} & 176 & \SIrange{0,4}{2,5}{\micro\meter} & \SI{18}{\meter/\px} & \num{5211} & 13 & Aérienne\\
Botswana & \num{377856} & 145 & \SIrange{0,4}{2,5}{\micro\meter} & \SI{30}{\meter/\px} & \num{3248} & 14 & Satellite\\
%NAOMI Mandji & 1 052 012 & 416 & 0,4-2,5µm & 1,3m & 1 052 012 & 9 & Aérienne\\
\bottomrule
\end{tabularx}
\caption{Récapitulatif des principaux jeux de données publics annotés en imagerie hyperspectrale.}
\label{tab:hyperx_datasets}
\end{table}

\subsection{Approches traditionnelles}

Avant de s'atteler à l'étude de l'apprentissage profond pour la classification d'images hyperspectrales, il est intéressant de se pencher sur les méthodes classiquement utilisées dans la littérature. Nous présentons dans cette section un bref tour d'horizon des techniques déployées dans l'état de l'art pour le traitement d'images hyperspectrales, avec une attention particulière accordée aux méthodes supervisées.

\subsubsection{Pré-traitements et normalisations}

Comme nous l'avons vu, les données hyperspectrales brutes sont rarement exploitables directement. En plus des corrections atmosphériques et de l'ortho-rectification afin d'obtenir des cartes de réflectance géo-référencées, il est courant d'effectuer diverses opérations de normalisation des données.

En premier lieu, il est fréquent de retirer certaines longueurs d'onde difficiles à exploiter. Selon la calibration, certaines bandes peuvent être saturées et écrasent la dynamique des spectres. À l'inverse, compte-tenu de l'influence de l'humidité atmosphérique, les bandes d'absorption de l'eau sont généralement trop bruitées pour être exploitables. Dans l'ensemble, seules les bandes avec un rapport signal sur bruit acceptable sont conservées, ce qui permet de réduire la dimensionalité des données avec une perte minimale d'information.

Ensuite, il est souvent préférable de normaliser statistiquement les spectres. Plusieurs approches peuvent être utilisées selon les propriétés devant être mises en valeur\,:
\begin{itemize}
\item Si les formes des spectres sont plus importantes que leurs amplitudes, on utilisera l'angle spectral, version normalisée du spectre\,:
$$X^* := \frac{X}{\| X \|}~~,$$
\item La normalisation des moments statistiques de premier et second ordres (moyenne nulle et variance unitaire), globale ou pour chaque bande, permet de faire apparaître et de retirer des anomalies (à $\pm5\sigma$, par exemple)\,:
$$\mathcal{I}^* := \frac{\mathcal{I} - m_\mathcal{I}}{\sigma_\mathcal{I}} \text{ avec } m_\mathcal{I} \text{ la moyenne de } \mathcal{I} \text{ et } \sigma_\mathcal{I} \text{ sa variance,}$$
\item Enfin, la normalisation dans $[0,1]$ est couramment utilisée globalement pour simplifier les implémentations des calculs numériques. Alternativement, elle permet de donner la même importance à toutes les longueurs d'onde lorsqu'elle est appliquée\,:
$$\mathcal{I}^* := \frac{\mathcal{I} - \min(\mathcal{I})}{\max(\mathcal{I}) - \min(\mathcal{I})}~~.$$
\end{itemize}

Les valeurs aberrantes, par exemple supérieures au 98\ieme percentile ou au-delà de $m_\mathcal{I} + 5\sqrt{\sigma_\mathcal{I}}$, peuvent être tronquées ou supprimées pour limiter leur influence. Cela permet de prendre en compte des anomalies dues aux erreurs de correction, à des matériaux particulièrement réfléchissants (comme les métaux) ou aux réflexions multiples.

Soulignons que, dans un cadre parfait, un pixel d'une image hyperspectrale correspond à la réflectance du matériau observé sur une unité de surface. Toutefois, la résolution spatiale des images fait qu'un pixel correspond à une surface couvrant plusieurs matériaux, produisant ainsi des spectres de mélange. Concrètement, si $\varphi_1, \dots, \varphi_n$ désignent les spectres purs de l'ensemble des matériaux de la scène, alors en un pixel $(i,j)$, le spectre local observé sera une fonction $f$ des $\varphi_i$:
\begin{equation}
  \phi_{i,j}~~~=~~~f_{i,j}(\varphi_1, \dots, \varphi_n)~~~\simeq~~~\sum_{k = 1}^n \lambda_k \varphi_k~~.
\end{equation}
Dans le cas où la surface est plane, on peut faire l'hypothèse que $f$ est une simple combinaison linéaire, où le coefficient de pondération $\lambda_k$ correspond à la proportion du matériau $k$ dans la surface observée\footnote{Certains mélanges de matériaux présentent également des propriétés non-linéaires, mais on traite alors ces cas à part.}.

Un certain nombre de travaux s'intéressent à l'inversion de ce problème sous la forme d'une tâche de démélange~\cite{parra_unmixing_1999}. La classification la plus simple consiste ainsi à déterminer les matériaux purs qui composent la scène et de chercher à calculer des cartes d'abondance. Les spectres de référence des matériaux purs sont appelés les \emph{endmembers}\footnote{En minéralogie, un \emph{endmember} est un minéral en bout de chaîne de pureté. La plupart des minéraux sont des solutions solides, c'est-à-dire des mélanges de ces \emph{endmembers}).} et constituent une base de décomposition des spectres mélangés. Les cartes d'abondance correspondent alors aux proportions des différents matériaux en chaque point. Généralement, en connaissant les spectres purs $S_k$ et l'image $\mathcal{I}$, il est possible d'inverser le système linéaire pour obtenir les coefficients $\lambda_k$ du mélange en chaque point. Ces méthodes reposent principalement sur des mécaniques d'algèbre linéaire et des méthodes numériques d'inversion de problème. Des méthodes d'apprentissage, par exemple par \emph{clustering} permettant d'obtenir les \emph{endmembers} quand ils sont inconnus. L'identification des \emph{endmembers} et le démélange est hors du cadre de travail considéré pour nos travaux.

\subsubsection{Classification de spectres}

Les approches de classification de données hyperspectrales les plus simples opèrent pixel à pixel et traitent ainsi les spectres indépendamment les uns des autres. Nous présentons ici quelques unes de ces approches unidimensionnels. Nous écartons volontairement les approches expertes pour ne considérer que les méthodes d'apprentissage statistique.

Une première approche consiste à réduire la dimension spectrale des données afin de lutter contre la malédiction de la dimensionalité. En effet, compte-tenu de la haute résolution spectrale des imageurs, les réflectances voisines tendent à être fortement correlées, et la signature spectrale une information très redondante. Il est donc souvent intéressant de réduire la taille des données en ne considérant que les bandes contenant de l'information discriminante~\cite{le_bris_extraction_2015,bevilacqua_unsupervised_2017}. \citet{rodarmel_principal_2002} applique ainsi une \gls{ACP} aux spectres avant leur classification. Le calcul des indices physiques comme le \gls{NDVI} ou \gls{NDWI} rentre également dans ce type d'approches.

La classification se fait ensuite de façon traditionnelle, en utilisant des modèles statistiques classiques : arbres de décision et forêts aléatoires, machines à vecteurs de support (SVM), etc. La réduction de dimension permet de simplifier l'espace de représentation et facilite donc l'apprentissage.

Cependant, l'approche purement spectrale n'est pas satisfaisante dans la mesure où elle n'exploite pas la structure spatiale des objets. En effet, les progrès technologiques permettent d'améliorer la résolution des capteurs et donc d'augmenter le nombre de pixels acquis pour une même surface. Des pixels voisins partageront vraisemblablement de nombreuses propriétés spectrales, et des structures peuvent alors apparaître (par exemple, les bâtiments ont généralement des formes polygonales tandis que la végétation présente une apparence fractale). Prendre en compte l'aspect spatial dans l'analyse permet de rendre le modèle plus robuste et plus performant en prenant en compte ces dépendances structurelles. Il existe trois grandes familles d'approches selon la place de l'aspect spatial dans le processus de classification.

L'approche la plus simple consiste à effectuer une classification spectre par spectre en utilisant un modèle unidimensionnel, pusi à régulariser a posterio les prédictions. Les modèles graphiques comme les \gls{CRF} se prêtent particulièrement bien à cette tâche~\cite{wu_semi-supervised_2016}. La régularisation spatiale intervient ainsi durant une seconde étape de traitement, indépendante de la première.
À l'inverse, il est possible de faire intervenir l'aspect spatial en amont en utilisant le principe de classification par région, déjà présenté dans la~\cref{sec:classif_region}. \citet{tarabalka_segmentation_2010,fauvel_advances_2013} décrivent plusieurs méthodes en deux étapes\,: premièrement une segmentation de l'image hyperspectrale, puis des prédictions pixelliques agrégées par région afin d'introduire une cohérence spatiale locale.
Enfin, il existe également des approches s'appuyant sur des caractéristiques spatiales-spectrales. C'est l'approche originellement poursuivie afin d'exploiter la corrélation entre pixels spatialement proches pour le calcul des \textit{endmembers}~\cite{plaza_spatial/spectral_2002,dellacqua_exploiting_2004} en utilisant un mélange de classifieurs spatial et spectral. Les approches plus récentes utilisent des modèles statistiques à noyaux spécifiquement conçus pour travailler sur des voisinages locaux de spectres, de taille statique ou adaptative, pour en extraire une combinaison de caractéristiques spatiales et spectrales. Notamment, \citet{camps-valls_composite_2006} a introduit la possibilité de travailler sur des \glspl{SVM} à noyau spatial-spectral pour les données hyperspectrales, technique qui sera ensuite largement réutilisée dans la littérature \cite{tarabalka_spectralspatial_2009,fauvel_spatial-spectral_2012}. Plus récemment, \citet{cui_scalable_2017} introduit des \gls{SVM} à noyaux adaptés aux profils d'attributs morphologiques, tandis que~\citet{tuia_multiclass_2015} propose une méthode adaptative de sélection de noyaux de convolutions à partir de filtres aléatoires.

\subsection{Apprentissage profond et imagerie hyperspectrale}

Les approches présentées jusqu'ici utilisent des modèles statistiques peu profond. Néanmoins, à partir de 2013, la littérature en classification d'images hyperspectrales a commencé à mettre en \oe{}uvre des réseaux de neurones profonds pour ce type de données.

\begin{figure}[h]
  \resizebox{\textwidth}{!}{\input{Chapitre3/cnn1d.tikz}}
  \caption[\glssymbol{CNN} unidimensionnel pour la classification de spectres.]{\glssymbol{CNN} unidimensionnel pour la classification de spectres de~\citet{hu_deep_2015}.}
\end{figure}

L'évolution la plus simple consiste à remplacer les classifieurs standard (\gls{SVM} ou forêts aléatoires) par un perceptron multi-couche. Conceptuellement, le processus est identique mais si le réseau est assez profond, il peut s'avérer plus expressif et doté d'une meilleure capacité de discrimination. Cette approche existe en réalité depuis les années 2000~\cite{goel_classification_2003,ratle_semisupervised_2010} en utilisant des réseaux à une ou deux couches cachées. Elle a été réactualisée par~\citet{hu_deep_2015} en 2015 en utilisant des \glspl{CNN} unidimensionnels appliqués sur les spectres individuels (cf.~\cref{fig:cnn1d_hsi}).

Il est intéressant de constater que peu d'articles en apprentissage profond pour l'imagerie hyperspectrale s'embarassent des problématiques de sélections de bandes, de rejet des valeurs saturées ou d'analyse fine des phénomènes physiques mis en jeu. La plupart des études considèrent les données brutes normalisées, incluant les bandes spectrales bruitées ou saturées. En pratique, la robustesse des réseaux profonds permet de ne pas avoir à se préoccuper de ces considérations, le modèle éliminant naturellement les données non informatives.

En outre, les autoencodeurs ont permis de significativement faire progresser la question de la réduction des données hyperspectrales. Vus sous l'angle de la compression de données, les autoencodeurs ont permis de réduire les données hyperspectrales avec une perte d'information minimale, de façon bien plus efficace qu'une \gls{ACP}, avec de nombreuses applications pratiques en débruitage~\cite{xing_stacked_2015}. Les représentations ainsi apprises peuvent être utilisées pour la classification par n'importe quel modèle statistique~\cite{fu_semi-supervised_2016}.

Comme précédemment, il existe des approches spatiales-spectrales basées sur des descripteurs combinant une caractéristique spectrale, dérivée de la réponse radiométrique, et une caractéristique spatiale, dépendant des pixels voisins. Un descripteur classique consiste à concaténer le spectre du pixel considéré avec un le résultat d'une \gls{ACP} appliquée sur un voisinage local de dimensions $w\times h$ dont on conserve les $K$ premières composantes (généralement, $w = h \simeq 8$ et $K = 3$). Ce vecteur est ensuite utilisé pour entraîner un classifieur profond, supervisé ou non\,: \gls{DBN}~\cite{li_classification_2014,chen_spectral-spatial_2015}, \gls{RBM}~\cite{lin_spectral-spatial_2013,midhun_deep_2014} ou cascade d'autoencodeurs~\cite{chen_deep_2014,ma_spectral-spatial_2016,tao_unsupervised_2015,wang_spectralspatial_2017}.

\begin{figure}[h]
  \resizebox{\textwidth}{!}{\input{Chapitre3/pcacnn.tikz}}
  \caption[Architecture hybride \glsname{ACP}+\glssymbol{CNN} pour la classification d'hypercubes.]{Architecture hybride \glsname{ACP}+\glssymbol{CNN} de~\citet{makantasis_deep_2015} pour la classification d'hypercubes.}
\end{figure}

Toutefois, le retour sur le devant de la scène des \glspl{CNN} au début des années 2010 a également influencé la communauté de l'imagerie hyperspectrale. Ces réseaux sont initialement prévus pour traiter des images \gls{RVB} ou en niveaux de gris et manipulent donc des filtres convolutifs 2D. \citet{makantasis_deep_2015,slavkovikj_hyperspectral_2015} utilisent une architecture hybride alternant convolutions spatiales et réductions de dimension (par \gls{ACP} chez~\citet{makantasis_deep_2015} et sous-échantillonnage chez~\citet{slavkovikj_hyperspectral_2015}). Les caractéristiques résultantes sont ensuite vectorisées et transmises à un perceptron multi-couches réalisant la classification, comme schématisé par la~\cref{fig:pcacnn}. L'intérêt de cette approche est bien sûr de pouvoir apprendre automatiquement la représentation des données adaptée à la classification. \citet{zhao_combining_2015} étend cette technique au cadre semi-supervisé en utilisant des autoencodeurs convolutifs multi-échelle. Dans le cadre non-supervisé, \citet{romero_unsupervised_2015} proposent un \gls{CNN} pour l'extraction de caractéristique effectuant une réduction de dimension en prenant en compte le spectre, mais aussi ses voisins, l'objectif étant d'apprendre une représentation parcimonieuse des données. Enfin, \citet{zhao_spectral-spatial_2016,yue_spectral-spatial_2015} proposent une approche mixte utilisant un \gls{CNN} comme extracteur de caractéristiques spatiales, qu'ils combinent à un \gls{CNN} 1D pour générer un descripteur spatial-spectral.


Bien que performantes, ces architectures différencient dans leur traitement les aspects spatiaux et spectraux de l'hypercube. Cependant, les approches traditionnelles ont montré la pertinence des noyaux spatiaux-spectraux et de l'apprentissage conjoint qu'ils permettent. Plusieurs travaux ont donc introduit des convolutions tridimensionnels permettant d'apprendre des noyaux opérant directement sur le cube de données. En particulier~\citet{ben_hamida_deep_2016,chen_deep_2016} proposent des \gls{CNN} alternant convolutions 3D et convolutions 1D pour la réduction de dimension. \citet{luo_hsi-cnn_2018} suggère une approche alternative en remplaçant l'\gls{ACP} de~\citet{slavkovikj_hyperspectral_2015} par une couche convolutive 3D réalisant la réduction de dimension, suivie d'un \gls{CNN} 2D traditionnel. Comme d'habitude, la classification s'opère pixel à pixel par deux couches entièrement connectées en fin de réseau. En comparaison, \citet{lee_contextual_2016} étend cette structure en un \glspl{FCN} dont la première couche extrait une caractéristique spatiale-spectrale en utilisant deux convolutions parallèles, une en 1D et une en 3D, en s'inspirant du module \emph{Inception}. Le reste de l'architecture présente une structure résiduelle enchaînant les convolutions 1D.

\begin{figure}[h]
  \resizebox{\textwidth}{!}{\input{Chapitre3/cnn3d.tikz}}
  \caption[\glssymbol{CNN} 3D pour la classification d'hypercubes.]{\glssymbol{CNN} 3D de~\citet{chen_deep_2016} pour la classification d'hypercubes.}
  \label{fig:cnn3d_hsi}
\end{figure}

Enfin, des réseaux convolutifs entièrement 3D ont finalement été introduits, adaptant l'architecture canonique des \gls{CNN} pour l'imagerie \gls{RVB} aux hypercubes. Divers architectures ont été proposées~\cite{li_spectralspatial_2017}, incluant des variantes bien connues de la communauté vision par ordinateur, comme l'extraction de caractéristiques multi-échelle~\cite{he_multi-scale_2017} et l'entraînement semi-supervisé~\cite{liu_semi-supervised_2017}. Dans l'ensemble, il s'agit d'une extension du modèle canonique des \glspl{CNN} de~\citet{lecun_gradient-based_1998} aux données tridimensionnelles, comme schématisé dans la~\cref{fig:cnn3d_hsi}.

%\sebastien{Faire un tableau de synthese avec +/- ?}
Malgré la profusion de littérature concernant la classification d'images hyperspectrales, il n'existe pas de cadre standardisé d'évaluation des modèles. En particulier, chaque auteur considère un ou plusieurs des jeux de données présentés en~\cref{sec:hyperspectral_datasets} avec des partitions entraînement/validation/test différent. En outre, les implémentations mises à disposition de la communauté scientifique sont particulièrement rares en comparaison des pratiques habituelles en vision par ordinateur. Par conséquent, nous développons une \emph{DeepHyperX}\footnote{\url{https://gitlab.inria.fr/naudeber/DeepHyperX}}, une boîte à outils d'apprentissage profond pour la cartographie automatisée à partir d'images hyperspectrales. Ce logiciel utilise la bibliothèque logicielle \gls{PyTorch}~\cite{noauthor_pytorch_2016} et permet de comparer divers modèles de l'état de l'art sur un ensemble standardisé de jeux de données. La boîte à outils est modulaire et comprend nativement plusieurs approches supervisées, allant de la \gls{SVM} linéaire aux \gls{CNN} 3D de l'état de l'art en passant par les \gls{CNN} 1D. Ces modèles peuvent être entraînés et validés sur divers jeux de données de la littérature comme Pavia Center et Pavia University, Indian Pines, Kennedy Space Center ou \gls{DFC} 2018. Les hyperparamètres les plus courants peuvent être ajustés afin d'évaluer l'influence des dimensions du voisinage local considéré, du nombre d'échantillons d'apprentissage ou de la stratégie d'optimisation. Cette boîte à outils nous permet de comparer les performances des modèles de l'état de l'art dans un cadre unifié.
%\nicolas{En théorie, on peut aussi y inclure un paquet de modèles de \gls{scikit-learn} mais est-ce que ça a vraiment un intérêt ?}
%\bertrand{Je concorde avec toi, et on avisera en fonction des commentaires des reviewers}

%\here{Based on this toolbox, a comparative study of the various families of deep learning approaches will be proposed on the reference datasets.}

%\bertrand{-> NA: add toolbox content description}
Techniquement, cette boîte à outils est écrite en Python~\cite{python_software_foundation_python_nodate} et consiste en une interface construite autour des bibliothèques \gls{PyTorch}~\cite{noauthor_pytorch_2016} et \gls{scikit-learn}~\cite{pedregosa_scikit-learn_2011}. Les réseaux de neurones profonds sont implémentés à l'aide de \gls{PyTorch}, afin de permettre une exécution sur \gls{CPU} comme sur \gls{GPU}, tandis que les \gls{SVM} utilisent \gls{scikit-learn}. Plusieurs jeux de données publics sont pré-configurés afin de faciliter l'expérimentation. L'architecture modulaire de la boîte à outils permet aux programmeurs de facilement ajouter de nouveaux jeux de données ou de nouveaux modèles de réseaux profonds pour tester de nouvelles idées ou évaluer les performances sur des acquisitions privées.

Dans la suite, nous évaluons plusieurs architectures profondes de la littérature pour la classification d'images hyperspectrales de télédétection. À notre connaissance, il s'agit de la première étude systématique des différents modèles de réseaux convolutifs introduits dans l'état de l'art. La majorité des articles réalise des expériences légèrement différentes les unes des autres, soit en excluant certaines classes, soit en considérant des partitions entraînement/validation différentes. De plus, l'approche la plus fréquente consiste à sélectionner un des jeux de données publics, à entraîner un modèle un ensemble de pixels tirés aléatoirement dans l'image et de valider ses performances sur le reste de l'image. Or, cette approche n'est pas réaliste, dans la mesure où des pixels proches les uns des autres seront fortement corrélés. Par conséquent, le jeu de validation sera très proche du jeu d'apprentissage et les métriques de classification ne seront pas réellement indicatives de la capacité de généralisation du modèle. Au contraire, ces pratiques récompensent le sur-apprentissage et sont généralement découragées par la communauté de l'apprentissage automatique. Nous suivons donc les approches classiques d'évaluation de modèles statistiques en considérant des partitions entraînement/validation \emph{spatialement disjointes} et en moyennant les résultats sur plusieurs entraînements, si possible par validation croisée sur plusieurs plis. Dans le cas des \glspl{CNN} 3D, cela garantit notamment qu'aucun pixel du jeu de validation n'aura été vu accidentellement durant l'apprentissage.

Pour la suite, nous considérons les partitions définies par l'\gls{IEEE} \gls{GRSS} sur leur plate-forme d'évaluation DASE\footnote{\emph{GRSS Data and Algorithm Standard Evaluation website}\,: \url{http://dase.ticinumaerospace.com/}} pour les jeux de données Indian Pines, Pavia University et \glssymbol{DFC} 2018. Les hyperparamètres sont choisis en considérant un jeu de validation séparé incluant 5\% du jeu d'apprentissage.

Nous utilisons notre boîte à outils afin de comparer plusieurs réimplémentations de modèles de l'état de l'art. Ceux-ci ont été reproduits le plus fidèlement possible. Les changements appliqués sont listés ci-dessous\,:
\begin{itemize}
	\item \gls{CNN} 1D de~\citet{hu_deep_2015}. L'algorithme d'optimisation n'étant pas spécifié dans l'article original, nous utilisons la descente de gradient stochastique avec moment.
    \item \gls{RNN} 1D de~\citet{mou_deep_2017}. Nous utilisons la fonction d'activation $tanh$ usuelle en lieu et la place de la version paramétrisée des auteurs.
    \item \gls{CNN} 3D+1D de~\citet{ben_hamida_deep_2016}. Pas de modification.
    \item \gls{CNN} 3D de~\citet{li_spectralspatial_2017}. Nous avons augmenté le nombre de filtres de 16 à 32 dans les couches convolutives.
\end{itemize}

Les \glspl{CNN} 3D sont entraînés sur des voisinages de dimensions \num{5x5}. Afin d'établir des scores de référence, nous utilisons deux modèles simples\,: une \gls{SVM} obtenue par \emph{grid search} et un réseau entièrement connecté 1D à trois couches utilisant \gls{ReLU}~\cite{nair_rectified_2010} comme activation et auquel \emph{Dropout}~\cite{srivastava_dropout_2014} est appliqué. Le déséquilibre entre les classes est géré au niveau de la fonction de coût en utilisant un rééquilibrage par fréquence médiane inverse. Les données sont augmentées par symétries verticales et horizontales. Les résultats sont détaillés dans le~\cref{tab:results}, incluant l'exactitude globale et le $\kappa$ de Cohen sur les trois jeux de données. Les expériences ont été répétées 5 fois sur Pavia University et Indian Pines, mais seulement une fois sur \glssymbol{DFC} 2018 compte-tenu de sa taille plus importante.

Comme il était possible de s'y attendre, nous obtenons des résultats significativement inférieurs à ceux indiqués dans les articles originaux, notamment car nous utilisons une partition apprentissage/validation disjointe. Cela nous permet notamment de mettre en évidence un comportement particulier d'Indian Pines par rapport aux autres jeux de données. En effet, la prise en compte du contexte spatial sur ce jeu de données diminue en pratique les performances. Nous émettons l'hypothèse que la faible résolution spatiale d'Indian Pines (\SI{20}{\meter/\px}) implique que chaque pixel est déjà un mélange de la réflectance des cultures sur \SI{400}{\meter\squared}, et que les pixels voisins n'apportent pas plus d'information discriminante.  Pour Pavia University et \glssymbol{DFC} 2018, sur lesquels la résolution est plus haute, les \glspl{CNN} 3D sont significativement plus performants que les modèles 1D, augmentant l'exactitude globale de respectivement 3\% et 2\%. En particulier, le jeu de données \glssymbol{DFC} 2018 est difficile compte-tenu du grand nombre de classes similaires. Dans nos expériences, le réseau entièrement connecté 1D souffre d'un important surapprentissage et s'avère moins discriminant qu'une simple \gls{SVM}. Ce surapprentissage est d'autant plus important sur le DFC 2018 car le jeu d'apprentissage est entièrement disjoint de l'image initiale, contrairement à Indian Pines et Pavia University. Enfin, le \gls{CNN} 3D de~\citet{ben_hamida_deep_2016} ne parvient pas à extraire de l'information spatiale discriminante sur le \glssymbol{DFC} 2018. En pratique, les deux premières couches 3D sont insuffisantes et le champ réceptif du réseau est trop faible pour correctement modéliser les relations spatiales entre pixels sur un jeu de données haute résolution.

\begin{table}
  \setlength\tabcolsep{3pt}
\begin{tabularx}{\textwidth}{Y c c c c c c}
\toprule
Modèle & \multicolumn{2}{c}{Indian Pines} & \multicolumn{2}{c}{Pavia University} & \multicolumn{2}{c}{\glssymbol{DFC} 2018}\\
& Exactitude & $\kappa$ & Exactitude & $\kappa$ & Exactitude & $\kappa$\\
\midrule
\glssymbol{SVM} & 81.43 & 0.788 & 69.56 & 0.592 & 42.51 & 0.39\\
1D \glssymbol{NN} & \bres{83.13}{0.84} & \bres{0.807}{0.009} & \res{76.9}{0.86} & \res{0.711}{0.010} & 41.08 & 0.37\\
1D \glssymbol{CNN}~\cite{hu_deep_2015} & \bbres{82.99}{0.93} & \bbres{0.806}{0.011} & \res{81.18}{1.96} & \res{0.759}{0.023} & \textit{47.01} & \textit{0.44}\\
\glssymbol{RNN}~\cite{mou_deep_2017} & \res{79.70}{0.91} & \res{0.769}{0.011} & \res{67.71}{1.25} & \res{0.599}{0.014} & 41.53 & 0.38\\
3D+1D \glssymbol{CNN}~\cite{ben_hamida_deep_2016} & \res{74.31}{0.73} & \res{0.707}{0.008} & \bbres{83.80}{1.29} & \bbres{0.792}{0.016} & 46.28 & 0.43\\
3D \glssymbol{CNN}~\cite{li_spectralspatial_2017} & \res{75.47}{0.85} & \res{0.719}{0.010} & \bres{84.32}{0.72} & \bres{0.799}{0.009} & \textbf{49.26} & \textbf{0.46}\\
\bottomrule
\end{tabularx}
\caption{Résultats de classification de différents modèles de notre boîte à outils \emph{DeepHyperX} sur les jeux de données Indian Pines, Pavia University et \glssymbol{DFC} 2018. Les meilleurs résultats sont en \textbf{gras} et les suivants sont en \textit{italique}.}
\label{tab:results}
\end{table}

Un obstacle majeur identifié dans ces travaux est la difficulté d'entraîner des modèles ne souffrant pas de surapprentissage. En effet, le nombre d'échantillons considéré étant faible, les modèles utilisés comportent souvent suffisamment de poids pour mémoriser le jeu de données. Augmenter le nombre d'échantillons d'apprentissage n'est pas chose facile. Notamment, les différences de capteurs rend complexe l'apprentissage par transfert. Si des approches d'adaptation de domaine existent pour appliquer un classifieur à de nouvelles acquisitions~\cite{tuia_domain_2016}, elles ne résolvent pas le problème de l'entraînement initial de celui-ci. Un palliatif possible consiste à générer des données synthétiques suffisamment réalistes pour améliorer la généralisation du modèle. Cette technique sera étudiée dans le~\cref{chap:generalization}.

À noter également que la plupart des architectures actuelles se content d'effectuer une classification pixel à pixel. Comme nous l'avons vu dans le~\cref{chap:cartographie}, à mesure que les images hyperspectrales augmentent en résolution et en dimensions, il sera vraisemblablement nécessaire de concevoir des architectures 3D entièrement convolutives.

Enfin, il est souhaitable de voir apparaître de nouveaux jeux de données annotés en hyperspectral étant plus complexes et plus grands que ceux existants. En effet, ceux-ci semblent avoir largement atteint leurs limites. L'état de l'art rapporte généralement des améliorations quantitativement incrémentales dont la pertinence statistique est discutable. Comment évaluer un modèle obtenant 99,5\% de précision par rapport à une autre à 99,8\%? Un jeu de données proposant un cadre expérimental unifié permettrait de comparer équitablement diverses approches sur des tâches complexes qui ne seraient pas abordables sans imagerie hyperspectrale.

\section{Imagerie laser et modèles de terrain}

Hors du domaine optique, un des capteurs de prédilection pour l'observation de la Terre est le \gls{Lidar}. Il s'agit d'un capteur laser permettant d'évaluer, entre autres, la hauteur des points de la surface du globe. Lors d'une acquisition optique aérienne, il est courant d'également embarquer dans l'appareil un \gls{Lidar} afin d'étudier la topologie du terrain. Nous nous intéressons dans cette section aux possibilités d'exploiter directement ces données à l'aide de \gls{FCN}.

\begin{figure}[h]
  \foreach\picname\picpath in {Ortho-image \gls{IRRV}/vaihingen_top_30,\gls{MNE}/vaihingen_dsm_30,\gls{MNH}/vaihingen_ndsm_30,\gls{NDVI}/vaihingen_ndvi_30,Composite/vaihingen_composite_30}{%
  \begin{subfigure}{0.2\textwidth}
    \includegraphics[width=\textwidth]{\picpath}
    \caption*{\picname}
  \end{subfigure}%
  }%
  \caption{Tuile 30 du jeu de données ISPRS Vaihingen selon plusieurs modalités.}
  \label{fig:composite_vaihingen}
\end{figure}

\subsection{Modèle de terrain}

Les acquisitions de nuage de points par imagerie \gls{Lidar} permettent d'obtenir des modèles numériques détaillant la topologie du terrain observé\,: \gls{MNT}, \gls{MNE} et \gls{MNH}. Si l'obtention de ces modèles par rasterisation des nuages de points \gls{Lidar} ne fait pas partie de l'étendue de cette thèse, il s'agit néanmoins d'une source de données particulièrement intéressante. En effet, les modèles de terrain permettent d'accéder à une information concernant l'élévation locale du terrain (\gls{MNT}) et des objets qui s'y trouvent \gls{MNH}. La majorité des images de télédétection aéroportées et satellitaires étant acquises au nadir, celles-ci n'expriment donc pas d'information de hauteur ou de distance, contrairement aux images multimédia. Il en découle une absence d'occlusion, mais cela rend également plus complexe l'estimation de la hauteur des objets observés à partir des images optiques seules. Si les ombres portées peuvent donner une information indirecte concernant l'élévation des objets, celle-ci est toutefois peu fiable car dépendante de la topologie du terrain et surtout des conditions environnementales d'illumination (azimuth de l'acquisition, position du soleil, météo).

Or, l'environnement urbain présente de nombreux éléments surélevés pouvant se confondre avec le sol\,: ponts, parkings aériens, toits bétonnés ou végétalisés, végétation arborescente dense\dots Une analyse visuelle des cartes sémantiques obtenues par les réseaux profonds présentés dans le~\cref{chap:2} indique que ce sont ces éléments qui sont généralement incorrectement prédits. Les modèles de terrain permettent donc d'accéder à une information physique complémentaire à celle des images optiques pouvant ainsi renforcer l'exactitude des modèles appris.

Un modèle numérique de terrain se présente comme une image associant à chaque pixel, c'est-à-dire à chaque point associé à des coordonnées géographiques, un scalaire indiquant son élévation. Le référentiel de cette élévation peut varier et n'est pas forcément constant, dans le cas du \gls{MNH} notamment. En normalisant ces données, il est possible de considérer les modèles numériques de terrain comme des images en niveaux de gris. Une première question est donc de savoir comment se comportent des réseaux profonds tels que SegNet pour la segmentation sémantique à partir de ces images.

En première proche, nous considérons les images en niveaux de gris correspondant aux \gls{MNE} et aux \gls{MNH} du jeu de données \glssymbol{ISPRS} Vaihingen. Nous réutilisons les hyperparamètres d'optimisation obtenus lors de la mise en \oe{}uvre d'un SegNet \gls{IRRV} sur ce même jeu de données. L'objectif est de mesurer la quantité d'information présente au sein des modalités dérivées du \gls{Lidar}.

Nous entraînons donc un modèle SegNet à un seul canal sur le \gls{MNE} et le \gls{MNH}. Nous utilisons les tuiles 1, 3, 7, 11, 13, 23, 26, 28, 17, 32, 34 et 37 pour l'apprentissage et les tuiles 5, 15, 21 et 30 pour la validation. Les poids sont initialisés aléatoirement en utilisant la méthode de~\citet{}. Le tableau~\cref{tab:dsm_ndsm_vaihingen} récapitule les scores $F_1$ obtenus pour les cinq classes d'intérêt du jeu de données \glssymbol{ISPRS} Vaihingen ainsi que l'exactitude globale du modèle.

% %%%%% MNE
% 80.29205763767011%
% ---
% F1Score :
% roads: 0.779415312885314
% buildings: 0.9268757846122162
% low veg.: 0.5657090282376125
% trees: 0.8415006794720813
% cars: 0.6059259620940244
%
% %%%%% MNH
% Total accuracy : 80.52758069146536%
% ---
% F1Score :
% roads: 0.7856670825994235
% buildings: 0.9316243288023842
% low veg.: 0.5586461631907925
% trees: 0.8380410367251122
% cars: 0.32288119322143516
% clutter: 0.0
% ---
% Kappa: 0.737016599297
%
% %%%%% MNH + MNE
% 80.30005077025118%
% ---
% F1Score :
% roads: 0.7767342668121028
% buildings: 0.9346511066747522
% low veg.: 0.5592538918958128
% trees: 0.84007892186919
% cars: 0.28390195138792274
% clutter: 0.0
% ---
% Kappa: 0.734327321822

\begin{table}[h]
  \begin{tabularx}{\textwidth}{Y c c c c c c}
    \toprule
    Entrée & Routes & Bâtiments & Vég. basse & Arbres & Véhicules & Exactitude\\
    \midrule
    \gls{MNH} & 78.57 & 93.16 & 55.86 & 83.80 & 32.29 & 80.53\\
    \gls{MNE} & 77.94 & 92.69 & 56.57 & 84.15 & 60.60 & 80.29\\
    \gls{MNH} + \gls{MNE} & 77.67 & 93.47 & 55.93 & 84.01 & 28.39 & 80.30\\
    \bottomrule
  \end{tabularx}
  \caption{Résultats de validation sur le jeu de données \glssymbol{ISPRS} Vaihingen pour un modèle SegNet entraîné sur les \gls{MNE} et \gls{MNH}.}
  \label{tab:dsm_ndsm_vaihingen}
\end{table}

On constate que l'utilisation seule d'un des modèles numériques de terrain permet d'obtenir des scores $F_1$ élevés en inférence pour les routes, les bâtiments et les arbres. En effet, ces classes sont les plus simples à discriminer à partir de l'information de hauteur fournie par le \gls{Lidar}. Les bâtiments présentent des surfaces surélevées régulières planes, les arbres sont des objets hauts à la surface chaotique et le sol est une large surface plane régulière à faible pente. Il est intéressant de constater que le modèle intègre un a priori spatial concernant la répartition de la végétation basse, qu'il place aléatoirement autour des arbres afin de créer des zones végétalisées. En outre, les véhicules sont prédits avec une précision relative à partir du \gls{MNH}. Toutefois, le procédé de normalisation utilisé pour générer le \gls{MNH} aplanit les zones appartenant au sol et les voitures y disparaissent généralement, provoquant une chute catastrophique des performances pour la classe des véhicules.

Néanmoins, malgré ce succès relatif, les modèles numériques de terrains obtiennent une exactitude globale nettement inférieure à celle obtenue à partir des images optiques \gls{IRRV}.

\subsection{Construction d'une image composite}

Comme nous l'avons vu, les modèles numériques de terrain seuls ne sont pas capables de couvrir l'intégralité des classes d'intérêt pour la segmentation sémantique en zone urbaine. En effet, l'information qui y est contenue n'est pas suffisante pour pouvoir distinguer la végétation basse des surfaces imperméables, ni pour distinguer les véhicules dont la hauteur est trop faible pour être détectée de façon robuste à partir du \gls{MNH}.

Couvrir toutes les classes nécessite ainsi une information de hauteur, mais également une information radiométrique. Le \gls{NDVI} est un indice de végétation défini comme le rapport normalisé entre la réponse d'une surface dans le proche infrarouge et sa réponse dans le rouge\,:
\begin{equation}
\mathit{NDVI} = \frac{\mathit{IR} - R}{\mathit{IR} + R}~~.
\eqname{définition du NDVI}
\end{equation}

Le \gls{NDVI} prend des valeurs entre $+1$ et $-1$ indiquant respectivement une présence forte de végétation et une absence complète de végétation. Le \gls{NDVI} fonctionne car il modélise le pic de réponse de la végétation dans le proche-infrarouge et une absorption dans le spectre rouge dûs à la présence de chlorophylle dans le feuillage. Ainsi, le \gls{NDVI} permet de caractériser la présence et la densité de végétation présente sur la surface observée~\cite{myneni_interpretation_1995}. Le \gls{NDVI} permet par ailleurs de caractériser des structures artificielles lorsqu'il est très faible~\cite{sakamoto_automatic_2004}.

On construit donc une image composite à trois canaux à partir du \gls{MNE}, du \gls{MNH} et du \gls{NDVI}, telle qu'illustré dans la~\cref{fig:composite_vaihingen}.
% Comp
% 89.61478814380284%
% F1Score :
% roads: 0.9134480900492845
% buildings: 0.9547855675370014
% low veg.: 0.7646813973826078
% trees: 0.8939003249834194
% cars: 0.7346995377503852
% clutter: 0.0
% ---
% Kappa: 0.859940991225

% Comp from scratch
% 89.06775353659347%
% roads: 0.913852301466021
% buildings: 0.9502188144558582
% low veg.: 0.7568177146600956
% trees: 0.8865529217652657
% cars: 0.6185688317590158
% clutter: 0.0
% ---
% Kappa: 0.852588791607

% IRRV
% 90.46925005692208%
% ---
% F1Score :
% roads: 0.9142738428326874
% buildings: 0.953739495548255
% low veg.: 0.7997408598463142
% trees: 0.9052690509055247
% cars: 0.9040960354251713
% clutter: 0.0
% ---
% Kappa: 0.871723577303

\begin{table}[h]
  \begin{tabularx}{\textwidth}{c Y c c c c c c}
    \toprule
    Entrée & Pré-entraînement & Routes & Bâtiments & Vég. basse & Arbres & Véhicules & Exactitude\\
    \midrule
    Composite & n & 91.39 & 95.02 & 75.68 & 88.66 & 61.86 & 89.07\\
    Composite & o & 91.34 & 95.48 & 76.47 & 89.39 & 73.47 & 89.61\\
    \gls{IRRV}& o & 91.43 & 95.37 & 79.97 & 90.53 & 90.41 & 90.47\\
    \bottomrule
  \end{tabularx}
  \caption{Résultats de validation sur le jeu de données \glssymbol{ISPRS} Vaihingen pour un modèle SegNet entraîné sur les images composites, avec et sans pré-entraînement.}
  \label{tab:composite_vaihingen}
\end{table}

\begin{table}[h]
  \begin{tabularx}{\textwidth}{c Y c c c c c c}
    \toprule
    Entrée & Pré-entraînement & Routes & Bâtiments & Vég. basse & Arbres & Véhicules & Exactitude\\
    \midrule
    Composite & n & 83.56 & 88.38 & 70.38 & 67.70 & 81.89 & 79.35\\
    Composite & o & 88.64 & 94.46 & 78.75 & 81.75 & 91.44 & 86.83\\
    \gls{RVB} & o & 92.35 & 97.62 & 85.18 & 87.19 & 96.11 & 91.22\\
    \bottomrule
  \end{tabularx}
  \caption{Résultats de validation sur le jeu de données \glssymbol{ISPRS} Potsdam pour un modèle SegNet entraîné sur les images composites, avec et sans pré-entraînement.}
  \label{tab:composite_vaihingen}
\end{table}

Le~\cref{tab:composite_vaihingen} détaille les résultats obtenus par un SegNet entraîné sur les images composite \gls{MNE}/\gls{MNH}/\gls{NDVI} du jeu de données ISPRS Vaihingen. Les performances sont nettement supérieures à celles des modèles \gls{MNE}, \gls{MNH} et \gls{NDVI} pris séparément. En outre, la possibilité d'utiliser les poids pré-entraînés de VGG-16 permet de gagner en exactitude sur l'ensemble des classes. Toutefois, la performance globale du modèle n'atteint pas celle de SegNet entraîné directement sur les images \gls{IRRV}.

\begin{figure}[h]
  \foreach\picpath\pictitle in {vaihingen_top_30/Image \gls{IRRV},vaihingen_predirrg_30/Prédiction \gls{IRRV},vaihingen_predcomp_30/Prédiction composite,vaihingen_errors_30/Masque d'erreurs}{%
  \hfill
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[height=\textwidth,angle=90]{\picpath}
    \caption{\pictitle}
  \end{subfigure}
  \hfill
  }%
  \caption{En \textcolor{LimeGreen}{vert clair} les erreurs du modèle entraîné en composite, en \textcolor{ForestGreen}{vert foncé} les erreurs du modèle entraîné en \gls{IRRV} et en \textcolor{Goldenrod}{jaune} l'intersection des deux masques.\\
  \isprslegende}
  \label{fig:vaihingen_errors}
\end{figure}

La~\cref{fig:vaihingen_errors} illustre les prédictions obtenues via SegNet entraîné respectivement sur les données \gls{IRRV} et sur les données composites. Le premier ne contient que 12\% de pixels erronés tandis que le second est à environ 13\% d'erreur. Cependant, il est remarquable que ces erreurs sont complémentaires, c'est-à-dire qu'elles ne se produisent pas sur les mêmes pixels. En effet, chaque modalité renseigne le modèle de différente façon. Si nous étions capables de fusionner parfaitement les deux cartes, de telle sorte que seuls les pixels qui sont classifiés de façon erronée dans les deux modèles soit en échec, alors le taux d'erreur tomberait à 7\% sur cette image (masque jaune). Nous avons donc tout intérêt à étudier les possibilités d'apprentissage multi-modales offertes par les réseaux profonds. C'est l'objet du chapitre suivant.

%\bibliographystyle{plainnat}
%\bibliography{Chapitre3/Biblio}
\printbibliography
