
@inproceedings{audebert_semantic_2016,
  title = {Semantic {{Segmentation}} of {{Earth Observation Data Using Multimodal}} and {{Multi}}-Scale {{Deep Networks}}},
  doi = {10.1007/978-3-319-54181-5_12},
  abstract = {This work investigates the use of deep fully convolutional neural networks (DFCNN) for pixel-wise scene labeling of Earth Observation images. Especially, we train a variant of the SegNet architecture on remote sensing data over an urban area and study different strategies for performing accurate semantic segmentation. Our contributions are the following: (1) we transfer efficiently a DFCNN from generic everyday images to remote sensing images; (2) we introduce a multi-kernel convolutional layer for fast aggregation of predictions at multiple scales; (3) we perform data fusion from heterogeneous sensors (optical and laser) using residual correction. Our framework improves state-of-the-art accuracy on the ISPRS Vaihingen 2D Semantic Labeling dataset.},
  eventtitle = {Asian {{Conference}} on {{Computer Vision}}},
  booktitle = {Computer {{Vision}} – {{ACCV}} 2016},
  publisher = {{Springer, Cham}},
  date = {2016-11-20},
  pages = {180-196},
  author = {Audebert, Nicolas and Le Saux, Bertrand and Lefèvre, Sébastien},
  file = {/home/naudeber/Bibliographie//Springer, Cham/2016/Audebert et al 2016 - Semantic Segmentation of Earth Observation Data Using Multimodal and.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/394SFXNP/Audebert et al_2016_Semantic Segmentation of Earth Observation Data Using Multimodal and.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/PC7RWZAX/978-3-319-54181-5_12.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/TJHU6H38/978-3-319-54181-5_12.html}
}

@article{simonyan_very_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.1556},
  primaryClass = {cs},
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  url = {http://arxiv.org/abs/1409.1556},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  urldate = {2015-11-18},
  date = {2014-09-04},
  keywords = {À lire,Computer Science - Computer Vision and Pattern Recognition},
  author = {Simonyan, Karen and Zisserman, Andrew},
  file = {/home/naudeber/Bibliographie//arXiv1409.1556 [cs]/2014/Simonyan Zisserman 2014 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8VM3BPQT/1409.html}
}

@inproceedings{kampffmeyer_semantic_2016,
  title = {Semantic {{Segmentation}} of {{Small Objects}} and {{Modeling}} of {{Uncertainty}} in {{Urban Remote Sensing Images Using Deep Convolutional Neural Networks}}},
  url = {http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w19/html/Kampffmeyer_Semantic_Segmentation_of_CVPR_2016_paper.html},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  urldate = {2016-07-18},
  date = {2016},
  pages = {1--9},
  author = {Kampffmeyer, Michael and Salberg, Arnt-Borre and Jenssen, Robert},
  file = {/home/naudeber/Bibliographie//undefined/2016/Kampffmeyer et al 2016 - Semantic Segmentation of Small Objects and Modeling of Uncertainty in Urban.pdf}
}

@inproceedings{ngiam_multimodal_2011,
  title = {Multimodal Deep Learning},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Ngiam_399.pdf},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning ({{ICML}}-11)},
  urldate = {2016-03-15},
  date = {2011},
  pages = {689--696},
  author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y.},
  file = {/home/naudeber/Bibliographie//undefined/2011/Ngiam et al 2011 - Multimodal deep learning_3.pdf;/home/naudeber/Bibliographie//undefined/2011/Ngiam et al 2011 - Multimodal deep learning_4.pdf}
}

@article{marmanis_classification_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.01337},
  title = {Classification {{With}} an {{Edge}}: {{Improving Semantic Image Segmentation}} with {{Boundary Detection}}},
  url = {http://arxiv.org/abs/1612.01337},
  shorttitle = {Classification {{With}} an {{Edge}}},
  abstract = {We present an end-to-end trainable deep convolutional neural network (DCNN) for semantic segmentation with built-in awareness of semantically meaningful boundaries. Semantic segmentation is a fundamental remote sensing task, and most state-of-the-art methods rely on DCNNs as their workhorse. A major reason for their success is that deep networks learn to accumulate contextual information over very large windows (receptive fields). However, this success comes at a cost, since the associated loss of effecive spatial resolution washes out high-frequency details and leads to blurry object boundaries. Here, we propose to counter this effect by combining semantic segmentation with semantically informed edge detection, thus making class-boundaries explicit in the model, First, we construct a comparatively simple, memory-efficient model by adding boundary detection to the Segnet encoder-decoder architecture. Second, we also include boundary detection in FCN-type models and set up a high-end classifier ensemble. We show that boundary detection significantly improves semantic segmentation with CNNs. Our high-end ensemble achieves $>$ 90\% overall accuracy on the ISPRS Vaihingen benchmark.},
  journaltitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
  urldate = {2016-12-21},
  date = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Marmanis, Dimitrios and Schindler, Konrad and Wegner, Jan Dirk and Galliani, Silvano and Datcu, Mihai and Stilla, Uwe},
  file = {/home/naudeber/Bibliographie//arXiv1612.01337 [cs]/2016/Marmanis et al 2016 - Classification With an Edge.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/XBNT2Q32/Marmanis et al_2016_Classification With an Edge.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VEPZ68S6/1612.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/Z3Z4WVNI/1612.html}
}

@article{campos-taberner_processing_2016,
  title = {Processing of {{Extremely High}}-{{Resolution LiDAR}} and {{RGB Data}}: {{Outcome}} of the 2015 {{IEEE GRSS Data Fusion Contest Part A}}: 2-{{D Contest}}},
  volume = {9},
  issn = {1939-1404},
  doi = {10.1109/JSTARS.2016.2569162},
  shorttitle = {Processing of {{Extremely High}}-{{Resolution LiDAR}} and {{RGB Data}}},
  abstract = {In this paper, we discuss the scientific outcomes of the 2015 data fusion contest organized by the Image Analysis and Data Fusion Technical Committee (IADF TC) of the IEEE Geoscience and Remote Sensing Society (IEEE GRSS). As for previous years, the IADF TC organized a data fusion contest aiming at fostering new ideas and solutions for multisource studies. The 2015 edition of the contest proposed a multiresolution and multisensorial challenge involving extremely high-resolution RGB images and a three-dimensional (3-D) LiDAR point cloud. The competition was framed in two parallel tracks, considering 2-D and 3-D products, respectively. In this paper, we discuss the scientific results obtained by the winners of the 2-D contest, which studied either the complementarity of RGB and LiDAR with deep neural networks (winning team) or provided a comprehensive benchmarking evaluation of new classification strategies for extremely high-resolution multimodal data (runner-up team). The data and the previously undisclosed ground truth will remain available for the community and can be obtained at http://www.grss-ieee.org/community/technical-committees/data-fusion/2015-ieee-grss-data-fusion-contest/. The 3-D part of the contest is discussed in the Part-B paper [1].},
  number = {12},
  journaltitle = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  date = {2016-12},
  pages = {5547-5559},
  keywords = {Data integration,deep neural networks,Earth,extremely high spatial resolution,image analysis and data fusion (IADF),landcover classification,Laser radar,LiDAR,multimodal-data fusion,multiresolution-,multisource-,remote sensing,Spatial resolution,Three-dimensional displays},
  author = {Campos-Taberner, Manuel and Romero-Soriano, Adriana and Gatta, Carlo and Camps-Valls, Gustau and Lagrange, Adrien and Le Saux, Bertrand and Beaupère, Anne and Boulch, Alexandre and Chan-Hon-Tong, Adrien and Herbin, Stéphane and Randrianarivo, Hicham and Ferecatu, Marin and Shimoni, Michal and Moser, Gabriele and Tuia, Devis},
  file = {/home/naudeber/Bibliographie//IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing/2016/Campos-Taberner et al 2016 - Processing of Extremely High-Resolution LiDAR and RGB Data.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FC7U9666/articleDetails.html}
}

@article{sherrah_fully_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.02585},
  primaryClass = {cs},
  title = {Fully {{Convolutional Networks}} for {{Dense Semantic Labelling}} of {{High}}-{{Resolution Aerial Imagery}}},
  url = {http://arxiv.org/abs/1606.02585},
  abstract = {The trend towards higher resolution remote sensing imagery facilitates a transition from land-use classification to object-level scene understanding. Rather than relying purely on spectral content, appearance-based image features come into play. In this work, deep convolutional neural networks (CNNs) are applied to semantic labelling of high-resolution remote sensing data. Recent advances in fully convolutional networks (FCNs) are adapted to overhead data and shown to be as effective as in other domains. A full-resolution labelling is inferred using a deep FCN with no downsampling, obviating the need for deconvolution or interpolation. To make better use of image features, a pre-trained CNN is fine-tuned on remote sensing data in a hybrid network context, resulting in superior results compared to a network trained from scratch. The proposed approach is applied to the problem of labelling high-resolution aerial imagery, where fine boundary detail is important. The dense labelling yields state-of-the-art accuracy for the ISPRS Vaihingen and Potsdam benchmark data sets.},
  urldate = {2016-09-26},
  date = {2016-06-08},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Sherrah, Jamie},
  file = {/home/naudeber/Bibliographie//arXiv1606.02585 [cs]/2016/Sherrah 2016 - Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/R8U6HVPF/1606.html}
}

@inproceedings{hoffman_learning_2016,
  title = {Learning with Side Information through Modality Hallucination},
  url = {http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Hoffman_Learning_With_Side_CVPR_2016_paper.html},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  urldate = {2017-02-23},
  date = {2016},
  pages = {826--834},
  author = {Hoffman, Judy and Gupta, Saurabh and Darrell, Trevor},
  file = {/home/naudeber/Bibliographie//undefined/2016/Hoffman et al 2016 - Learning with side information through modality hallucination.pdf}
}

@inproceedings{guo_two-stream_2016,
  title = {Two-Stream Convolutional Neural Network for Accurate {{RGB}}-{{D}} Fingertip Detection Using Depth and Edge Information},
  url = {http://ieeexplore.ieee.org/abstract/document/7532831/},
  booktitle = {Image {{Processing}} ({{ICIP}}), 2016 {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  urldate = {2017-02-24},
  date = {2016},
  pages = {2608--2612},
  author = {Guo, Hengkai and Wang, Guijin and Chen, Xinghao},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WUGHGHK8/1612.07978.pdf}
}

@article{lin_refinenet_2016,
  title = {{{RefineNet}}: {{Multi}}-{{Path Refinement Networks}} with {{Identity Mappings}} for {{High}}-{{Resolution Semantic Segmentation}}},
  url = {https://arxiv.org/abs/1611.06612},
  shorttitle = {{{RefineNet}}},
  journaltitle = {arXiv preprint arXiv:1611.06612},
  urldate = {2017-02-24},
  date = {2016},
  author = {Lin, Guosheng and Milan, Anton and Shen, Chunhua and Reid, Ian},
  file = {/home/naudeber/Bibliographie//arXiv preprint arXiv1611.06612/2016/Lin et al 2016 - RefineNet.pdf}
}

@inproceedings{vakalopoulou_simultaneous_2016,
  langid = {english},
  title = {Simultaneous Registration, Segmentation and Change Detection from Multisensor, Multitemporal Satellite Image Pairs.},
  url = {https://hal.inria.fr/hal-01413373/document},
  doi = {10.1109/IGARSS.2016.7729469},
  abstract = {In this paper, a novel generic framework has been designed, developed and validated for addressing simultaneously the tasks of image registration, segmentation and change detection from multisensor, multiresolution, multitemporal satel- lite image pairs. Our approach models the inter-dependencies of variables through a higher order graph. The proposed formulation is modular with respect to the nature of images (various similarity metrics can be considered), the nature of deformations (arbitrary interpolation strategies), and the nature of segmentation likelihoods (various classification approaches can be employed). Inference of the proposed formulation is achieved through its mapping to an overparametrized pairwise graph which is then optimized using linear programming. Experimental results and the performed quantitative evaluation indicate the high potentials of the developed method.},
  eventtitle = {International {{Geoscience}} and {{Remote Sensing Symposium}} ({{IGARSS}})},
  urldate = {2017-03-22},
  date = {2016-07-10},
  author = {Vakalopoulou, Maria and Platias, Christos and Papadomanolaki, Maria and Paragios, Nikos and Karantzalos, Konstantinos},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/3CV2N9W3/Vakalopoulou et al_2016_Simultaneous registration, segmentation and change detection from multisensor,.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9VZP76FX/hal-01413373.html}
}

@article{isola_image--image_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.07004},
  primaryClass = {cs},
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  url = {http://arxiv.org/abs/1611.07004},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  urldate = {2017-03-22},
  date = {2016-11-21},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/U5VVQAVV/Isola et al_2016_Image-to-Image Translation with Conditional Adversarial Networks.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/QGEK5G7R/1611.html}
}

@article{geis_joint_2017,
  langid = {english},
  title = {Joint Use of Remote Sensing Data and Volunteered Geographic Information for Exposure Estimation: Evidence from {{Valparaíso}}, {{Chile}}},
  volume = {86},
  issn = {0921-030X, 1573-0840},
  url = {https://link.springer.com/article/10.1007/s11069-016-2663-8},
  doi = {10.1007/s11069-016-2663-8},
  shorttitle = {Joint Use of Remote Sensing Data and Volunteered Geographic Information for Exposure Estimation},
  abstract = {The impact of natural hazards on mankind has increased dramatically over the past decades. Global urbanization processes and increasing spatial concentrations of exposed elements induce natural hazard risk at a uniquely high level. To mitigate affiliated perils requires detailed knowledge about elements at risk. Considering a high spatiotemporal variability of elements at risk, detailed information is costly in terms of both time and economic resources and therefore often incomplete, aggregated, or outdated. To alleviate these restrictions, the availability of very-high-resolution satellite images promotes accurate and detailed analysis of exposure over various spatial scales with large-area coverage. In the past, valuable approaches were proposed; however, the design of information extraction procedures with a high level of automatization remains challenging. In this paper, we uniquely combine remote sensing data and volunteered geographic information from the OpenStreetMap project (OSM) (i.e., freely accessible geospatial information compiled by volunteers) for a highly automated estimation of crucial exposure components (i.e., number of buildings and population) with a high level of spatial detail. To this purpose, we first obtain labeled training segments from the OSM data in conjunction with the satellite imagery. This allows for learning a supervised algorithmic model (i.e., rotation forest) in order to extract relevant thematic classes of land use/land cover (LULC) from the satellite imagery. Extracted information is jointly deployed with information from the OSM data to estimate the number of buildings with regression techniques (i.e., a multi-linear model from ordinary least-square optimization and a nonlinear support vector regression model are considered). Analogously, urban LULC information is used in conjunction with OSM data to spatially disaggregate population information. Experimental results were obtained for the city of Valparaíso in Chile. Thereby, we demonstrate the relevance of the approaches by estimating number of affected buildings and population referring to a historical tsunami event.},
  number = {1},
  journaltitle = {Natural Hazards},
  shortjournal = {Nat Hazards},
  urldate = {2017-03-24},
  date = {2017-03-01},
  pages = {81-105},
  author = {Geiß, Christian and Schauß, Anne and Riedlinger, Torsten and Dech, Stefan and Zelaya, Cecilia and Guzmán, Nicolás and Hube, Mathías A. and Arsanjani, Jamal Jokar and Taubenböck, Hannes},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/MMPPAVA3/Geiß et al_2017_Joint use of remote sensing data and volunteered geographic information for.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/RR2EN2BX/10.html}
}

@thesis{mnih_machine_2013,
  title = {Machine {{Learning}} for {{Aerial Image Labeling}}},
  institution = {{University of Toronto}},
  date = {2013},
  author = {Mnih, Volodymyr}
}

@inproceedings{chen_deepvgi_2017,
  title = {{{DeepVGI}}: {{Deep Learning}} with {{Volunteered Geographic Information}}},
  booktitle = {26th {{International World Wide Web Conference}} ({{Poster}})},
  publisher = {{ACM}},
  date = {2017},
  author = {Chen, Jiaoyan and Zipf, Alexander},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HFEGZFRP/Chen_Zipf_2017_DeepVGI.pdf}
}

@article{danylo_contributing_2016,
  title = {Contributing to {{WUDAPT}}: {{A Local Climate Zone Classification}} of {{Two Cities}} in {{Ukraine}}},
  volume = {9},
  issn = {1939-1404, 2151-1535},
  url = {http://ieeexplore.ieee.org/document/7447735/},
  doi = {10.1109/JSTARS.2016.2539977},
  shorttitle = {Contributing to {{WUDAPT}}},
  number = {5},
  journaltitle = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  urldate = {2017-03-24},
  date = {2016-05},
  pages = {1841-1853},
  author = {Danylo, Olha and See, Linda and Bechtel, Benjamin and Schepaschenko, Dmitry and Fritz, Steffen},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/RRJ9VR2P/Danylo et al_2016_Contributing to WUDAPT.pdf}
}

@inproceedings{hazirbas_fusenet_2016,
  langid = {english},
  title = {{{FuseNet}}: {{Incorporating Depth}} into {{Semantic Segmentation}} via {{Fusion}}-{{Based CNN Architecture}}},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-54181-5_14},
  doi = {10.1007/978-3-319-54181-5_14},
  shorttitle = {{{FuseNet}}},
  abstract = {In this paper we address the problem of semantic labeling of indoor scenes on RGB-D data. With the availability of RGB-D cameras, it is expected that additional depth measurement will improve the accuracy. Here we investigate a solution how to incorporate complementary depth information into a semantic segmentation framework by making use of convolutional neural networks (CNNs). Recently encoder-decoder type fully convolutional CNN architectures have achieved a great success in the field of semantic segmentation. Motivated by this observation we propose an encoder-decoder type network, where the encoder part is composed of two branches of networks that simultaneously extract features from RGB and depth images and fuse depth features into the RGB feature maps as the network goes deeper. Comprehensive experimental evaluations demonstrate that the proposed fusion-based architecture achieves competitive results with the state-of-the-art methods on the challenging SUN RGB-D benchmark obtaining 76.27\% global accuracy, 48.30\% average class accuracy and 37.29\% average intersection-over-union score.},
  eventtitle = {Asian {{Conference}} on {{Computer Vision}}},
  booktitle = {Computer {{Vision}} – {{ACCV}} 2016},
  publisher = {{Springer, Cham}},
  urldate = {2017-03-31},
  date = {2016-11-20},
  pages = {213-228},
  author = {Hazirbas, Caner and Ma, Lingni and Domokos, Csaba and Cremers, Daniel},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/CBE66U65/Hazirbas et al_2016_FuseNet.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/KCPWN6WW/978-3-319-54181-5_14.html}
}

@inproceedings{he_deep_2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  eventtitle = {{{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  date = {2016-06},
  pages = {770-778},
  keywords = {neural nets,object detection,learning (artificial intelligence),Image recognition,Visualization,Neural networks,Training,image classification,image segmentation,CIFAR-10,COCO object detection dataset,COCO segmentation,ILSVRC & COCO 2015 competitions,ILSVRC 2015 classification task,ImageNet dataset,ImageNet localization,ImageNet test set,VGG nets,deep residual learning,deep residual nets,deeper neural network training,residual function learning,residual nets,visual recognition tasks,Complexity theory,Degradation},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/KUCF9Q86/7780459.html}
}

@inproceedings{eitel_multimodal_2015,
  title = {Multimodal Deep Learning for Robust {{RGB}}-{{D}} Object Recognition},
  doi = {10.1109/IROS.2015.7353446},
  abstract = {Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams - one for each modality - which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset [15] and show recognition in challenging RGB-D real-world noisy settings.},
  eventtitle = {2015 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  booktitle = {2015 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  date = {2015-09},
  pages = {681-687},
  keywords = {learning (artificial intelligence),image colour analysis,object recognition,convolutional neural networks,Training,feature extraction,feedforward neural nets,image fusion,robot vision,CNN,RGB-D architecture,RGB-D object dataset,RGB-D real-world noisy settings,accurate learning,data augmentation scheme,fusion network,imperfect sensor data,multimodal deep learning,multistage training methodology,real-world robotics applications,real-world robotics tasks,realistic noise patterns,robust RGB-D object recognition,robust learning,Image coding,Robot sensing systems,Robustness,Streaming media},
  author = {Eitel, Andreas and Springenberg, Jost Tobias and Spinello, Luciano and Riedmiller, Martin and Burgard, Wolfram},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/RDXQ8H3F/Eitel et al_2015_Multimodal deep learning for robust RGB-D object recognition.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/Q6868ZET/7353446.html}
}

@inproceedings{paisitkriangkrai_effective_2015,
  title = {Effective Semantic Pixel Labelling with Convolutional Networks and {{Conditional Random Fields}}},
  doi = {10.1109/CVPRW.2015.7301381},
  abstract = {Large amounts of available training data and increasing computing power have led to the recent success of deep convolutional neural networks (CNN) on a large number of applications. In this paper, we propose an effective semantic pixel labelling using CNN features, hand-crafted features and Conditional Random Fields (CRFs). Both CNN and hand-crafted features are applied to dense image patches to produce per-pixel class probabilities. The CRF infers a labelling that smooths regions while respecting the edges present in the imagery. The method is applied to the ISPRS 2D semantic labelling challenge dataset with competitive classification accuracy.},
  eventtitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  date = {2015-06},
  pages = {36-43},
  keywords = {Accuracy,classification accuracy,CNN features,computing power,conditional random fields,Convolutional networks,CRF,deep-convolutional neural networks,dense-image patches,edge detection,feature extraction,hand-crafted features,image classification,Image edge detection,image region smoothing,ISPRS 2D semantic labelling challenge dataset,Labeling,neural nets,per-pixel class probabilities,probability,semantic pixel labelling,Semantics,smoothing methods,Training,training data,Visualization},
  author = {Paisitkriangkrai, Sakrapee and Sherrah, Jamie and Janney, Pranam and Van Den Hengel, Anton},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/5WKQRPFF/7301381.html}
}

@inproceedings{lagrange_benchmarking_2015,
  title = {Benchmarking Classification of Earth-Observation Data: {{From}} Learning Explicit Features to Convolutional Networks},
  doi = {10.1109/IGARSS.2015.7326745},
  shorttitle = {Benchmarking Classification of Earth-Observation Data},
  abstract = {In this paper, we address the task of semantic labeling of multisource earth-observation (EO) data. Precisely, we benchmark several concurrent methods of the last 15 years, from expert classifiers, spectral support-vector classification and high-level features to deep neural networks. We establish that (1) combining multisensor features is essential for retrieving some specific classes, (2) in the image domain, deep convolutional networks obtain significantly better overall performances and (3) transfer of learning from large generic-purpose image sets is highly effective to build EO data classifiers.},
  eventtitle = {2015 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}} ({{IGARSS}})},
  booktitle = {2015 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}} ({{IGARSS}})},
  date = {2015-07},
  pages = {4173-4176},
  keywords = {Buildings,deep convolutional networks,deep neural networks,expert classifiers,feature extraction,generic-purpose image sets,geophysical image processing,high-level features,image classification,image domain,Laser radar,learning explicit features,multisensor features,multisource earth-observation data benchmarking classification,neural nets,Neural networks,Pattern analysis,remote sensing,semantic labeling,Semantics,spectral support-vector classification,support vector machines,terrain mapping},
  author = {Lagrange, Adrien and Le Saux, Bertrand and Beaupère, Anne and Boulch, Alexandre and Chan-Hon-Tong, Adrien and Herbin, Stéphane and Randrianarivo, Hicham and Ferecatu, Marin},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WXE5EVSM/7326745.html}
}

@article{badrinarayanan_segnet_2017,
  title = {{{SegNet}}: {{A Deep Convolutional Encoder}}-{{Decoder Architecture}} for {{Scene Segmentation}}},
  volume = {39},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2016.2644615},
  shorttitle = {{{SegNet}}},
  abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.a- .uk/projects/segnet/.},
  number = {12},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  date = {2017-12},
  pages = {2481-2495},
  keywords = {Computer architecture,Decoder,Decoding,Deep Convolutional Neural Networks,Encoder,image segmentation,Indoor Scenes,Neural networks,Pooling,Road Scenes,Roads,Semantic Pixel-Wise Segmentation,Semantics,Training,Upsampling},
  author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9SZV73FK/7803544.html}
}

@article{yuhas_integration_1989,
  title = {Integration of Acoustic and Visual Speech Signals Using Neural Networks},
  volume = {27},
  number = {11},
  journaltitle = {IEEE Communications Magazine},
  date = {1989},
  pages = {65--71},
  author = {Yuhas, Ben P. and Goldstein, Moise H. and Sejnowski, Terrence J.},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/D4J4GP3N/00041402.pdf}
}

@incollection{schuller_avec_2011,
  langid = {english},
  title = {{{AVEC}} 2011–{{The First International Audio}}/{{Visual Emotion Challenge}}},
  isbn = {978-3-642-24570-1 978-3-642-24571-8},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-24571-8_53},
  abstract = {The Audio/Visual Emotion Challenge and Workshop (AVEC 2011) is the first competition event aimed at comparison of multimedia processing and machine learning methods for automatic audio, visual and audiovisual emotion analysis, with all participants competing under strictly the same conditions. This paper first describes the challenge participation conditions. Next follows the data used – the SEMAINE corpus – and its partitioning into train, development, and test partitions for the challenge with labelling in four dimensions, namely activity, expectation, power, and valence. Further, audio and video baseline features are introduced as well as baseline results that use these features for the three sub-challenges of audio, video, and audiovisual emotion recognition.},
  booktitle = {Affective {{Computing}} and {{Intelligent Interaction}}},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-04-09},
  date = {2011},
  pages = {415-424},
  author = {Schuller, Björn and Valstar, Michel and Eyben, Florian and McKeown, Gary and Cowie, Roddy and Pantic, Maja},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/KZ64BGMW/Schuller et al. - 2011 - AVEC 2011–The First International AudioVisual Emo.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9HJPZQ79/978-3-642-24571-8_53.html},
  doi = {10.1007/978-3-642-24571-8_53}
}

@article{hodosh_framing_2013,
  title = {Framing {{Image Description As}} a {{Ranking Task}}: {{Data}}, {{Models}} and {{Evaluation Metrics}}},
  volume = {47},
  issn = {1076-9757},
  url = {http://dl.acm.org/citation.cfm?id=2566972.2566993},
  shorttitle = {Framing {{Image Description As}} a {{Ranking Task}}},
  abstract = {The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. Our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. We also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. Our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated.},
  number = {1},
  journaltitle = {J. Artif. Int. Res.},
  urldate = {2018-04-09},
  date = {2013-05},
  pages = {853--899},
  author = {Hodosh, Micah and Young, Peter and Hockenmaier, Julia}
}

@article{srivastava_multimodal_2014,
  title = {Multimodal {{Learning}} with {{Deep Boltzmann Machines}}},
  volume = {15},
  issn = {1532-4435},
  url = {http://dl.acm.org/citation.cfm?id=2627435.2697059},
  abstract = {Data often consists of multiple diverse modalities. For example, images are tagged with textual information and videos are accompanied by audio. Each modality is characterized by having distinct statistical properties. We propose a Deep Boltzmann Machine for learning a generative model of such multimodal data. We show that the model can be used to create fused representations by combining features across modalities. These learned representations are useful for classification and information retrieval. By sampling from the conditional distributions over each data modality, it is possible to create these representations even when some data modalities are missing. We conduct experiments on bimodal image-text and audio-video data. The fused representation achieves good classification results on the MIR-Flickr data set matching or outperforming other deep models as well as SVM based models that use Multiple Kernel Learning. We further demonstrate that this multimodal model helps classification and retrieval even when only unimodal data is available at test time.},
  number = {1},
  journaltitle = {J. Mach. Learn. Res.},
  urldate = {2018-04-09},
  date = {2014-01},
  pages = {2949--2980},
  keywords = {deep learning,Boltzmann machines,multimodal learning,neural networks,unsupervised learning},
  author = {Srivastava, Nitish and Salakhutdinov, Ruslan},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/2HSJUPMP/Srivastava et Salakhutdinov - 2014 - Multimodal Learning with Deep Boltzmann Machines.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/XZDPVC2T/Srivastava et Salakhutdinov - 2014 - Multimodal Learning with Deep Boltzmann Machines.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/QMDW49FZ/srivastava14b.html}
}

@article{atrey_multimodal_2010,
  langid = {english},
  title = {Multimodal Fusion for Multimedia Analysis: A Survey},
  volume = {16},
  issn = {0942-4962, 1432-1882},
  url = {https://link.springer.com/article/10.1007/s00530-010-0182-0},
  doi = {10.1007/s00530-010-0182-0},
  shorttitle = {Multimodal Fusion for Multimedia Analysis},
  abstract = {This survey aims at providing multimedia researchers with a state-of-the-art overview of fusion strategies, which are used for combining multiple modalities in order to accomplish various multimedia analysis tasks. The existing literature on multimodal fusion research is presented through several classifications based on the fusion methodology and the level of fusion (feature, decision, and hybrid). The fusion methods are described from the perspective of the basic concept, advantages, weaknesses, and their usage in various analysis tasks as reported in the literature. Moreover, several distinctive issues that influence a multimodal fusion process such as, the use of correlation and independence, confidence level, contextual information, synchronization between different modalities, and the optimal modality selection are also highlighted. Finally, we present the open issues for further research in the area of multimodal fusion.},
  number = {6},
  journaltitle = {Multimedia Systems},
  shortjournal = {Multimedia Systems},
  urldate = {2018-04-12},
  date = {2010-11-01},
  pages = {345-379},
  author = {Atrey, Pradeep K. and Hossain, M. Anwar and Saddik, Abdulmotaleb El and Kankanhalli, Mohan S.},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WVMBTGT2/Atrey et al. - 2010 - Multimodal fusion for multimedia analysis a surve.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HS4FVFEE/s00530-010-0182-0.html}
}

@article{neverova_moddrop_2016,
  title = {{{ModDrop}}: Adaptive Multi-Modal Gesture Recognition},
  url = {https://hal.archives-ouvertes.fr/hal-01178733},
  shorttitle = {{{ModDrop}}},
  abstract = {We present a method for gesture detection and localisation based on multi-scale and multi-modal deep learning. Each visual modality captures spatial information at a particular spatial scale (such as motion of the upper body or a hand), and the whole system operates at three temporal scales. Key to our technique is a training strategy which exploits: i) careful initialization of individual modalities; and ii) gradual fusion involving random dropping of separate channels (dubbed "ModDrop") for learning cross-modality correlations while preserving uniqueness of each modality-specific representation. We present experiments on the ChaLearn 2014 Looking at People Challenge gesture recognition track, in which we placed first out of 17 teams. Fusing multiple modalities at several spatial and temporal scales leads to a significant increase in recognition rates, allowing the model to compensate for errors of the individual classifiers as well as noise in the separate channels. Futhermore, the proposed ModDrop training technique ensures robustness of the classifier to missing signals in one or several channels to produce meaningful predictions from any number of available modalities. In addition, we demonstrate the applicability of the proposed fusion scheme to modalities of arbitrary nature by experiments on the same dataset augmented with audio.},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  urldate = {2018-04-12},
  date = {2016-04},
  pages = {to appear},
  author = {Neverova, Natalia and Wolf, Christian and Taylor, Graham W. and Nebout, Florian},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/IZMQKQK7/Neverova et al. - 2016 - ModDrop adaptive multi-modal gesture recognition.pdf}
}

@inproceedings{valada_deep_2016,
  langid = {english},
  title = {Deep {{Multispectral Semantic Scene Understanding}} of {{Forested Environments Using Multimodal Fusion}}},
  isbn = {978-3-319-50114-7 978-3-319-50115-4},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-50115-4_41},
  doi = {10.1007/978-3-319-50115-4_41},
  abstract = {Semantic scene understanding of unstructured environments is a highly challenging task for robots operating in the real world. Deep Convolutional Neural Network architectures define the state of the art in various segmentation tasks. So far, researchers have focused on segmentation with RGB data. In this paper, we study the use of multispectral and multimodal images for semantic segmentation and develop fusion architectures that learn from RGB, Near-InfraRed channels, and depth data. We introduce a first-of-its-kind multispectral segmentation benchmark that contains 15, 000 images and 366 pixel-wise ground truth annotations of unstructured forest environments. We identify new data augmentation strategies that enable training of very deep models using relatively small datasets. We show that our UpNet architecture exceeds the state of the art both qualitatively and quantitatively on our benchmark. In addition, we present experimental results for segmentation under challenging real-world conditions. Benchmark and demo are publicly available at http://deepscene.cs.uni-freiburg.de.},
  eventtitle = {International {{Symposium}} on {{Experimental Robotics}}},
  booktitle = {2016 {{International Symposium}} on {{Experimental Robotics}}},
  series = {Springer Proceedings in Advanced Robotics},
  publisher = {{Springer, Cham}},
  urldate = {2018-04-12},
  date = {2016-10-03},
  pages = {465-477},
  author = {Valada, Abhinav and Oliveira, Gabriel L. and Brox, Thomas and Burgard, Wolfram},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/LJKA6IUP/978-3-319-50115-4_41.html}
}

@inproceedings{wu_online_2013,
  location = {{New York, NY, USA}},
  title = {Online {{Multimodal Deep Similarity Learning}} with {{Application}} to {{Image Retrieval}}},
  isbn = {978-1-4503-2404-5},
  url = {http://doi.acm.org/10.1145/2502081.2502112},
  doi = {10.1145/2502081.2502112},
  abstract = {Recent years have witnessed extensive studies on distance metric learning (DML) for improving similarity search in multimedia information retrieval tasks. Despite their successes, most existing DML methods suffer from two critical limitations: (i) they typically attempt to learn a linear distance function on the input feature space, in which the assumption of linearity limits their capacity of measuring the similarity on complex patterns in real-world applications; (ii) they are often designed for learning distance metrics on uni-modal data, which may not effectively handle the similarity measures for multimedia objects with multimodal representations. To address these limitations, in this paper, we propose a novel framework of online multimodal deep similarity learning (OMDSL), which aims to optimally integrate multiple deep neural networks pretrained with stacked denoising autoencoder. In particular, the proposed framework explores a unified two-stage online learning scheme that consists of (i) learning a flexible nonlinear transformation function for each individual modality, and (ii) learning to find the optimal combination of multiple diverse modalities simultaneously in a coherent process. We conduct an extensive set of experiments to evaluate the performance of the proposed algorithms for multimodal image retrieval tasks, in which the encouraging results validate the effectiveness of the proposed technique.},
  booktitle = {Proceedings of the 21st {{ACM International Conference}} on {{Multimedia}}},
  series = {MM '13},
  publisher = {{ACM}},
  urldate = {2018-04-12},
  date = {2013},
  pages = {153--162},
  keywords = {image retrieval,deep learning,distance metric learning,online learning,similarity learning},
  author = {Wu, Pengcheng and Hoi, Steven C.H. and Xia, Hao and Zhao, Peilin and Wang, Dayong and Miao, Chunyan},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/922XFWRD/Wu et al. - 2013 - Online Multimodal Deep Similarity Learning with Ap.pdf}
}

@inproceedings{li_modout_2017,
  location = {{Washington D.C., United States}},
  title = {Modout: {{Learning}} to {{Fuse Face}} and {{Gesture Modalities}} with {{Stochastic Regularization}}},
  url = {https://hal.archives-ouvertes.fr/hal-01444614},
  shorttitle = {Modout},
  abstract = {Model selection methods based on stochastic regularization such as
  Dropout have been widely used in deep learning due to their
  simplicity and effectiveness. The standard Dropout method treats all
  units, visible or hidden, in the same way, thus ignoring any $\backslash$emph\{a
    priori\} information related to grouping or structure. Such
  structure is present in multi-modal learning applications such as
  affect analysis and gesture recognition, where
  subsets of units may correspond to individual modalities. In this
  paper we describe Modout, a model selection method based on
  stochastic regularization, which is particularly useful in the
  multi-modal setting. Different from previous methods, it is capable
  of learning whether or when to fuse two modalities in a layer, which
  is usually considered to be an architectural hyper-parameter by deep
  learning researchers and practitioners. Modout is evaluated on one
  synthetic and two real multi-modal datasets. 
  The results indicate improved performance compared to other
  stochastic regularization methods. The result on the Montalbano
  dataset shows that learning a fusion structure by Modout is on par
  with a state-of-the-art carefully designed architecture.},
  booktitle = {International {{Conference}} on {{Automatic Face}} and {{Gesture Recognition}}},
  urldate = {2018-04-12},
  date = {2017-05},
  keywords = {deep learning,gesture recognition},
  author = {Li, Fan and Neverova, Natalia and Wolf, Christian and Taylor, Graham W.},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/RYPSSMCL/hal-01444614.html}
}

@article{menze_multimodal_2015,
  title = {The {{Multimodal Brain Tumor Image Segmentation Benchmark}} ({{BRATS}})},
  volume = {34},
  issn = {0278-0062},
  doi = {10.1109/TMI.2014.2377694},
  abstract = {In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients - manually annotated by up to four raters - and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74\%-85\%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
  number = {10},
  journaltitle = {IEEE Transactions on Medical Imaging},
  date = {2015-10},
  pages = {1993-2024},
  keywords = {1,Benchmark,benchmark testing,Benchmark testing,Biomedical imaging,biomedical MRI,brain,Brain,BRATS,Dice scores,Educational institutions,glioma patients,hierarchical majority vote,human interrater variability,image segmentation,Image segmentation,Lesions,medical image processing,MICCAI 2012 conference,MICCAI 2013 conference,MRI,multicontrast MR scans,Multimodal Brain Tumor Image Segmentation Benchmark,Oncology/tumor,tumor image simulation software,tumor segmentation algorithm,tumours},
  author = {Menze, Bjoern H. and Jakab, Andras and Bauer, Stefan and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Kirby, Justin and Burren, Y. and Porz, N. and Slotboom, J. and Wiest, R. and Lanczi, L. and Gerstner, E. and Weber, M. A. and Arbel, T. and Avants, B. B. and Ayache, N. and Buendia, P. and Collins, D. L. and Cordier, N. and Corso, J. J. and Criminisi, A. and Das, T. and Delingette, H. and Demiralp, C and Durst, C. R. and Dojat, M. and Doyle, S. and Festa, J. and Forbes, F. and Geremia, E. and Glocker, B. and Golland, P. and Guo, X. and Hamamci, A. and Iftekharuddin, K. M. and Jena, R. and John, N. M. and Konukoglu, E. and Lashkari, D. and Mariz, J. A. and Meier, R. and Pereira, S. and Precup, D. and Price, S. J. and Raviv, T. R. and Reza, S. M. S. and Ryan, M. and Sarikaya, D. and Schwartz, L. and Shin, H. C. and Shotton, J. and Silva, C. A. and Sousa, N. and Subbanna, N. K. and Szekely, G. and Taylor, T. J. and Thomas, O. M. and Tustison, N. J. and Unal, G. and Vasseur, F. and Wintermark, M. and Ye, D. H. and Zhao, L. and Zhao, B. and Zikic, D. and Prastawa, M. and Reyes, M. and Leemput, K. Van},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/6A6PXKD6/6975210.html}
}

@inproceedings{mroueh_deep_2015,
  title = {Deep Multimodal Learning for {{Audio}}-{{Visual Speech Recognition}}},
  doi = {10.1109/ICASSP.2015.7178347},
  abstract = {In this paper, we present methods in deep multimodal learning for fusing speech and visual modalities for Audio-Visual Automatic Speech Recognition (AV-ASR). First, we study an approach where uni-modal deep networks are trained separately and their final hidden layers fused to obtain a joint feature space in which another deep network is built. While the audio network alone achieves a phone error rate (PER) of 41\% under clean condition on the IBM large vocabulary audio-visual studio dataset, this fusion model achieves a PER of 35.83\% demonstrating the tremendous value of the visual channel in phone classification even in audio with high signal to noise ratio. Second, we present a new deep network architecture that uses a bilinear softmax layer to account for class specific correlations between modalities. We show that combining the posteriors from the bilinear networks with those from the fused model mentioned above results in a further significant phone error rate reduction, yielding a final PER of 34.03\%.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  date = {2015-04},
  pages = {2130-2134},
  keywords = {acoustic noise,acoustic signal processing,audio network,audio-visual automatic speech recognition,Audio-Visual Automatic Speech Recognition (AV-ASR),audio-visual speech recognition,AV-ASR,bilinear networks,bilinear softmax layer,Correlation,deep multimodal learning,deep network architecture,Deep Neural Networks,Error analysis,fusing speech,fusion model,hidden layers,IBM large vocabulary audio-visual studio,IBM large vocabulary audio-visual studio dataset,Joints,Multimodal Learning,PER,phone classification,phone error rate,signal-noise ratio,significant phone error rate reduction,Speech,speech recognition,Speech recognition,Training,uni-modal deep networks,visual channel,visual modalities,Visualization},
  author = {Mroueh, Youssef and Marcheret, Étienne and Goel, Vaibhava},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/BS5HQMRF/cookiedetectresponse.html}
}

@article{noda_audio-visual_2015,
  langid = {english},
  title = {Audio-Visual Speech Recognition Using Deep Learning},
  volume = {42},
  issn = {0924-669X, 1573-7497},
  url = {https://link.springer.com/article/10.1007/s10489-014-0629-7},
  doi = {10.1007/s10489-014-0629-7},
  abstract = {Audio-visual speech recognition (AVSR) system is thought to be one of the most promising solutions for reliable speech recognition, particularly when the audio is corrupted by noise. However, cautious selection of sensory features is crucial for attaining high recognition performance. In the machine-learning community, deep learning approaches have recently attracted increasing attention because deep neural networks can effectively extract robust latent features that enable various recognition algorithms to demonstrate revolutionary generalization capabilities under diverse application conditions. This study introduces a connectionist-hidden Markov model (HMM) system for noise-robust AVSR. First, a deep denoising autoencoder is utilized for acquiring noise-robust audio features. By preparing the training data for the network with pairs of consecutive multiple steps of deteriorated audio features and the corresponding clean features, the network is trained to output denoised audio features from the corresponding features deteriorated by noise. Second, a convolutional neural network (CNN) is utilized to extract visual features from raw mouth area images. By preparing the training data for the CNN as pairs of raw images and the corresponding phoneme label outputs, the network is trained to predict phoneme labels from the corresponding mouth area input images. Finally, a multi-stream HMM (MSHMM) is applied for integrating the acquired audio and visual HMMs independently trained with the respective features. By comparing the cases when normal and denoised mel-frequency cepstral coefficients (MFCCs) are utilized as audio features to the HMM, our unimodal isolated word recognition results demonstrate that approximately 65 \% word recognition rate gain is attained with denoised MFCCs under 10 dB signal-to-noise-ratio (SNR) for the audio signal input. Moreover, our multimodal isolated word recognition results utilizing MSHMM with denoised MFCCs and acquired visual features demonstrate that an additional word recognition rate gain is attained for the SNR conditions below 10 dB.},
  number = {4},
  journaltitle = {Applied Intelligence},
  shortjournal = {Appl Intell},
  urldate = {2018-04-12},
  date = {2015-06-01},
  pages = {722-737},
  author = {Noda, Kuniaki and Yamaguchi, Yuki and Nakadai, Kazuhiro and Okuno, Hiroshi G. and Ogata, Tetsuya},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/SMEUCCPK/Noda et al. - 2015 - Audio-visual speech recognition using deep learnin.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/ENYBTPEH/s10489-014-0629-7.html}
}

@inproceedings{ringeval_introducing_2013,
  title = {Introducing the {{RECOLA}} Multimodal Corpus of Remote Collaborative and Affective Interactions},
  doi = {10.1109/FG.2013.6553805},
  abstract = {We present in this paper a new multimodal corpus of spontaneous collaborative and affective interactions in French: RECOLA, which is being made available to the research community. Participants were recorded in dyads during a video conference while completing a task requiring collaboration. Different multimodal data, i.e., audio, video, ECG and EDA, were recorded continuously and synchronously. In total, 46 participants took part in the test, for which the first 5 minutes of interaction were kept to ease annotation. In addition to these recordings, 6 annotators measured emotion continuously on two dimensions: arousal and valence, as well as social behavior labels on live dimensions. The corpus allowed us to take self-report measures of users during task completion. Methodologies and issues related to affective corpus construction are briefly reviewed in this paper. We further detail how the corpus was constructed, i.e., participants, procedure and task, the multimodal recording setup, the annotation of data and some analysis of the quality of these annotations.},
  eventtitle = {2013 10th {{IEEE International Conference}} and {{Workshops}} on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  booktitle = {2013 10th {{IEEE International Conference}} and {{Workshops}} on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  date = {2013-04},
  pages = {1-8},
  keywords = {arousal dimension,Collaboration,Context,dyads,emotion measurement,French language,Mood,multimodal data,natural languages,Physiology,RECOLA multimodal corpus,remote collaborative and affective interactions,research community,self-report measures,social behavior labels,social sciences,Software,Synchronization,valence dimension,video conference},
  author = {Ringeval, Fabien and Sonderegger, Andreas and Sauer, Juergen and Lalanne, Denis},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/QKESJ6Z6/6553805.html}
}

@article{min_kinectfacedb_2014,
  title = {{{KinectFaceDB}}: {{A Kinect Database}} for {{Face Recognition}}},
  volume = {44},
  issn = {2168-2216},
  doi = {10.1109/TSMC.2014.2331215},
  shorttitle = {{{KinectFaceDB}}},
  abstract = {The recent success of emerging RGB-D cameras such as the Kinect sensor depicts a broad prospect of 3-D data-based computer applications. However, due to the lack of a standard testing database, it is difficult to evaluate how the face recognition technology can benefit from this up-to-date imaging sensor. In order to establish the connection between the Kinect and face recognition research, in this paper, we present the first publicly available face database (i.e., KinectFaceDB1) based on the Kinect sensor. The database consists of different data modalities (well-aligned and processed 2-D, 2.5-D, 3-D, and video-based face data) and multiple facial variations. We conducted benchmark evaluations on the proposed database using standard face recognition methods, and demonstrated the gain in performance when integrating the depth data with the RGB data via score-level fusion. We also compared the 3-D images of Kinect (from the KinectFaceDB) with the traditional high-quality 3-D scans (from the FRGC database) in the context of face biometrics, which reveals the imperative needs of the proposed database for face recognition research.},
  number = {11},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  date = {2014-11},
  pages = {1534-1548},
  keywords = {3D data-based computer applications,Cameras,data modalities,Database,Databases,depth data,Face,face biometrics,face database,face recognition,Face recognition,face recognition methods,face recognition technology,facial variations,FRGC database,high-quality 3D scans,image colour analysis,image fusion,image sensors,Kinect,Kinect database,Kinect sensor,KinectFaceDB,Lighting,RGB data,RGB-D cameras,score-level fusion,Standards,up-to-date imaging sensor,Video sequences,video-based face data,visual databases},
  author = {Min, Rui and Kose, Neslihan and Dugelay, Jean-Luc},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/BNNUXN5M/6866883.html}
}

@inproceedings{ofli_berkeley_2013,
  title = {Berkeley {{MHAD}}: {{A}} Comprehensive {{Multimodal Human Action Database}}},
  doi = {10.1109/WACV.2013.6474999},
  shorttitle = {Berkeley {{MHAD}}},
  abstract = {Over the years, a large number of methods have been proposed to analyze human pose and motion information from images, videos, and recently from depth data. Most methods, however, have been evaluated on datasets that were too specific to each application, limited to a particular modality, and more importantly, captured under unknown conditions. To address these issues, we introduce the Berkeley Multimodal Human Action Database (MHAD) consisting of temporally synchronized and geometrically calibrated data from an optical motion capture system, multi-baseline stereo cameras from multiple views, depth sensors, accelerometers and microphones. This controlled multimodal dataset provides researchers an inclusive testbed to develop and benchmark new algorithms across multiple modalities under known capture conditions in various research domains. To demonstrate possible use of MHAD for action recognition, we compare results using the popular Bag-of-Words algorithm adapted to each modality independently with the results of various combinations of modalities using the Multiple Kernel Learning. Our comparative results show that multimodal analysis of human motion yields better action recognition rates than unimodal analysis.},
  eventtitle = {2013 {{IEEE Workshop}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  booktitle = {2013 {{IEEE Workshop}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  date = {2013-01},
  pages = {53-60},
  keywords = {accelerometer,Accelerometers,action recognition,bag-of-words algorithm,Berkeley MHAD,Cameras,Databases,depth sensor,human pose information,Humans,image motion analysis,learning (artificial intelligence),microphone,Microphones,motion information,multibaseline stereo camera,multimodal analysis,multimodal human action database,multiple kernel learning,optical motion capture system,pose estimation,Synchronization,Videos,visual databases},
  author = {Ofli, Ferda and Chaudhry, Rizwan and Kurillo, Gregorij and Vidal, René and Bajcsy, Ruzena},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HSHQRDHR/6474999.html}
}

@article{gomez-chova_multimodal_2015,
  title = {Multimodal {{Classification}} of {{Remote Sensing Images}}: {{A Review}} and {{Future Directions}}},
  volume = {103},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2015.2449668},
  shorttitle = {Multimodal {{Classification}} of {{Remote Sensing Images}}},
  abstract = {Earth observation through remote sensing images allows the accurate characterization and identification of materials on the surface from space and airborne platforms. Multiple and heterogeneous image sources can be available for the same geographical region: multispectral, hyperspectral, radar, multitemporal, and multiangular images can today be acquired over a given scene. These sources can be combined/fused to improve classification of the materials on the surface. Even if this type of systems is generally accurate, the field is about to face new challenges: the upcoming constellations of satellite sensors will acquire large amounts of images of different spatial, spectral, angular, and temporal resolutions. In this scenario, multimodal image fusion stands out as the appropriate framework to address these problems. In this paper, we provide a taxonomical view of the field and review the current methodologies for multimodal classification of remote sensing images. We also highlight the most recent advances, which exploit synergies with machine learning and signal processing: sparse methods, kernel-based fusion, Markov modeling, and manifold alignment. Then, we illustrate the different approaches in seven challenging remote sensing applications: 1) multiresolution fusion for multispectral image classification; 2) image downscaling as a form of multitemporal image fusion and multidimensional interpolation among sensors of different spatial, spectral, and temporal resolutions; 3) multiangular image classification; 4) multisensor image fusion exploiting physically-based feature extractions; 5) multitemporal image classification of land covers in incomplete, inconsistent, and vague image sources; 6) spatiospectral multisensor fusion of optical and radar images for change detection; and 7) cross-sensor adaptation of classifiers. The adoption of these techniques in operational settings will help to m- nitor our planet from space in the very near future.},
  number = {9},
  journaltitle = {Proceedings of the IEEE},
  date = {2015-09},
  pages = {1560-1584},
  keywords = {airborne platforms,Classification,Earth observation,fusion,geophysical image processing,geophysical techniques,heterogeneous image sources,image classification,image fusion,Image fusion,image multimodal classification,kernel-based fusion,machine learning,manifold alignment,Markov modeling,material characterization,material classification,material identification,multiangular,multidimensional interpolation,multimodal image analysis,multimodal image fusion,multiresolution fusion,multisource,multispectral image classification,multitemporal,multitemporal image fusion,optical images,radar images,remote sensing,Remote sensing,remote sensing image,satellite sensors,Satellites,Sensors,signal processing,space platforms,sparse methods,Spatial resolution,Synthetic aperture radar},
  author = {Gómez-Chova, Luis and Tuia, Devis and Moser, Gabriele and Camps-Valls, Gustau},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/RRCMD8N8/7182258.html}
}

@article{lahat_multimodal_2015,
  title = {Multimodal {{Data Fusion}}: {{An Overview}} of {{Methods}}, {{Challenges}}, and {{Prospects}}},
  volume = {103},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2015.2460697},
  shorttitle = {Multimodal {{Data Fusion}}},
  abstract = {In various disciplines, information about the same phenomenon can be acquired from different types of detectors, at different conditions, in multiple experiments or subjects, among others. We use the term “modality” for each such acquisition framework. Due to the rich characteristics of natural phenomena, it is rare that a single modality provides complete knowledge of the phenomenon of interest. The increasing availability of several modalities reporting on the same system introduces new degrees of freedom, which raise questions beyond those related to exploiting each modality separately. As we argue, many of these questions, or “challenges,” are common to multiple domains. This paper deals with two key issues: “why we need data fusion” and “how we perform it.” The first issue is motivated by numerous examples in science and technology, followed by a mathematical framework that showcases some of the benefits that data fusion provides. In order to address the second issue, “diversity” is introduced as a key concept, and a number of data-driven solutions based on matrix and tensor decompositions are discussed, emphasizing how they account for diversity across the data sets. The aim of this paper is to provide the reader, regardless of his or her community of origin, with a taste of the vastness of the field, the prospects, and the opportunities that it holds.},
  number = {9},
  journaltitle = {Proceedings of the IEEE},
  date = {2015-09},
  pages = {1449-1477},
  keywords = {acquisition framework,Blind source separation,data acquisition,data diversity,data fusion,Data integration,data-driven solutions,Electroencephalography,Laser radar,latent variables,matrix decomposition,modality term,multimodal data fusion,Multimodal sensors,multimodality,multiset data analysis,overview,sensor fusion,Sensors,Synthetic aperture radar,tensor decomposition,tensor decompositions,tensors},
  author = {Lahat, Dana and Adali, Tülay and Jutten, Christian},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/AKPA4Z43/7214350.html}
}

@article{ye_robust_2017,
  title = {Robust {{Registration}} of {{Multimodal Remote Sensing Images Based}} on {{Structural Similarity}}},
  volume = {55},
  issn = {0196-2892},
  doi = {10.1109/TGRS.2017.2656380},
  abstract = {Automatic registration of multimodal remote sensing data [e.g., optical, light detection and ranging (LiDAR), and synthetic aperture radar (SAR)] is a challenging task due to the significant nonlinear radiometric differences between these data. To address this problem, this paper proposes a novel feature descriptor named the histogram of orientated phase congruency (HOPC), which is based on the structural properties of images. Furthermore, a similarity metric named HOPCncc is defined, which uses the normalized correlation coefficient (NCC) of the HOPC descriptors for multimodal registration. In the definition of the proposed similarity metric, we first extend the phase congruency model to generate its orientation representation and use the extended model to build HOPCncc. Then, a fast template matching scheme for this metric is designed to detect the control points between images. The proposed HOPCncc aims to capture the structural similarity between images and has been tested with a variety of optical, LiDAR, SAR, and map data. The results show that HOPCncc is robust against complex nonlinear radiometric differences and outperforms the state-of-the-art similarities metrics (i.e., NCC and mutual information) in matching performance. Moreover, a robust registration method is also proposed in this paper based on HOPCncc, which is evaluated using six pairs of multimodal remote sensing images. The experimental results demonstrate the effectiveness of the proposed method for multimodal image registration.},
  number = {5},
  journaltitle = {IEEE Transactions on Geoscience and Remote Sensing},
  date = {2017-05},
  pages = {2941-2958},
  keywords = {LiDAR,remote sensing,Robustness,Feature extraction,Remote sensing,Image registration,SAR,image matching,feature descriptor,multimodal image analysis,automatic image registration,fast template matching scheme,histogram of orientated phase congruency,image registration,light detection and ranging,map data,multimodal remote sensing data,multimodal remote sensing image registration,Nonlinear optics,normalized correlation coefficient,optical radar,phase congruency,Radiometry,robust registration method,structural similarity,synthetic aperture radar},
  author = {Ye, Yuanxin and Shan, Jie and Bruzzone, Lorenzo and Shen, Li},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/2GA58VAH/7862734.html}
}

@article{le_saux_2018_2018,
  title = {2018 {{IEEE GRSS Data Fusion Contest}}: {{Multimodal Land Use Classification}} [{{Technical Committees}}]},
  volume = {6},
  issn = {2473-2397},
  doi = {10.1109/MGRS.2018.2798161},
  shorttitle = {2018 {{IEEE GRSS Data Fusion Contest}}},
  abstract = {Presents information on the 2018 IEEE GRSS Data Fusion Contest.},
  number = {1},
  journaltitle = {IEEE Geoscience and Remote Sensing Magazine},
  date = {2018-03},
  pages = {52-54},
  author = {Le Saux, Bertrand and Yokoya, Naoto and Hansch, Ronny and Prasad, Saurabh},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/UTBE7ZSW/cookiedetectresponse.html}
}

@article{tuia_2017_2017,
  title = {The 2017 {{IEEE Geoscience}} and {{Remote Sensing Society Data Fusion Contest}}: {{Open Data}} for {{Global Multimodal Land Use Classification}} [{{Technical Committees}}]},
  volume = {5},
  issn = {2473-2397},
  doi = {10.1109/MGRS.2017.2760346},
  shorttitle = {The 2017 {{IEEE Geoscience}} and {{Remote Sensing Society Data Fusion Contest}}},
  abstract = {Presents information on the 2017 IEEE Geoscience and Remote Sensing Society Data Fusion Contest.},
  number = {4},
  journaltitle = {IEEE Geoscience and Remote Sensing Magazine},
  date = {2017-12},
  pages = {110-114},
  author = {Tuia, Devis and Moser, Gabriele and Le Saux, Bertrand and Bechtel, Benjamin and See, Linda},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/S5KYRQTF/cookiedetectresponse.html}
}

@article{baltrusaitis_multimodal_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.09406},
  primaryClass = {cs},
  title = {Multimodal {{Machine Learning}}: {{A Survey}} and {{Taxonomy}}},
  url = {http://arxiv.org/abs/1705.09406},
  shorttitle = {Multimodal {{Machine Learning}}},
  abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
  urldate = {2018-04-12},
  date = {2017-05-25},
  keywords = {Computer Science - Learning},
  author = {Baltrušaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WM47Z7Z9/Baltrušaitis et al. - 2017 - Multimodal Machine Learning A Survey and Taxonomy.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9LXSGR6P/1705.html}
}

@inproceedings{dechesne_how_2017,
  title = {How to Combine Lidar and Very High Resolution Multispectral Images for Forest Stand Segmentation?},
  doi = {10.1109/IGARSS.2017.8127572},
  abstract = {Forest stands are a basic unit of analysis for forest inventory and mapping. Stands are defined as large forested areas of homogeneous tree species composition and age. Their accurate delineation is usually performed by human operators through visual analysis of very high resolution (VHR) infra-red and visible images. This task is tedious, highly time consuming, and needs to be automated for scalability and efficient updating purposes. The most appropriate fusion of two remote sensing modalities (lidar and multispectral images) is investigated here. The multispectral images give information about the tree species while 3D lidar point clouds provide geometric information. The fusion is operated at three different levels within a semantic segmentation workflow: over-segmentation, classification, and regularization. Results show that over-segmentation can be performed either on lidar or optical images without performance loss or gain, whereas fusion is mandatory for efficient semantic segmentation. Eventually, the fusion strategy dictates the composition and nature of the forest stands, assessing the high versatility of our approach.},
  eventtitle = {2017 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}} ({{IGARSS}})},
  booktitle = {2017 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}} ({{IGARSS}})},
  date = {2017-07},
  pages = {2772-2775},
  keywords = {segmentation,geophysical image processing,Laser radar,classification,Vegetation,Three-dimensional displays,image classification,image segmentation,image fusion,vegetation mapping,Image segmentation,Feature extraction,Databases,Remote sensing,vegetation,fusion,optical images,optical radar,3D lidar point clouds,forest inventory,forest mapping,forest stands,forested areas,forestry,fusion strategy,homogeneous tree species composition,infrared images,Lidar,lidar image,multispectral imagery,remote sensing by laser beam,remote sensing modalities,semantic segmentation workflow,tree species,very-high-resolution multispectral images,visual analysis},
  author = {Dechesne, Clément and Mallet, Clément and Le Bris, Arnaud and Gouet-Brunet, Valérie},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/3F5HIF7P/8127572.html}
}

@article{dechesne_semantic_2017,
  title = {Semantic Segmentation of Forest Stands of Pure Species Combining Airborne Lidar Data and Very High Resolution Multispectral Imagery},
  volume = {126},
  issn = {0924-2716},
  url = {http://www.sciencedirect.com/science/article/pii/S0924271616302763},
  doi = {10.1016/j.isprsjprs.2017.02.011},
  abstract = {Forest stands are the basic units for forest inventory and mapping. Stands are defined as large forested areas (e.g., ⩾2ha) of homogeneous tree species composition and age. Their accurate delineation is usually performed by human operators through visual analysis of very high resolution (VHR) infra-red images. This task is tedious, highly time consuming, and should be automated for scalability and efficient updating purposes. In this paper, a method based on the fusion of airborne lidar data and VHR multispectral images is proposed for the automatic delineation of forest stands containing one dominant species (purity superior to 75\%). This is the key preliminary task for forest land-cover database update. The multispectral images give information about the tree species whereas 3D lidar point clouds provide geometric information on the trees and allow their individual extraction. Multi-modal features are computed, both at pixel and object levels: the objects are individual trees extracted from lidar data. A supervised classification is then performed at the object level in order to coarsely discriminate the existing tree species in each area of interest. The classification results are further processed to obtain homogeneous areas with smooth borders by employing an energy minimum framework, where additional constraints are joined to form the energy function. The experimental results show that the proposed method provides very satisfactory results both in terms of stand labeling and delineation (overall accuracy ranges between 84\% and 99\%).},
  journaltitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
  shortjournal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  urldate = {2018-04-13},
  date = {2017-04-01},
  pages = {129-145},
  keywords = {Feature selection,Lidar,Energy minimization,Forest stand delineation,Fusion,Multispectral imagery,Regularisation,Supervised classification,Tree species},
  author = {Dechesne, Clément and Mallet, Clément and Le Bris, Arnaud and Gouet-Brunet, Valérie},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WPIRC3P4/Dechesne et al. - 2017 - Semantic segmentation of forest stands of pure spe.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/E4W5V7GP/S0924271616302763.html}
}

@article{guo_relevance_2011,
  title = {Relevance of Airborne Lidar and Multispectral Image Data for Urban Scene Classification Using {{Random Forests}}},
  volume = {66},
  issn = {0924-2716},
  url = {http://www.sciencedirect.com/science/article/pii/S0924271610000705},
  doi = {10.1016/j.isprsjprs.2010.08.007},
  abstract = {Airborne lidar systems have become a source for the acquisition of elevation data. They provide georeferenced, irregularly distributed 3D point clouds of high altimetric accuracy. Moreover, these systems can provide for a single laser pulse, multiple returns or echoes, which correspond to different illuminated objects. In addition to multi-echo laser scanners, full-waveform systems are able to record 1D signals representing a train of echoes caused by reflections at different targets. These systems provide more information about the structure and the physical characteristics of the targets. Many approaches have been developed, for urban mapping, based on aerial lidar solely or combined with multispectral image data. However, they have not assessed the importance of input features. In this paper, we focus on a multi-source framework using aerial lidar (multi-echo and full waveform) and aerial multispectral image data. We aim to study the feature relevance for dense urban scenes. The Random Forests algorithm is chosen as a classifier: it runs efficiently on large datasets, and provides measures of feature importance for each class. The margin theory is used as a confidence measure of the classifier, and to confirm the relevance of input features for urban classification. The quantitative results confirm the importance of the joint use of optical multispectral and lidar data. Moreover, the relevance of full-waveform lidar features is demonstrated for building and vegetation area discrimination.},
  number = {1},
  journaltitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
  shortjournal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  urldate = {2018-04-13},
  date = {2011-01-01},
  pages = {56-66},
  keywords = {Lidar,Multispectral image,Random forests,Urban,Variable importance},
  author = {Guo, Li and Chehata, Nesrine and Mallet, Clément and Boukir, Samia},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/XXRR6Q59/Guo et al. - 2011 - Relevance of airborne lidar and multispectral imag.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WJWZ7ZUQ/S0924271610000705.html}
}

@inproceedings{karpathy_deep_2015,
  title = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  date = {2015},
  pages = {3128--3137},
  author = {Karpathy, Andrej and Fei-Fei, Li},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/BZ3VX9G6/cvpr2015.pdf}
}

@article{ordonez_deep_2016,
  langid = {english},
  title = {Deep {{Convolutional}} and {{LSTM Recurrent Neural Networks}} for {{Multimodal Wearable Activity Recognition}}},
  volume = {16},
  url = {http://www.mdpi.com/1424-8220/16/1/115},
  doi = {10.3390/s16010115},
  abstract = {Human activity recognition (HAR) tasks have traditionally been solved using engineered features obtained by heuristic processes. Current research suggests that deep convolutional neural networks are suited to automate feature extraction from raw sensor inputs. However, human activities are made of complex sequences of motor movements, and capturing this temporal dynamics is fundamental for successful HAR. Based on the recent success of recurrent neural networks for time series domains, we propose a generic deep framework for activity recognition based on convolutional and LSTM recurrent units, which: (i) is suitable for multimodal wearable sensors; (ii) can perform sensor fusion naturally; (iii) does not require expert knowledge in designing features; and (iv) explicitly models the temporal dynamics of feature activations. We evaluate our framework on two datasets, one of which has been used in a public activity recognition challenge. Our results show that our framework outperforms competing deep non-recurrent networks on the challenge dataset by 4\% on average; outperforming some of the previous reported results by up to 9\%. Our results show that the framework can be applied to homogeneous sensor modalities, but can also fuse multimodal sensors to improve performance. We characterise key architectural hyperparameters’ influence on performance to provide insights about their optimisation.},
  number = {1},
  journaltitle = {Sensors},
  urldate = {2018-04-25},
  date = {2016-01-18},
  pages = {115},
  keywords = {deep learning,machine learning,sensor fusion,human activity recognition,LSTM,neural network,wearable sensors},
  author = {Ordóñez, Francisco Javier and Roggen, Daniel},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/44S2TFVB/Ordóñez et Roggen - 2016 - Deep Convolutional and LSTM Recurrent Neural Netwo.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/6NBWB7N7/html.html}
}

@inproceedings{kim_deep_2013,
  title = {Deep Learning for Robust Feature Generation in Audiovisual Emotion Recognition},
  doi = {10.1109/ICASSP.2013.6638346},
  abstract = {Automatic emotion recognition systems predict high-level affective content from low-level human-centered signal cues. These systems have seen great improvements in classification accuracy, due in part to advances in feature selection methods. However, many of these feature selection methods capture only linear relationships between features or alternatively require the use of labeled data. In this paper we focus on deep learning techniques, which can overcome these limitations by explicitly capturing complex non-linear feature interactions in multimodal data. We propose and evaluate a suite of Deep Belief Network models, and demonstrate that these models show improvement in emotion classification performance over baselines that do not employ deep learning. This suggests that the learned high-order non-linear relationships are effective for emotion recognition.},
  eventtitle = {2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  booktitle = {2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  date = {2013-05},
  pages = {3687-3691},
  keywords = {Accuracy,Acoustics,audiovisual emotion recognition,deep belief network models,deep belief networks,deep learning,deep learning techniques,emotion classification,emotion recognition,Emotion recognition,feature selection methods,high-level affective content,learning (artificial intelligence),low-level human-centered signal cues,multimodal data,multimodal features,robust feature generation,Speech,Speech processing,Speech recognition,Training,unsupervised feature learning},
  author = {Kim, Yelin and Lee, Honglak and Provost, Emily Mower},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/PHKJEMW4/6638346.html}
}

@inproceedings{meier_adaptive_1996,
  title = {Adaptive Bimodal Sensor Fusion for Automatic Speechreading},
  volume = {2},
  doi = {10.1109/ICASSP.1996.543250},
  abstract = {We present work on improving the performance of automated speech recognizers by using additional visual information: (lip-/speechreading); achieving error reduction of up to 50\%. This paper focuses on different methods of combining the visual and acoustic data to improve the recognition performance. We show this on an extension of an existing state-of-the-art speech recognition system, a modular MS-TDNN. We have developed adaptive combination methods at several levels of the recognition network. Additional information such as estimated signal-to-noise ratio (SNR) is used in some cases. The results of the different combination methods are shown for clean speech and data with artificial noise (white, music, motor). The new combination methods adapt automatically to varying noise conditions making hand-tuned parameters unnecessary},
  eventtitle = {1996 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing Conference Proceedings}}},
  booktitle = {1996 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing Conference Proceedings}}},
  date = {1996-05},
  pages = {833-836 vol. 2},
  keywords = {acoustic data,Acoustic noise,acoustic signal processing,Acoustic testing,adaptive bimodal sensor fusion,adaptive combination methods,adaptive signal processing,artificial noise,automated speech recognizer performance,automatic speechreading,Background noise,clean speech,error reduction,image processing,Interactive systems,lip reading,Loudspeakers,modular MS-TDNN,motor,multilayer perceptrons,music,noise conditions,recognition network,sensor fusion,Sensor fusion,Signal to noise ratio,signal-to-noise ratio,SNR,Speech enhancement,speech recognition,Speech recognition,speech recognition system,visual data,visual information,white noise,White noise},
  author = {Meier, Uwe and Hürst, Wolfgang and Duchnowski, Paul},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/SH2IV729/543250.html}
}

@inproceedings{lee_rdfnet_2017,
  title = {{{RDFNet}}: {{RGB}}-{{D Multi}}-Level {{Residual Feature Fusion}} for {{Indoor Semantic Segmentation}}},
  doi = {10.1109/ICCV.2017.533},
  shorttitle = {{{RDFNet}}},
  abstract = {In multi-class indoor semantic segmentation using RGB-D data, it has been shown that incorporating depth feature into RGB feature is helpful to improve segmentation accuracy. However, previous studies have not fully exploited the potentials of multi-modal feature fusion, e.g., simply concatenating RGB and depth features or averaging RGB and depth score maps. To learn the optimal fusion of multimodal features, this paper presents a novel network that extends the core idea of residual learning to RGB-D semantic segmentation. Our network effectively captures multilevel RGB-D CNN features by including multi-modal feature fusion blocks and multi-level feature refinement blocks. Feature fusion blocks learn residual RGB and depth features and their combinations to fully exploit the complementary characteristics of RGB and depth data. Feature refinement blocks learn the combination of fused features from multiple levels to enable high-resolution prediction. Our network can efficiently train discriminative multi-level features from each modality end-to-end by taking full advantage of skip-connections. Our comprehensive experiments demonstrate that the proposed architecture achieves the state-of-the-art accuracy on two challenging RGB-D indoor datasets, NYUDv2 and SUN RGB-D.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  date = {2017-10},
  pages = {4990-4999},
  keywords = {Computer architecture,Convolution,depth data,depth features,depth score maps,feature extraction,Feature extraction,Fuses,image classification,image colour analysis,image fusion,image segmentation,Image segmentation,learning (artificial intelligence),modality end-to-end,multiclass indoor semantic segmentation,multilevel feature refinement blocks,multilevel features,multilevel RGB-D CNN features,multimodal feature fusion blocks,multimodal features,object detection,optimal fusion,residual feature fusion,residual learning,RGB feature,RGB-D data,RGB-D indoor datasets,RGB-D multilevel,RGB-D semantic segmentation,Semantics,SUN,SUN RGB-D},
  author = {Lee, Seungyong and Park, Seong-Jin and Hong, Ki-Sang},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/NPSTZJ8L/Lee et al. - 2017 - RDFNet RGB-D Multi-level Residual Feature Fusion .pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/U67EJ6LZ/Park_RDFNet_RGB-D_Multi-Level_ICCV_2017_paper.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VT3RKKVM/8237795.html}
}

@phdthesis{maggiori_learning_2017,
  langid = {english},
  title = {Learning Approaches for Large-Scale Remote Sensing Image Classification},
  url = {https://hal.inria.fr/tel-01589661/document},
  abstract = {The analysis of airborne and satellite images is one of the core subjects in remote sensing. In recent years, technological developments have facilitated the availability of large-scale sources of data, which cover significant extents of the earth’s surface, often at impressive spatial resolutions. In addition to the evident computational complexity issues that arise, one of the current challenges is to handle the variability in the appearance of the objects across different geographic regions. For this, it is necessary to design classification methods that go beyond the analysis of individual pixel spectra, introducing higher-level contextual information in the process. In this thesis, we first propose a method to perform classification with shape priors, based on the optimization of a hierarchical subdivision data structure. We then delve into the use of the increasingly popular convolutional neural networks (CNNs) to learn deep hierarchical contextual features. We investigate CNNs from multiple angles, in order to address the different points required to adapt them to our problem. Among other subjects, we propose different solutions to output high-resolution classification maps and we study the acquisition of training data. We also created a dataset of aerial images over dissimilar locations, and assess the generalization capabilities of CNNs. Finally, we propose a technique to polygonize the output classification maps, so as to integrate them into operational geographic information systems, thus completing the typical processing pipeline observed in a wide number of applications. Throughout this thesis, we experiment on hyperspectral, atellite and aerial images, with scalability, generalization and applicability goals in mind.},
  institution = {{Université Côte d'Azur}},
  urldate = {2018-06-05},
  date = {2017-06-22},
  author = {Maggiori, Emmanuel},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/6INDP93T/Maggiori - 2017 - Learning approaches for large-scale remote sensing.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/6BJQWUL2/tel-01589661.html}
}

@inproceedings{liu_dense_2017,
  title = {Dense {{Semantic Labeling}} of {{Very}}-{{High}}-{{Resolution Aerial Imagery}} and {{LiDAR}} with {{Fully}}-{{Convolutional Neural Networks}} and {{Higher}}-{{Order CRFs}}},
  doi = {10.1109/CVPRW.2017.200},
  abstract = {The increasing availability of very-high-resolution (VHR) aerial optical images as well as coregistered Li-DAR data opens great opportunities for improving object-level dense semantic labeling of airborne remote sensing imagery. As a result, efficient and effective multisensor fusion techniques are needed to fully exploit these complementary data modalities. Recent researches demonstrated how to process remote sensing images using pre-trained deep convolutional neural networks (DCNNs) at the feature level. In this paper, we propose a decision-level fusion approach using a probabilistic graphical model for the task of dense semantic labeling. Our proposed method first obtains two initial probabilistic labeling predictions from a fully-convolutional neural network and a linear classifier, e.g. logistic regression, respectively. These two predictions are then combined within a higher-order conditional random field (CRF). We utilize graph cut inference to estimate the final dense semantic labeling results. Higher-order CRF modeling helps to resolve fusion ambiguities by explicitly using the spatial contextual information, which can be learned from the training data. Experiments on the ISPRS 2D semantic labeling Potsdam dataset show that our proposed approach compares favorably to the state-of-the-art baseline methods.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  date = {2017-07},
  pages = {1561-1570},
  keywords = {Labeling,learning (artificial intelligence),geophysical image processing,probability,Laser radar,Neural networks,convolution,Semantics,Training,image classification,conditional random field,Optical imaging,Remote sensing,deep convolutional neural networks,image resolution,sensor fusion,optical radar,graph theory,airborne remote sensing imagery,coregistered Li-DAR data,DCNN training,dense semantic labeling,fully-convolutional neural networks,graph cut inference,higher-order CRFs,linear classifier,multisensor fusion,probabilistic graphical model,radar imaging,remote sensing by radar,very-high-resolution aerial imagery,VHR aerial optical images},
  author = {Liu, Yansong and Piramanayagam, Sankaranarayanan and Monteiro, Sildomar T. and Saber, Eli},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/2SZZN4SD/Liu et al. - 2017 - Dense Semantic Labeling of Very-High-Resolution Ae.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VBV9PSML/8014934.html}
}

@inproceedings{guerry_look_2017,
  title = {"{{Look}} at This One" Detection Sharing between Modality-Independent Classifiers for Robotic Discovery of People},
  doi = {10.1109/ECMR.2017.8098679},
  abstract = {With the advent of low-cost RGBD sensors, many solutions have been proposed for extraction and fusion of colour and depth information. In this paper, we propose new different fusion approaches of these multimodal sources for people detection. We are especially concerned by a scenario where a robot evolves in a changing environment. We extend the use of the Faster RCNN framework proposed by Girshick et al. [1] to this use case (i), we significantly improve performances on people detection on the InOutDoor RGBD People dataset [2] and the RGBD people dataset [3] (ii), we show these fusion handle efficiently sensor defect like complete lost of a modality (iii). Furthermore we propose a new dataset for people detection in difficult conditions: ONERA.ROOM (iv).},
  eventtitle = {2017 {{European Conference}} on {{Mobile Robots}} ({{ECMR}})},
  booktitle = {2017 {{European Conference}} on {{Mobile Robots}} ({{ECMR}})},
  date = {2017-09},
  pages = {1-6},
  keywords = {Cameras,colour information extraction,colour information fusion,Computer architecture,depth information extraction,depth information fusion,Faster RCNN framework,feature extraction,fusion approaches,image classification,image colour analysis,image fusion,image sensors,InOutDoor RGBD People dataset,look at this one detection sharing,low-cost RGBD sensors,mobile robots,modality-independent classifiers,neural nets,object detection,people detection,Proposals,RGBD people dataset,Robot sensing systems,robot vision,robotic discovery,sensor defect,Training},
  author = {Guerry, Joris and Le Saux, Bertrand and Filliat, David},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8S2CSKKD/Guerry et al. - 2017 - #x0022\;Look at this one #x0022\; detection sharing .pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/S3KP9949/8098679.html}
}

@inproceedings{schwarz_rgb-d_2015,
  title = {{{RGB}}-{{D}} Object Recognition and Pose Estimation Based on Pre-Trained Convolutional Neural Network Features},
  doi = {10.1109/ICRA.2015.7139363},
  abstract = {Object recognition and pose estimation from RGB-D images are important tasks for manipulation robots which can be learned from examples. Creating and annotating datasets for learning is expensive, however. We address this problem with transfer learning from deep convolutional neural networks (CNN) that are pre-trained for image categorization and provide a rich, semantically meaningful feature set. We incorporate depth information, which the CNN was not trained with, by rendering objects from a canonical perspective and colorizing the depth channel according to distance from the object center. We evaluate our approach on the Washington RGB-D Objects dataset, where we find that the generated feature set naturally separates classes and instances well and retains pose manifolds. We outperform state-of-the-art on a number of subtasks and show that our approach can yield superior results when only little training data is available.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  booktitle = {2015 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  date = {2015-05},
  pages = {1329-1335},
  keywords = {Accuracy,neural nets,Image color analysis,learning (artificial intelligence),object recognition,transfer learning,Training,CNN,Estimation,visual databases,RGB-D images,Feature extraction,Support vector machines,pose estimation,canonical perspective,convolutional neural network features,depth channel,image categorization,manipulation robots,manipulators,object center,Pipelines,pose manifolds,Washington RGB-D objects dataset},
  author = {Schwarz, Max and Schulz, Hannes and Behnke, Sven},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/JB7V9BER/Schwarz et al. - 2015 - RGB-D object recognition and pose estimation based.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/SFL73YNC/7139363.html}
}

@incollection{simonyan_two-stream_2014,
  title = {Two-{{Stream Convolutional Networks}} for {{Action Recognition}} in {{Videos}}},
  url = {http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2018-06-20},
  date = {2014},
  pages = {568--576},
  author = {Simonyan, Karen and Zisserman, Andrew},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/TGJI8NMB/Simonyan et Zisserman - 2014 - Two-Stream Convolutional Networks for Action Recog.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9S95C528/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.html}
}

@article{song_combining_2017,
  title = {Combining {{Models}} from {{Multiple Sources}} for {{RGB}}-{{D Scene Recognition}}},
  url = {https://www.ijcai.org/proceedings/2017/631},
  abstract = {Electronic proceedings of IJCAI 2017},
  urldate = {2018-07-03},
  date = {2017},
  pages = {4523-4529},
  author = {Song, Xinhang and Jiang, Shuqiang and Herranz, Luis},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/H98954T9/631.html}
}

@inproceedings{xie_adversarial_2017,
  langid = {english},
  title = {Adversarial {{Examples}} for {{Semantic Segmentation}} and {{Object Detection}}},
  isbn = {978-1-5386-1032-9},
  url = {http://ieeexplore.ieee.org/document/8237415/},
  doi = {10.1109/ICCV.2017.153},
  abstract = {It has been well demonstrated that adversarial examples, i.e., natural images with visually imperceptible perturbations added, cause deep networks to fail on image classiﬁcation. In this paper, we extend adversarial examples to semantic segmentation and object detection which are much more difﬁcult. Our observation is that both segmentation and detection are based on classifying multiple targets on an image (e.g., the target is a pixel or a receptive ﬁeld in segmentation, and an object proposal in detection). This inspires us to optimize a loss function over a set of targets for generating adversarial perturbations. Based on this, we propose a novel algorithm named Dense Adversary Generation (DAG), which applies to the state-of-the-art networks for segmentation and detection. We ﬁnd that the adversarial perturbations can be transferred across networks with different training data, based on different architectures, and even for different recognition tasks. In particular, the transfer ability across networks with the same architecture is more signiﬁcant than in other cases. Besides, we show that summing up heterogeneous perturbations often leads to better transfer performance, which provides an effective method of black-box adversarial attack.},
  publisher = {{IEEE}},
  urldate = {2018-07-03},
  date = {2017-10},
  pages = {1378-1387},
  author = {Xie, Cihang and Wang, Jianyu and Zhang, Zhishuai and Zhou, Yuyin and Xie, Lingxi and Yuille, Alan},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/KKX45PIF/Xie et al. - 2017 - Adversarial Examples for Semantic Segmentation and.pdf}
}


