
@inproceedings{audebert_semantic_2016,
  title = {Semantic {{Segmentation}} of {{Earth Observation Data Using Multimodal}} and {{Multi}}-Scale {{Deep Networks}}},
  doi = {10.1007/978-3-319-54181-5_12},
  abstract = {This work investigates the use of deep fully convolutional neural networks (DFCNN) for pixel-wise scene labeling of Earth Observation images. Especially, we train a variant of the SegNet architecture on remote sensing data over an urban area and study different strategies for performing accurate semantic segmentation. Our contributions are the following: (1) we transfer efficiently a DFCNN from generic everyday images to remote sensing images; (2) we introduce a multi-kernel convolutional layer for fast aggregation of predictions at multiple scales; (3) we perform data fusion from heterogeneous sensors (optical and laser) using residual correction. Our framework improves state-of-the-art accuracy on the ISPRS Vaihingen 2D Semantic Labeling dataset.},
  language = {en},
  booktitle = {Computer {{Vision}} \textendash{} {{ACCV}} 2016},
  publisher = {{Springer, Cham}},
  author = {Audebert, Nicolas and Le Saux, Bertrand and Lef{\`e}vre, S{\'e}bastien},
  month = nov,
  year = {2016},
  pages = {180-196},
  file = {/home/naudeber/Bibliographie//Springer, Cham/2016/Audebert et al 2016 - Semantic Segmentation of Earth Observation Data Using Multimodal and.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/394SFXNP/Audebert et al_2016_Semantic Segmentation of Earth Observation Data Using Multimodal and.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/PC7RWZAX/978-3-319-54181-5_12.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/TJHU6H38/978-3-319-54181-5_12.html}
}

@article{simonyan_very_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.1556},
  primaryClass = {cs},
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  journal = {arXiv:1409.1556 [cs]},
  author = {Simonyan, Karen and Zisserman, Andrew},
  month = sep,
  year = {2014},
  keywords = {Ã€ lire,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//arXiv1409.1556 [cs]/2014/Simonyan Zisserman 2014 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8VM3BPQT/1409.html}
}

@inproceedings{ngiam_multimodal_2011,
  title = {Multimodal Deep Learning},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning ({{ICML}}-11)},
  author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y.},
  year = {2011},
  pages = {689--696},
  file = {/home/naudeber/Bibliographie//undefined/2011/Ngiam et al 2011 - Multimodal deep learning_3.pdf;/home/naudeber/Bibliographie//undefined/2011/Ngiam et al 2011 - Multimodal deep learning_4.pdf}
}

@inproceedings{guo_two-stream_2016,
  title = {Two-Stream Convolutional Neural Network for Accurate {{RGB}}-{{D}} Fingertip Detection Using Depth and Edge Information},
  booktitle = {Image {{Processing}} ({{ICIP}}), 2016 {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  author = {Guo, Hengkai and Wang, Guijin and Chen, Xinghao},
  year = {2016},
  pages = {2608--2612},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WUGHGHK8/1612.07978.pdf}
}

@article{lin_refinenet_2016,
  title = {{{RefineNet}}: {{Multi}}-{{Path Refinement Networks}} with {{Identity Mappings}} for {{High}}-{{Resolution Semantic Segmentation}}},
  shorttitle = {{{RefineNet}}},
  journal = {arXiv preprint arXiv:1611.06612},
  author = {Lin, Guosheng and Milan, Anton and Shen, Chunhua and Reid, Ian},
  year = {2016},
  file = {/home/naudeber/Bibliographie//arXiv preprint arXiv1611.06612/2016/Lin et al 2016 - RefineNet.pdf}
}

@inproceedings{vakalopoulou_simultaneous_2016,
  title = {Simultaneous Registration, Segmentation and Change Detection from Multisensor, Multitemporal Satellite Image Pairs.},
  doi = {10.1109/IGARSS.2016.7729469},
  abstract = {In this paper, a novel generic framework has been designed, developed and validated for addressing simultaneously the tasks of image registration, segmentation and change detection from multisensor, multiresolution, multitemporal satel- lite image pairs. Our approach models the inter-dependencies of variables through a higher order graph. The proposed formulation is modular with respect to the nature of images (various similarity metrics can be considered), the nature of deformations (arbitrary interpolation strategies), and the nature of segmentation likelihoods (various classification approaches can be employed). Inference of the proposed formulation is achieved through its mapping to an overparametrized pairwise graph which is then optimized using linear programming. Experimental results and the performed quantitative evaluation indicate the high potentials of the developed method.},
  language = {en},
  author = {Vakalopoulou, Maria and Platias, Christos and Papadomanolaki, Maria and Paragios, Nikos and Karantzalos, Konstantinos},
  month = jul,
  year = {2016},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/3CV2N9W3/Vakalopoulou et al_2016_Simultaneous registration, segmentation and change detection from multisensor,.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9VZP76FX/hal-01413373.html}
}

@article{isola_image--image_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.07004},
  primaryClass = {cs},
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  journal = {arXiv:1611.07004 [cs]},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/U5VVQAVV/Isola et al_2016_Image-to-Image Translation with Conditional Adversarial Networks.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/QGEK5G7R/1611.html}
}

@article{geis_joint_2017,
  title = {Joint Use of Remote Sensing Data and Volunteered Geographic Information for Exposure Estimation: Evidence from {{Valpara{\'\i}so}}, {{Chile}}},
  volume = {86},
  issn = {0921-030X, 1573-0840},
  shorttitle = {Joint Use of Remote Sensing Data and Volunteered Geographic Information for Exposure Estimation},
  doi = {10.1007/s11069-016-2663-8},
  abstract = {The impact of natural hazards on mankind has increased dramatically over the past decades. Global urbanization processes and increasing spatial concentrations of exposed elements induce natural hazard risk at a uniquely high level. To mitigate affiliated perils requires detailed knowledge about elements at risk. Considering a high spatiotemporal variability of elements at risk, detailed information is costly in terms of both time and economic resources and therefore often incomplete, aggregated, or outdated. To alleviate these restrictions, the availability of very-high-resolution satellite images promotes accurate and detailed analysis of exposure over various spatial scales with large-area coverage. In the past, valuable approaches were proposed; however, the design of information extraction procedures with a high level of automatization remains challenging. In this paper, we uniquely combine remote sensing data and volunteered geographic information from the OpenStreetMap project (OSM) (i.e., freely accessible geospatial information compiled by volunteers) for a highly automated estimation of crucial exposure components (i.e., number of buildings and population) with a high level of spatial detail. To this purpose, we first obtain labeled training segments from the OSM data in conjunction with the satellite imagery. This allows for learning a supervised algorithmic model (i.e., rotation forest) in order to extract relevant thematic classes of land use/land cover (LULC) from the satellite imagery. Extracted information is jointly deployed with information from the OSM data to estimate the number of buildings with regression techniques (i.e., a multi-linear model from ordinary least-square optimization and a nonlinear support vector regression model are considered). Analogously, urban LULC information is used in conjunction with OSM data to spatially disaggregate population information. Experimental results were obtained for the city of Valpara{\'\i}so in Chile. Thereby, we demonstrate the relevance of the approaches by estimating number of affected buildings and population referring to a historical tsunami event.},
  language = {en},
  number = {1},
  journal = {Natural Hazards},
  author = {Gei\ss, Christian and Schau\ss, Anne and Riedlinger, Torsten and Dech, Stefan and Zelaya, Cecilia and Guzm{\'a}n, Nicol{\'a}s and Hube, Math{\'\i}as A. and Arsanjani, Jamal Jokar and Taubenb{\"o}ck, Hannes},
  month = mar,
  year = {2017},
  pages = {81-105},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/MMPPAVA3/GeiÃŸ et al_2017_Joint use of remote sensing data and volunteered geographic information for.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/RR2EN2BX/10.html}
}

@phdthesis{mnih_machine_2013,
  title = {Machine {{Learning}} for {{Aerial Image Labeling}}},
  school = {University of Toronto},
  author = {Mnih, Volodymyr},
  year = {2013}
}

@inproceedings{chen_deepvgi_2017,
  title = {{{DeepVGI}}: {{Deep Learning}} with {{Volunteered Geographic Information}}},
  booktitle = {26th {{International World Wide Web Conference}} ({{Poster}})},
  publisher = {{ACM}},
  author = {Chen, Jiaoyan and Zipf, Alexander},
  year = {2017},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HFEGZFRP/Chen_Zipf_2017_DeepVGI.pdf}
}

@article{danylo_contributing_2016,
  title = {Contributing to {{WUDAPT}}: {{A Local Climate Zone Classification}} of {{Two Cities}} in {{Ukraine}}},
  volume = {9},
  issn = {1939-1404, 2151-1535},
  shorttitle = {Contributing to {{WUDAPT}}},
  doi = {10.1109/JSTARS.2016.2539977},
  number = {5},
  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  author = {Danylo, Olha and See, Linda and Bechtel, Benjamin and Schepaschenko, Dmitry and Fritz, Steffen},
  month = may,
  year = {2016},
  pages = {1841-1853},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/RRJ9VR2P/Danylo et al_2016_Contributing to WUDAPT.pdf}
}

@inproceedings{hazirbas_fusenet_2016,
  title = {{{FuseNet}}: {{Incorporating Depth}} into {{Semantic Segmentation}} via {{Fusion}}-{{Based CNN Architecture}}},
  shorttitle = {{{FuseNet}}},
  doi = {10.1007/978-3-319-54181-5_14},
  abstract = {In this paper we address the problem of semantic labeling of indoor scenes on RGB-D data. With the availability of RGB-D cameras, it is expected that additional depth measurement will improve the accuracy. Here we investigate a solution how to incorporate complementary depth information into a semantic segmentation framework by making use of convolutional neural networks (CNNs). Recently encoder-decoder type fully convolutional CNN architectures have achieved a great success in the field of semantic segmentation. Motivated by this observation we propose an encoder-decoder type network, where the encoder part is composed of two branches of networks that simultaneously extract features from RGB and depth images and fuse depth features into the RGB feature maps as the network goes deeper. Comprehensive experimental evaluations demonstrate that the proposed fusion-based architecture achieves competitive results with the state-of-the-art methods on the challenging SUN RGB-D benchmark obtaining 76.27\% global accuracy, 48.30\% average class accuracy and 37.29\% average intersection-over-union score.},
  language = {en},
  booktitle = {Computer {{Vision}} \textendash{} {{ACCV}} 2016},
  publisher = {{Springer, Cham}},
  author = {Hazirbas, Caner and Ma, Lingni and Domokos, Csaba and Cremers, Daniel},
  month = nov,
  year = {2016},
  pages = {213-228},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/CBE66U65/Hazirbas et al_2016_FuseNet.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/KCPWN6WW/978-3-319-54181-5_14.html}
}

@inproceedings{he_deep_2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\texttimes{} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month = jun,
  year = {2016},
  keywords = {neural nets,object detection,learning (artificial intelligence),Image recognition,Visualization,Neural networks,Training,image classification,image segmentation,CIFAR-10,COCO object detection dataset,COCO segmentation,ILSVRC \& COCO 2015 competitions,ILSVRC 2015 classification task,ImageNet dataset,ImageNet localization,ImageNet test set,VGG nets,deep residual learning,deep residual nets,deeper neural network training,residual function learning,residual nets,visual recognition tasks,Complexity theory,Degradation},
  pages = {770-778},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/KUCF9Q86/7780459.html}
}

@inproceedings{eitel_multimodal_2015,
  title = {Multimodal Deep Learning for Robust {{RGB}}-{{D}} Object Recognition},
  doi = {10.1109/IROS.2015.7353446},
  abstract = {Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams - one for each modality - which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset [15] and show recognition in challenging RGB-D real-world noisy settings.},
  booktitle = {2015 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Eitel, Andreas and Springenberg, Jost Tobias and Spinello, Luciano and Riedmiller, Martin and Burgard, Wolfram},
  month = sep,
  year = {2015},
  keywords = {learning (artificial intelligence),image colour analysis,object recognition,convolutional neural networks,Training,feature extraction,feedforward neural nets,image fusion,robot vision,CNN,RGB-D architecture,RGB-D object dataset,RGB-D real-world noisy settings,accurate learning,data augmentation scheme,fusion network,imperfect sensor data,multimodal deep learning,multistage training methodology,real-world robotics applications,real-world robotics tasks,realistic noise patterns,robust RGB-D object recognition,robust learning,Image coding,Robot sensing systems,Robustness,Streaming media},
  pages = {681-687},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/RDXQ8H3F/Eitel et al_2015_Multimodal deep learning for robust RGB-D object recognition.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/Q6868ZET/7353446.html}
}

@article{badrinarayanan_segnet_2017,
  title = {{{SegNet}}: {{A Deep Convolutional Encoder}}-{{Decoder Architecture}} for {{Scene Segmentation}}},
  volume = {39},
  issn = {0162-8828},
  shorttitle = {{{SegNet}}},
  doi = {10.1109/TPAMI.2016.2644615},
  abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.a- .uk/projects/segnet/.},
  number = {12},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  month = dec,
  year = {2017},
  keywords = {Neural networks,Computer architecture,Semantics,Training,image segmentation,Decoding,Roads,Decoder,Deep Convolutional Neural Networks,Encoder,Indoor Scenes,Pooling,Road Scenes,Semantic Pixel-Wise Segmentation,Upsampling},
  pages = {2481-2495},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9SZV73FK/7803544.html}
}

@inproceedings{lee_rdfnet_2017,
  title = {{{RDFNet}}: {{RGB}}-{{D Multi}}-Level {{Residual Feature Fusion}} for {{Indoor Semantic Segmentation}}},
  shorttitle = {{{RDFNet}}},
  doi = {10.1109/ICCV.2017.533},
  abstract = {In multi-class indoor semantic segmentation using RGB-D data, it has been shown that incorporating depth feature into RGB feature is helpful to improve segmentation accuracy. However, previous studies have not fully exploited the potentials of multi-modal feature fusion, e.g., simply concatenating RGB and depth features or averaging RGB and depth score maps. To learn the optimal fusion of multimodal features, this paper presents a novel network that extends the core idea of residual learning to RGB-D semantic segmentation. Our network effectively captures multilevel RGB-D CNN features by including multi-modal feature fusion blocks and multi-level feature refinement blocks. Feature fusion blocks learn residual RGB and depth features and their combinations to fully exploit the complementary characteristics of RGB and depth data. Feature refinement blocks learn the combination of fused features from multiple levels to enable high-resolution prediction. Our network can efficiently train discriminative multi-level features from each modality end-to-end by taking full advantage of skip-connections. Our comprehensive experiments demonstrate that the proposed architecture achieves the state-of-the-art accuracy on two challenging RGB-D indoor datasets, NYUDv2 and SUN RGB-D.},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Lee, S. and Park, S. J. and Hong, K. S.},
  month = oct,
  year = {2017},
  keywords = {object detection,learning (artificial intelligence),image colour analysis,Computer architecture,Semantics,feature extraction,image classification,image segmentation,image fusion,SUN RGB-D,Image segmentation,Feature extraction,Convolution,Fuses,depth data,multimodal features,depth features,depth score maps,modality end-to-end,multiclass indoor semantic segmentation,multilevel feature refinement blocks,multilevel features,multilevel RGB-D CNN features,multimodal feature fusion blocks,optimal fusion,residual feature fusion,residual learning,RGB feature,RGB-D data,RGB-D indoor datasets,RGB-D multilevel,RGB-D semantic segmentation,SUN},
  pages = {4990-4999},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/NPSTZJ8L/Lee et al. - 2017 - RDFNet RGB-D Multi-level Residual Feature Fusion .pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VT3RKKVM/8237795.html}
}

@phdthesis{maggiori_learning_2017,
  title = {Learning Approaches for Large-Scale Remote Sensing Image Classification},
  abstract = {The analysis of airborne and satellite images is one of the core subjects in remote sensing. In recent years, technological developments have facilitated the availability of large-scale sources of data, which cover significant extents of the earth's surface, often at impressive spatial resolutions. In addition to the evident computational complexity issues that arise, one of the current challenges is to handle the variability in the appearance of the objects across different geographic regions. For this, it is necessary to design classification methods that go beyond the analysis of individual pixel spectra, introducing higher-level contextual information in the process. In this thesis, we first propose a method to perform classification with shape priors, based on the optimization of a hierarchical subdivision data structure. We then delve into the use of the increasingly popular convolutional neural networks (CNNs) to learn deep hierarchical contextual features. We investigate CNNs from multiple angles, in order to address the different points required to adapt them to our problem. Among other subjects, we propose different solutions to output high-resolution classification maps and we study the acquisition of training data. We also created a dataset of aerial images over dissimilar locations, and assess the generalization capabilities of CNNs. Finally, we propose a technique to polygonize the output classification maps, so as to integrate them into operational geographic information systems, thus completing the typical processing pipeline observed in a wide number of applications. Throughout this thesis, we experiment on hyperspectral, atellite and aerial images, with scalability, generalization and applicability goals in mind.},
  language = {en},
  school = {Universit{\'e} C{\^o}te d'Azur},
  author = {Maggiori, Emmanuel},
  month = jun,
  year = {2017},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/6INDP93T/Maggiori - 2017 - Learning approaches for large-scale remote sensing.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/6BJQWUL2/tel-01589661.html}
}

@misc{noauthor_building_nodate,
  title = {Building {{Extraction}} from {{Remote Sensing Data Using Fully Convolutional Networks}} - {{Semantic Scholar}}},
  abstract = {Building detection and footprint extraction are highly demanded for many remote sensing applications. Though most previous works have shown promising results, the automatic extraction of building footprints still remains a nontrivial topic, especially in complex urban areas. Recently developed extensions of the CNN framework made it possible to perform dense pixel-wise classification of input images. Based on these abilities we propose a methodology, which automatically generates a full resolution binary building mask out of a Digital Surface Model (DSM) using a Fully Convolution Network (FCN) architecture. The advantage of using the depth information is that it provides geometrical silhouettes and allows a better separation of buildings from background as well as through its invariance to illumination and color variations. The proposed framework has mainly two steps. Firstly, the FCN is trained on a large set of patches consisting of normalized DSM (nDSM) as inputs and available ground truth building mask as target outputs. Secondly, the generated predictions from FCN are viewed as unary terms for a Fully connected Conditional Random Fields (FCRF), which enables us to create a final binary building mask. A series of experiments demonstrate that our methodology is able to extract accurate building footprints which are close to the buildings original shapes to a high degree. The quantitative and qualitative analysis show the significant improvements of the results in contrast to the multy-layer fully connected network from our previous work.},
  howpublished = {/paper/Building-Extraction-from-Remote-Sensing-Data-Using-Bittner-Cui/8607c1a6669f26c6a2b517603eeb7897160335ca},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HRA496KF/8607c1a6669f26c6a2b517603eeb7897160335ca.html}
}


