%!TEX root = Manuscrit.tex
\chapter{État de l'art}
\label{chap:etat}
%\citationChap{An algorithm must be seen to be believed.}{Donald Knuth}
\minitoc
\newpage

\begin{figure}
  \begin{subfigure}{0.5\textwidth}
     \includegraphics[width=\textwidth]{turing}
     %\label{fig:turing}
  \end{subfigure}
  \begin{subfigure}{0.5\textwidth}
     \includegraphics[width=\textwidth]{summer_vision}
     %\label{fig:summer_vision}
  \end{subfigure}
  \caption{Introductions de~\citet{turing_computing_1950} et~\citet{papert_summer_1966}, deux documents fondateurs de l'intelligence artificielle et de la vision par ordinateur.}
  \label{fig:introductions}
\end{figure}

Cette thèse s'inscrit dans une longue lignée de travaux en vision par ordinateur, intelligence artificielle et observation de la Terre. Dans ce chapitre, nous détaillons les contributions fondamentales sur lesquelles nous nous appuierons par la suite. En particulier, nous exposerons dans un premier temps les bases de l'apprentissage profond et en particulier le cadre théorique d'application des réseaux de neurones convolutifs pour le traitement d'image. Dans un second temps, nous détaillerons plus spécifiquement comment ce type modèles est appliqué dans l'état de l'art à la compréhension de scènes par le biais de la tâche de segmentation sémantique. Enfin, nous présenterons les travaux récents en interprétation automatique d'images de télédétection afin de faire apparaître les spécificités du cadre applicatif lié à l'observation de la Terre.

\section{Apprentissage profond pour la vision artificielle}

\subsection{Historique de l'apprentissage profond}

La possibilité d'attribuer des capacités cognitives à un ordinateur a été originellement formalisé par Alan Turing en 1950~\cite{turing_computing_1950}. Turing propose un cadre formel permettant de répondre à la question \og{} une machine peut-elle penser \fg{} en définissant ce qui est désormais connu comme le test de Turing. Selon lui, un ordinateur intelligent serait défini par sa capacité à imiter un humain de manière à ce que d'autres individus soient incapables de discerner sa véritable nature. Toutefois, Turing ne répond pas à la question du \og comment \fg{} et ne propose pas d'implémentation permettant d'atteindre cet objectif.

Néanmoins, en 1943, Warren McCulloch et Walter Pitts proposaient un système de neurones booléens~\cite{mcculloch_logical_1943} munis de deux état\,: actifs ou inactifs. Ils définissent un neurone formel comme étant un automate fini muni d'une fonction de transfert, permettant de transformer un ensemble d'entrée en une valeur de sortie. Certains neurones ne recevaient aucun signal d'un autre neurone, mais formaient eux-mêmes l'ensemble du signal d'entrées pour le reste du réseau. Les autres neurones calculaient alors des combinaisons logiques à partir des signaux qu'ils recevaient en entrée. Les travaux de McCulloch et Pitts montrent que de nombreux prédicats de logique temporelle sont calculables par de tels réseaux booléens. Cette étude théorique est étendue par Kleene qui étudie notamment des réseaux booléens dont le graphe présente des cycles, c'est-à-dire des réseaux \emph{récurrents}. Kleene montre notamment que de tels réseaux, qui correspondent en réalité à des automates finis, sont capables de modéliser des langages rationnels, c'est-à-dire des langages définis par des expressions régulières~\cite{kleene_representation_1956}.

En parallèle, les travaux du neuropsychologue Donald Hebb sur les sciences cognitives ont permis de mettre en avant l'idée de l'apprentissage hebbien. Celui-ci explique le mécanisme d'apprentissage au sein du cerveau par le renforcement d'une connexion entre deux neurones à chacune de leurs activations simultanées~\cite{hebb_organization_1949}. Hebb théorise également la possibilité que des neurones se regroupent en \og{} assemblées de cellules \fg{} qui s'activeraient de façon synchronisée, formant ainsi une représentation mentale des signaux envoyés au cerveau. Comme nous allons le voir, ces deux idées forment une source considérable d'inspiration pour les mécanismes d'apprentissage en intelligence artificielle.

En 1957, Frank Rosenblatt définit le Perceptron~\cite{rosenblatt_perceptron_1957}. Il s'agit d'un réseau de neurones acyclique, comme ceux de McCulloch et Pitts~\cite{mcculloch_logical_1943}. Les entrées et sorties sont booléennes et le réseau n'a qu'une seule couche. En outre, les poids des connexions sont déterminées automatiquement en utilisant la règle de Hebb~\cite{hebb_organization_1949}. En parallèle, Bernard Widrow construit physiquement l'ADALINE~\cite{widrow_adaptive_1960} en utilisant des memistors, en s'inspirant du modèle de McCulloch et Pitts. L'ADALINE est similaire dans sa conception au perceptron\,: il s'agit d'un réseau à une couche, linéaire et opérant sur la somme pondérée de ses entrées, qui utilise un algorithme de descente de gradient minimisant son erreur quadratique afin de mettre à jour ses poids. Cependant, ces deux modèles souffrent d'une limitation importante. Dans le cas où les données d'entrée sont linéairement séparables, le perceptron est assuré de pouvoir trouver la séparatrice optimale. Mais ces classifieurs sont linéaires et ne peuvent donc pas résoudre de problèmes non-linéairement séparables. Dans le livre \emph{Perceptrons}~\cite{minsky_perceptrons_1969}, Minsky et Papert montrent qu'un perceptron à une seule couche cachée est incapable de calculer la fonction XOR, quel que soit son nombre de neurones et en dépit de la simplicité apparente de l'opération. Toutefois, il n'existe pas encore de bonne stratégie pour optimiser les poids d'un perceptron à plusieurs couches qui intégreraient une non-linéarité. Les réseaux de neurones sont ignorés pendant plusieurs années. Dans sa thèse soutenue en 1975~\cite{werbos_beyond_1975}, Paul Werbos formalise un algorithme de descente de gradient pour la minimisation d'erreur dans un réseau de neurones en utilisant le théorème de dérivation des fonctions composées, qu'il nomme \emph{backpropagation} (rétro-propagation). Il faudra toutefois attendre dix ans pour voir apparaître les premières implémentations de l'algorithme de rétro-propagation du gradient pour entraîner des perceptrons multi-couches~\cite{rumelhart_learning_1986,lecun_learning_1986}.\footnote{L'ADALINE sera également adapté en multi-couches avec sa variante MADALINE~\cite{winter_madaline_1988}, utilisant un algorithme spécifique car utilisant la fonction d'activation $\operatorname{signe}$ dont le gradient est nul presque partout. Widrow et Lehr convergent deux ans plus tard vers une structure de Madaline utilisant la fonction sigmoïde comme activation, entraînable par rétropropagation.}

L'étude théorique des réseaux de neurones à propagation avant, et notamment des perceptrons, reprend alors. En 1989, George Cybenko démontre le théorème d'approximation universelle prouvant que les fonctions calculables par un perceptron sont denses dans l'ensemble des fonctions continues par morceaux, dans le cas de la sigmoïde comme fonction de transfert~\cite{cybenko_approximation_1989}. Ce résultat est étendu deux ans plus tard par Kurt Hornik en le généralisant à l'ensemble des fonctions d'activation~\cite{hornik_approximation_1991}. Le théorème formel est donné ci-dessous.

\begin{theorem}
Soit $\varphi$ une fonction bornée, croissante non-constante. Soit $C_0^n$ l'ensemble des fonctions continues définies sur $[0,1]^n$. Alors\,:
$$\forall \epsilon > 0, \forall F \in C_0^n, \exists N \in \mathbb{N}^{*}, \text{ des réels } v_i, b_i \in \mathbb{R} \text{ et des vecteurs } \mathbf{w}_i \in \mathbb{R}^n \text{ avec } i \in \{1\dots{}n\} \text{ tels que}$$
$$\hat{F} : \mathbf{x} \rightarrow \sum_{i=1}^N v_i \varphi\left(\mathbf{w}_i^t \mathbf{x} + b_i \right)$$
soit une approximation de $F$ à $\epsilon$ près, c'est-à-dire\,:
$$\forall \mathbf{x} \in [0,1]^n, ~\left| F(\mathbf{x}) - \hat{F}(\mathbf{x}) \right| < \epsilon~.$$
\end{theorem}

Autrement formulé, cela signifie que que toute fonction relativement régulière (continue par morceaux sur un ensemble de compacts) peut être approximée avec une précision arbitraire par un perceptron. Ce théorème très fort suggère donc que de tels réseaux de neurones artificiels peuvent simuler quasiment n'importe quelle fonction, sans pour autant donner de méthode de construction de tels réseaux. En parallèle de ces avancées théoriques, les applications pratiques des perceptrons étaient étudiées, notamment dans le cadre de la vision artificielle pour la reconnaissance de formes. Ainsi, un des premiers problèmes étudiés est la reconnaissance de caractères écrits, en particulier les chiffres et les lettres. En 1980, Kunihiko Fukushima introduit le \emph{Neocognitron}~\cite{fukushima_neocognitron_1980}, un perceptron multi-couches dont la structure est inspirée par les travaux de Hubel et Wiesel sur les cortex visuels des chats et des singes~\cite{hubel_receptive_1959,hubel_receptive_1968}. Le \emph{Neocognitron} extrait des caractéristiques locales de l'image robustes aux légères déformations, qui sont graduellement combinées en cascade par le réseau. En 1989, \citet{lecun_backpropagation_1989} proposent une architecture de perceptron multi-couche dont la première couche est \emph{convolutive} pour la reconnaissance de chiffres manuscrits. Cette approche est reprise par la suite pour donner naissance à l'architecture LeNet-5~\cite{lecun_gradient-based_1998}, premier \gls{CNN} moderne. En 2004, les méthodes de détection et classification d'objets par \gls{CNN} sont évaluées comme étant compétitives, voire supérieure, que celles obtenues à partir de \gls{SVM} opérant directement sur les pixels. Des premiers travaux apparaissent utilisant les représentations apprises par les \gls{CNN} pour remplacer les descripteurs images \emph{ad hoc} comme \gls{SIFT}~\cite{lowe_object_1999} ou \gls{HOG}~\cite{dalal_histograms_2005} pour la classification d'objets~\cite{serre_object_2005,huang_large-scale_2006}.

En 2006, Hinton et Salakhutdinov introduisent les réseaux de neurones auto-encodeurs pour la réduction de dimension~\cite{hinton_reducing_2006}. Leur approche utilise une pile de \gls{RBM}~\cite{ackley_learning_1985,salakhutdinov_deep_2009}, entraînées successivement couche par couche, chacune étant traitée comme entrée pour la suivante. Ce modèle hybride exploré dans un article de 2006~\cite{hinton_fast_2006} qui leur donne le nom de \gls{DBN}. L'année suivante, une équipe menée par Yoshua Bengio étend ce pré-entraînement par couche à des \gls{DBN} pour la régression~\cite{bengio_greedy_2007}. En outre, leurs travaux suggèrent que le pré-entraînement permet d'initialiser les couches supérieures à partir de meilleures représentations des abstractions de haut niveau que l'initialisation aléatoire. Bengio défend par ailleurs l'idée qu'un bon algorithme d'apprentissage doit être capable, en temps raisonnable, d'apprendre des représentations sémantiques pertinentes à des niveaux d'abstraction variés à partir de données non nécessairement annotées, c'est-à-dire en apprentissage non supervisé~\cite{bengio_learning_2009}. Il argumente en faveur des modèles profonds comme étant plus expressifs et pouvant apprendre des meilleures représentations en s'appuyant sur des travaux en neurosciences concernant le cortex visuel~\cite{serre_quantitative_2007}. L'introduction de fonctions d'activation non-saturantes en 2011~\cite{glorot_deep_2011} permettent de s'affranchir du pré-entraînement et des problèmes d'explosion des gradients, rendant alors possible l'entraînement d'architectures plus profondes.

Par ailleurs, c'est également en 2006 que les premières implémentations des \gls{CNN} sur \gls{GPU} voient le jour~\cite{chellapilla_high_2006} pour le traitement automatisée de documents, puis pour l'apprentissage non-supervisé de \gls{DBN}~\cite{raina_large-scale_2009} et la reconnaissance de caractères~\cite{ciresan_deep_2010}. En 2011, Dan Cire\c{s}an propose des méthodes à base de \gls{CNN} qui obtiennent la première place dans deux compétitions\,: reconnaissance de caractères chinois~\cite{liu_icdar_2011} et classification de panneaux de signalisation~\cite{stallkamp_german_2011}. Ces \gls{CNN} obtiennent en outre d'excellents résultats en reconnaissances de caractères latins et en classification d'objets pour des petits images sur la base de données CIFAR-10~\cite{ciresan_multi-column_2012}. En 2010, la compétition \gls{ILSVRC} de reconnaissance d'objets démarre en utilisant la banque de données ImageNet~\cite{deng_imagenet_2009} comme référence. Un million d'images sont annotées pour mille classes d'intérêt différentes. En 2012, la compétition est remportée par Alex Krizhevsky, Ilya Sutskever et Geoffrey Hinton à l'aide du réseau convolutif profond AlexNet~\cite{krizhevsky_imagenet_2012} implémenté sur \gls{GPU} à l'aide de la bibliothèque \gls{CUDA}. AlexNet obtient 15\% d'erreur, tandis que la seconde méthode du podium n'obtient que 26\%. Ce succès a été à l'origine de l'explosion en popularité des réseaux convolutifs et de l'apprentissage profond, notamment dans la communauté vision. La compétition \gls{ILSVRC} est depuis dominée par les approches \gls{CNN}, aussi bien en reconnaissance qu'en détection et segmentation d'objets. Le succès récent des réseaux convolutifs profonds est donc dû à la convergence de trois facteurs\,: des avancée théoriques (\gls{ReLU}, réseaux convolutifs) permettant d'entraîner des réseaux plus profonds, la mise à disposition de grandes bases de données annotées pour l'apprentissage supervisées et des implémentations rapides sur \gls{GPU} rendant les temps de calcul tolérables.

\begin{figure}[t]
  \resizebox{\textwidth}{!}{
  \input{Chapitre1/neurone.tikz}
  }
\caption{Modélisation d'un neurone artificiel.}
\label{fig:neurone}
\end{figure}

Dans la suite, nous explicitons le cadre formel des réseaux de neurones artificiels, avant de s'intéresser aux méthodes permettant de les entraîner pour différentes tâches avant de s'intéresser plus particulièrement aux modèles convolutifs.

\subsection{Réseaux de neurones artificiels}

\begin{figure}[t]
  \begin{subfigure}[b]{0.5\textwidth}
    \resizebox{\textwidth}{!}{
    \input{Chapitre1/perceptron.tikz}
    }
  \caption{Perceptron à une couche cachée.}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \resizebox{\textwidth}{!}{
    \input{Chapitre1/mlp.tikz}
    }
  \caption{Perceptron multi-couches.}
  \end{subfigure}
  \caption{Perceptron à une et plusieurs couches. Les entrées et sorties peuvent être de dimensions variables et sont représentées comme des neurones.}
  \label{fig:perceptron}
\end{figure}

La définition formelle d'un neurone artificiel a été introduite par McCulloch et Pitts~\cite{lettvin_what_1959} en 1959. Un neurone doté d'une fonction de transfert $\varphi$ opère sur un ensemble de $n$ neurones d'entrée émettant chacun une valeur $x_1\dots{}x_n$, auxquelles il est connecté par des synapses de poids $w_i$. La valeur d'entrée $x$ du neurone correspond à la somme pondérée des signaux d'entrée. Le neurone émet ensuite l'image $z = \varphi(x)$ de ce signal par sa fonction de transfert. Le schéma de la~\cref{fig:neurone} décrit ce procédé. L'activation en sortie d'un neurone s'obtient donc par la formule
\begin{equation}
z = \varphi\left(\sum_{i=1}^n w_i x_i + b\right)~.
\end{equation}

Plusieurs neurones connectés les uns aux autres forment un graphe orienté et pondéré. Un réseau de neurones à propagation avant désigne un graphe neuronal acyclique. En pratique, on considère des graphes $k$-partis que l'on peut alors représenter par \og couches \fg. Dans le cas d'un perceptron à couches multiples, les valeurs du signal d'entrée sont placées dans une couche d'entrée et les signaux de sortie envoyés dans une couche de sortie. La ou les couches de neurones réellement optimisables sont nommées \og couches cachées \fg{} et font l'interface entre l'entrée et la sortie. Les perceptrons multi-couche à une et plusieurs couches cachées sont illustrés dans la~\cref{fig:perceptron}.

La fonction d'activation des neurones peut prendre de nombreuses formes. Il est \emph{a minima} nécessaire que celle-ci soit non-linéaire, sans quoi elle rend le perceptron multi-couche équivalent au perceptron simple, et presque partout différentiable afin de pouvoir appliquer l'algorithme de rétro-propagation du gradient. La fonction d'activation est en outre généralement choisie monotone croissante et de dérivée monotone croissante. La~\cref{fig:saturantes} illustre plusieurs activations communément utilisées dans les réseaux de neurones artificiels profonds\,:
\begin{itemize}
  \item La \textbf{sigmoïde}, ou fonction logistique\,: $\sigma(x) = \frac{1}{1 + e^{-x}}$.
  \item La fonction \textbf{tangente hyperbolique}\,: $\tanh(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}$.
  \item La \textbf{marche de Heavyside}\,: $H(x) = 0 \text{ si } x < 0 \text{ et } 1 \text{ si } x \geq 0$.
  \item La fonction \textbf{\gls{ReLU}}\,: $ReLU(x) = \max(0,x)$.
\end{itemize}

\begin{figure}[t]
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{activations}
    \caption{Fonctions d'activation saturantes.}
    \label{fig:saturantes}
  \end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
  \includegraphics[width=\textwidth]{rectifiers}
  \caption{Fonctions d'activation non-saturantes.}
  \label{fig:rectifiers}
\end{subfigure}
\caption{Exemples de fonctions d'activation.}
\label{fig:activations}
\end{figure}

La marche de Heavyside, tout comme la fonction $\operatorname{signe}$, a des gradients nuls presque partout, sa dérivée étant l'impulsion de Dirac $\delta$. Ceci les rend peu utilisées en pratique car il est impossible de leur appliquer l'algorithme de rétro-propagation du gradient.
La fonction sigmoïde, bien que populaire à l'origine, est une fonction saturante qui souffre de gradients évanescents. Ce problème n'est pas circonscrit à la sigmoïde et est particulièrement pregnant dans le cas des réseaux de neurones récurrents~\cite{hochreiter_gradient_2001}. L'accumulation de couches dans un réseau fait que la rétro-propagation du gradient est similaire à une suite géométrique. Le produit cumulé de $n$ gradients dans des fonctions d'activation à tendance contractante produit un $n+1$ gradient plus faible que le précédent, et ainsi de suite. À l'inverse, il est possible d'avoir des gradients explosifs. Ce problème est empiré dans le cas des fonctions saturants comme la sigmoïde ou la tangente hyperbolique car leurs gradients sont nécessairement dans $[0,1]$. L'utilisation de la sigmoïde ou la tangente hyperbolique a varié dans la littérature. \citet{lecun_efficient_1998} recommande d'utiliser la fonction hyperbolique modifiée $f(x) = 1,7159 tanh(\frac{2}{3} x)$, notamment car celle-ci est bornée par $[-1,1]$ et centrée en 0, ce qui est adapté au travail sur des données normalisées à moyenne nulle et variance unitaire.

Pour limiter les gradients évanescents, des fonctions non-linéaires non-saturantes sont désormais majoritairement utilisées dans la littérature. \citet{glorot_deep_2011} ont ainsi proposé d'utiliser la fonction \gls{ReLU}, introduite précédemment pour les \gls{DBN}~\cite{nair_rectified_2010}, et la fonction SoftPlus pour les réseaux de neurones artificiels de toutes sortes. Leurs travaux aboutissent à trois conclusions importantes. Tout d'abord, les fonctions d'activation non-linéaires obtiennent généralement de meilleures performances que les réseaux utilisant $tanh$. Ensuite, les réseaux entraînés avec \gls{ReLU} ne nécessitent pas de pré-apprentissage non-supervisé couche par couche, ce qui accélère grandement les temps d'entraînement. Enfin, ces modèles sont plus parcimonieux que leurs équivalents usuels. En outre, la fonction \gls{ReLU} est simple à implémenter et rapide à calculer, ce qui a contribué à sa popularisation rapide dans la communauté.

Bien que ces hypothèses ne semblent pas nécessaires à la construction de réseaux profonds~\cite{oyallon_building_2017}, la plupart des fonctions d'activation usuelles sont continues, monotones, contractantes et s'appliquent indépendamment sur chaque activation. Plusieurs variantes ont été proposées autour des fonctions linéaires rectifiées, notamment une version paramétrique dont la partie négative est non-saturante, mais linéaire de pente $\alpha$ fixée par l'utilisateur (\emph{Leaky ReLU}~\cite{maas_rectifier_2013}) ou apprenable (\gls{PReLU}~\cite{he_delving_2015}). Une version similaire mais continue a également été proposée sous la forme des \gls{ELU}~\cite{clevert_fast_2015}. Plus récemment, une recherche automatique quasi-exhaustive sur une variété de fonctions d'activation possibles a permis de proposer la \emph{Swish}~\cite{ramachandran_searching_2018}, non-linéaire non-saturante mais ayant la particularité d'être non-monotone. Certaines de ces variantes sont illustrées dans la~\cref{fig:rectifiers} et leur formule est donnée ci-dessous\,:
\begin{itemize}
  \item La fonction \textbf{\emph{Leaky ReLU}}\,: $LReLU_\alpha(x) = max(0,x) - \alpha max(0,-x)$, avec $\alpha$ un hyperparamètre.
  \item La fonction \textbf{\gls{PReLU}}\,: $PReLU(x, \alpha) = max(0,x) - \alpha max(0,-x)$, avec $\alpha$ optimisable.
  \item La fonction \textbf{\gls{ELU}}\,: $ELU_\alpha(x) = x \text{ si } x > 0 \text{ et } \alpha (exp(x) - 1) \text{ sinon}$.
  \item La fonction \textbf{SoftPlus}\,: $s^+(x) = \ln(1 + e^x)$.
  %\item La fonction \textbf{\emph{Swish}}\,: $Swish_\beta(x) = x . \sigma(\beta x) = x . (1 + e^{-\beta x})^{-1}$.
\end{itemize}

Le théorème d'approximation universelle~\cite{cybenko_approximation_1989,hornik_approximation_1991} indique que l'ensemble des fonctions engendrées par les perceptrons est dense dans l'ensemble des fonctions continues par morceaux sur des compacts. Autrement dit, toute fonction $f : E \rightarrow \mathbb{R}^m$ avec $E = \bigcup_k C_k$ une union de compacts de $\mathbb{R}^n$ continue sur chaque compact peut être approximée à une précision $\epsilon$ arbitraire par un perceptron. On peut voir deux limitations à ce résultat. Le premier concerne la nature de la fonction d'activation. Le théorème généralisé par Hornik s'applique à tous les perceptrons sur lesquels on applique une fonction d'activation monotone non-constante et bornée. Cela inclut les fonctions saturantes $tanh$ et sigmoïde, mais exclut les fonctions linéaires rectifiées comme \gls{ReLU}. Ce problème a cependant été levé par Sonoda et Murata~\cite{sonoda_neural_2017}, qui ont montré que des réseaux munis de fonctions d'activation non-bornées satisfont tout de même le théorème d'approximation universelle


La deuxième limitation porte quant à elle sur la nature même du théorème. Celui-ci ne donne qu'une garantie purement théorique concernant l'existence d'un ensemble de paramètres permettant l'approximation à la précision voulue. Il ne donne pas de bornes sur le nombre de neurones nécessaire, ni de méthode de construction. Ainsi, il n'y aucune certitude que les poids puissent être obtenus par descente de gradient, ni d'indication sur l'architecture à choisir. En particulier, le théorème vaut pour des réseaux superficiels à une seule couche. Pourtant, le consensus scientifique dans la communauté tend à préférer des réseaux de plus en plus profonds. Ainsi, il a été démontré que des réseaux plus profonds demandent moins de poids et de neurones pour approximer des fonctions plus complexes~\cite{bianchini_complexity_2014,mhaskar_when_2017}. En particulier, la structure hiérarchique des réseaux multi-couches semble adapté à l'approximation de fonctions composées en contournant la malédiction de la dimension~\cite{poggio_why_2017}. Toutefois, cela augmente également le nombre d'hyperparamètres à régler pour définir l'architecture. L'absence de méthode systématique de construction des architectures de réseau profond conduit donc à l'utilisation intensive de l'approche essai-erreur ou à des méthodes de méta-apprentissage~\cite{zoph_neural_2016}.

\subsection{Entraînement des réseaux de neurones}

Un autre inconvénient au théorème d'approximation universelle est que même si l'architecture était connue, celui-ci ne donne aucune méthode pour trouver les valeurs des paramètres. En particulier, l'algorithme de rétro-propagation du gradient~\cite{werbos_beyond_1975,rumelhart_learning_1986,lecun_learning_1986} décrit ci-après se base sur une méthode de descente de gradient. Toutefois, rien ne garantit que les poids optimaux dont l'existence est assurée par le théorème d'approximation universelle ne soit trouvables par cet algorithme.

On peut alors utiliser l'algorithme de descente de gradient~\cite{cauchy_comptes_1847} afin de minimiser l'erreur totale en mettant à jour les poids. Cet algorithme, dit de la plus forte pente, permet d'approcher des minima locaux d'une fonction $f$ arbitraire en cherchant ses points stationnaires, c'est-à-dire les points où le gradient de $f$ s'annule. L'algorithme repose sur l'idée que $f$ décroît le plus rapidement dans la direction opposée à son gradient et fonctionne de la façon suivante\,:

\begin{definition}
  \label{eq:sgd}
  Algorithme de descente de gradient\,:

  Soit $f : \mathbb{R}^n \rightarrow \mathbb{R}$ une fonction différentiable et $\nabla f$ son gradient. Soit $x_0 \in \mathbb{R}^n$ un point initial, $\epsilon > 0$ le seuil de tolérance de l'algorithme et $\alpha > 0$ le pas de la descente. On définit alors la suite $(x_i)_{i \ge 0} \in \mathbb{R}^\mathbb{N}$ telle que\,:
  $$x_{i+1} = x_i - \alpha \nabla f(x_i)~.$$
  L'algorithme s'arrête lorsque $\nabla f(x_i) \le \epsilon$ et renvoie $x_i$.
\end{definition}

\def\L{\mathcal{L}}

Dans le cas des réseaux de neurones, la fonction $\L$ est appelée \og fonction objectif \fg ou \og fonction de coût \fg et correspond à une mesure de la performance du modèle. Généralement, $\L$ est une indicatrice de l'erreur totale du modèle sur l'ensemble du jeu de données d'entraînement $\Omega$, que l'on va chercher à minimiser. Ainsi, l'optimisation du réseau de neurones consiste à résoudre l'équation\,:
$$W^* = \operatorname{argmin}_W \L(W, \Omega)$$
pour un modèle paramétré par ses poids $W$ et de fonction objectif $\L$.

Tant que la fonction de coût $\L$ est dérivable, il est possible de la minimiser en utilisant l'algorithme de descente de gradient. En particulier, il est possible de calculer la mise à jour des poids en rétro-propageant la valeur du gradient d'une couche à la précédente\,: c'est l'algorithme de rétro-propagation~\cite{werbos_beyond_1975,lecun_efficient_1998,rumelhart_learning_1986}. On va alors chercher à mettre à jour les poids du réseau dans la direction opposée du gradient par rapport à ces mêmes poids $\nabla_W \L(W, \Omega)$.

\begin{definition}
  Algorithme de descente de gradient appliqué à un réseau de neurones\,:
  \begin{enumerate}
    \item Initialiser aléatoirement les poids $W$.
    \item Calculer $\nabla_W \L(W, \Omega)$ sur l'ensemble du jeu de données.
    \item Tant que $\nabla_W \L(W, \Omega) > \epsilon$\,:
      \begin{itemize}
          \item $W \leftarrow W - \alpha \nabla_W \L(W, \Omega)$
      \end{itemize}
  \end{enumerate}
\end{definition}

 Cependant, le cardinal de $\Omega$ peut être très grand en pratique, les bases de données d'apprentissage pouvant contenir des milliers, voire des millions d'exemples. Par conséquent, il est généralement nécessaire d'utiliser une variante en ligne de l'algorithme de gradient, l'algorithme de descente de gradient \emph{stochastique}, qui réalise la mise à jour des poids pour chaque exemple, en approximant l'erreur moyenne à partir de l'erreur sur un seul échantillon, c'est-à-dire\,:
\begin{definition}
  Algorithme de descente de gradient stochastique\,:
  \begin{enumerate}
    \item Initialiser aléatoirement les poids $W$.
    \item Tant que le critère d'arrêt n'est pas atteint\,:
      \begin{itemize}
          \item Tirer aléatoirement un exemple d'apprentissage $\omega \in \Omega$
          \item $W \leftarrow W - \alpha \nabla_W \L(W, \omega)$
      \end{itemize}
  \end{enumerate}
L'algorithme s'arrête lorsque le critère d'arrêt est vérifié, généralement lorsqu'un nombre d'itérations prédéfini est atteint.
\end{definition}

L'inconvénient de l'algorithme de descente de gradient stochastique est que l'approximation du gradient $\nabla_W E(W, \omega)$ est très bruitée et peut changer fortement de direction entre deux itérations successives. Afin de stabiliser la convergence, on utilise le plus souvent l'algorithme de descente de gradient stochastique \emph{par mini-lots}, également appelés \emph{mini-batches}. On approxime alors le gradient global non pas à partir d'un échantillon, mais à partir de la moyenne sur un nombre $k$ d'échantillons, qui forment un mini-lot ou \emph{mini-batch}\,:
\begin{definition}
  Algorithme de descente de gradient stochastique par mini-lots\,:
  \begin{enumerate}
    \item Initialiser aléatoirement les poids $W$.
    \item Tant que le critère d'arrêt n'est pas atteint\,:
      \begin{itemize}
          \item Tirer aléatoirement $k$ exemples d'apprentissage $(\omega_1,\dots,\omega_k) \in \Omega^k$
          \item $W \leftarrow W - \alpha \frac{1}{k} \sum_{i=1}^k \nabla_W \L(W, \omega_i)$
      \end{itemize}
  \end{enumerate}
L'algorithme s'arrête lorsque le critère d'arrêt est vérifié, généralement lorsqu'un nombre d'itérations prédéfini est atteint.
\end{definition}

La mise à jour s'appliquant sur l'ensemble des couches du réseau, il est donc nécessaire de pouvoir calculer $\frac{\partial \L}{\partial w_i}$ pour chaque ensemble de paramètres $w_i$. Toutefois, le calcul direct du gradient de $\L$ ne peut s'effectuer que sur la dernière couche. Pour remonter aux dérivées partielles des couches précédentes, il est nécessaire d'utiliser l'algorithme de rétro-propagation du gradient.

L'algorithme de rétro-propagation du gradient se fonde sur la règle de dérivation en chaîne, c'est-à-dire le théorème de dérivation des fonctions composées~\cite{lhospital_analyse_1716,lagrange_theorie_1797}\,:
\begin{theorem}
Soient $f$ et $g$ deux fonctions telles que $f : I \rightarrow J \subset \mathbb{R}$ et $g : J \rightarrow \mathbb{R}$. Soit $x \in I$ tel que $f$ admet une dérivée en $x$. Alors, la fonction composée $h = g \circ f : I \rightarrow \mathbb{R}$ admet une dérivée en $x$ de valeur :
$$h'(x) = (g \circ f)'(x) = f'(x) \times g'(f(x))~.$$

Si $f$ et $g$ sont dérivables respectivement sur $I$ et $J$, alors\,:
$$(g \circ f)' = f' \times (g' \circ f)~.$$

ou encore, en utilisant la notation de Leibniz, avec $z = g(y)$ et $y = f(x)$:
$$\frac{\diff z}{\diff x} = \frac{\diff z}{\diff y} \times \frac{\diff y}{\diff x}~.$$
\end{theorem}

Ce théorème s'étend aux dérivées partielles de fonctions à valeurs dans $\mathbb{R}^n$.

Pour diminuer l'erreur, il est nécessaire de mettre à jour les poids $w^k$ dans la direction opposée au gradient $\frac{\diff e}{\diff w^k}$. Or, grâce à la règle de la dérivation en chaîne\,:
$$\frac{\partial \L}{\diff w^k} = \frac{\partial \L}{\diff z^k} \times \frac{\partial z^k}{\partial w^k} = \frac{\partial \L}{\partial z^{(k+1)}} \times \frac{\partial z^{(k+1)}}{\partial z^k} \times \frac{\partial z^k}{\partial w^k}$$.

Autrement dit, il est possible de remonter le réseau en partant des couches les plus profondes jusqu'aux premières couches afin de faire remonter le gradient $\frac{\partial \L}{\partial w^k}$. Pour obtenir le gradient de l'erreur par rapport aux poids d'une couche, il est nécessaire de calculer le gradient des sorties par rapport aux poids ainsi que le gradient des sorties par rapport aux entrées.

Dans la réalité, les fonctions mises en jeu opèrent non pas sur des vecteurs mais sur des tenseurs $\mathbf{x}, \mathbf{y}, \mathbf{z}$. Toutefois, le théorème de dérivation des fonctions composées peut alors se réécrire en utilisant les jacobiennes $\mathbf{J}$\,:
$$\mathbf{J}_{F \circ G} = \mathbf{J}_F \circ G \cdot J_G$$
et l'algorithme de rétro-propagation s'applique encore.

C'est pour cette raison que les gradients peuvent devenir évanescents ou explosifs. De part les multiplications successives, les gradients suivent une croissance quasi-géométrique en remontant dans le réseau. Si la norme de la Jacobienne est systématiquement inférieure à 1, alors l'amplitude de ceux-ci tend vers $0$ et rend la convergence lente, voire impossible. Si la norme est plus grande que 1, alors les gradients croissent exponentiellement et les mises à jour des poids sont très instables.

% \begin{definition}
%   Algorithme de rétro-propagation du gradient\,:
%   $g \leftarrow \nabla_{z} \L(z, y)$
%
%   Pour chaque couche\,:
%   $g \leftarrow \nabla_{o^k} \L = g \otimes \phi'(o^k)$  calcul du gradient par rapport aux sorties avant activation
%   $\nabla$
% \end{definition}

En pratique, le modèle approxime une fonction $\mathcal{F}$ sur laquelle on applique une fonction de coût $\L$. Cette fonction de coût est une indicatrice de l'erreur commise par le réseau. Cette fonction doit vérifier la propriété\,:
$$\L(\widehat{\mathcal{F}_W}(x) - \mathcal{F}(x)) \rightarrow 0 \Rightarrow \hat{\mathcal{F}} \rightarrow \mathcal{F}~~,$$
c'est-à-dire que la minimisation de l'erreur implique la convergence du modèle vers la fonction réelle à approximer.

La nature exacte de la fonction de coût dépend de la tâche. Dans un cadre de régression, il est courant d'utiliser la norme $L_2$ ou la norme $L_1$. Pour chaque échantillon, on pourra alors comparer la prédiction $x$ avec la vérité terrain $y$ par
$$l(x,y) = |x - y| \text{ ou } l(x,y) = \lVert x - y \rVert~~.$$
La première correspond à la méthode des moindres carrés. La norme $L_1$ se montre plus robuste aux observations aberrantes, qui explosent dans le cas de la norme euclidienne. Toutefois, la norme $L_2$ a l'avantage d'être partout dérivable et est plus stable, notamment de par sa tolérance aux faibles erreurs (fonction contractante sur $[-1, 1]$).

Dans le cas de la classification, $y$ se présente sous la forme d'un encodage \emph{one-hot}. Pour un problème à $n$ classes, si $y$ appartient à la $k$\ieme classe, alors $y_i = \delta_{i,k}$ avec $\delta$ le symbole de Kronecker. Autrement dit, $y$ est le vecteur $(0, \dots, 0, 1, 0, \dots, 0)$ dont toutes les composantes sont nulles à l'exception de la $k$\ieme qui est unitaire. S'il est possible de traiter ces problèmes avec des fonctions de coût de régression, il est généralement plus pertinent d'utiliser la fonction d'entropie croisée. Celle-ci se calcule de la façon suivante\,:
\begin{equation}
  H(z,y) = -\sum_{i=1}^n y_i \log(z_i)~~.
\end{equation}

L'entropie croisée est particulièrement intéressante dans ce cas car sa minimisation coïncide avec celle de la divergence de Kullback-Leibler entre la distribution statistique des $x$ et des $y$, c'est-à-dire l'image de $\mathcal{F}$ et celle de $\hat{\mathcal{F}}$. Toutefois, dans ce cas il est attendu que $x$ soit un vecteur de probabilité, c'est-à-dire que $x_i \in [0,1]$ et $\sum_i x_i = 1$. En pratique, les activations en sortie du réseau sont donc passées dans une fonction \emph{softmax}\,:
\begin{equation}
z_i = \mathit{softmax}(x)_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
\end{equation}

qui est une généralisation de la fonction sigmoïde au cas multi-classes.

Il est important de noter que l'algorithme de descente de gradient ne dispose d'une convergence garantie que dans le cas où la fonction $\mathcal{L}$ est convexe, ce qui n'est jamais le cas en pratique pour des réseaux profonds. Des variantes de la descente de gradient ont été proposées pour améliorer ses propriétés de convergence. En particulier, le pas de la descente, noté $\alpha$ dans la \cref{eq:sgd}, joue un rôle important dans l'optimisation des réseaux de neurones profonds. Appelé généralement taux d'apprentissage, $\alpha$ contrôle l'amplitude des mises à jour des poids du modèle. Si $\alpha$ est très élevé, les mises à jour seront très grandes et la convergence très instable. Si $\alpha$ est très faible, la convergence peut être très lente voire se bloquer dans des minima locaux. La plupart des variantes de la descente de gradient stochastique utilisent des politiques particulières pour la mise à jour des poids. Des méthodes dites avec \og moment \fg, inspirées du moment cinétique en mécanique, permettent notamment de faire conserver au gradient une partie de sa vélocité des itérations précédentes pour limiter les oscillations le long des courbes de niveaux de la fonction~\cite{qian_momentum_1999,nesterov_method_1983}. \citet{sutskever_importance_2013} ont notamment montré que l'utilisation d'une descente de gradient stochastique à moment permet d'améliorer les performances finales du modèle, y compris dans le cas d'une poids initiaux mal choisis. \citet{polyak_acceleration_1992} proposent par ailleurs d'utiliser le gradient moyennée sur les $n$ dernières itérations pour la mise à jour des poids, ce qui correspond à une forme de mini-lot asynchrone.

Une autre source d'amélioration proposée réside dans la politique du choix de $\alpha$. En effet, rien ne contraint $\alpha$ à être constant dans l'algorithme de descente de gradient. Il est possible de l'ajuster manuellement au cours de l'apprentissage, par exemple en commençant avec un taux d'apprentissage élevé au début, puis en le multipliant par un ratio $\gamma < 1$ à intervalles réguliers. \citet{bottou_stochastic_2012} recommande ainsi une descente de gradient stochastique moyennée couplée avec une évolution de $\alpha$ suivant la relation $\alpha_{i+1} = \alpha_0 (1 + \gamma \cdot i)^{-1}$, tandis que \citet{loshchilov_sgdr_2016} utilisent une variante du recuit simulé. Toutefois, cela nécessite une intervention manuelle dans la mesure où il est nécessaire de configurer un certain nombre d'hyperparamètres au préalable. Plusieurs travaux se sont donc penchés sur des méthodes avec moment dites adaptatives, dans lesquelles le taux d'apprentissage $\alpha$ est automatiquement ajusté selon une heuristique~\cite{duchi_adaptive_2011,tielman_lecture_2012,zeiler_adadelta_2012,kingma_adam_2014}.

Indépendamment l'algorithme de descente de gradient utilisé, un point crucial dans l'optimisation des réseaux profonds réside dans le choix des poids initiaux. L'\emph{initialisation} de ceux-ci conditionne également la capacité de la descente gradient à converger vers un optimal de bonne qualité. Si, comme nous avons pu le voir, les méthodes de pré-entraînement non-supervisées ont été plébiscitées par le passé~\cite{hinton_fast_2006,bengio_greedy_2007}, les réseaux sont dorénavant directement entraînés de façon supervisée. L'idée principale concernant l'initialisation des poids consiste à leur assigner des valeurs aléatoires qui limitent l'explosion ou l'évanescence des gradients. \citet{glorot_understanding_2010} propose ainsi une méthode d'initialisation permettant d'obtenir des activations initiales normalement distribuées, ce qui facilite l'apprentissage en garantissant un flot raisonnable des gradients. Cette approche a été reprise et adaptée par la suite pour les fonctions d'activation de type \gls{ReLU}~\cite{he_delving_2015}. Dans la même veine, \citet{saxe_exact_2013} ont proposé d'initialiser les noyaux de convolution profonds avec des matrices aléatoires orthogonales afin d'une part de conserver la norme des activations d'une couche à l'autre et d'autre part de décorréler entre eux les filtres initiaux.

Compte-tenu de la nature stochastique de l'optimisation des réseaux de neurones, la communauté a consolidé un certain nombre de bonnes pratiques~\cite{lecun_efficient_1998,bengio_practical_2012,bottou_stochastic_2012}. Comme pour l'apprentissage automatique des modèles non profonds, il est recommandé de normaliser les données d'entrée. Généralement, les images sont ainsi normalisées en soustrayant la valeur du pixel moyen calculé sur l'ensemble du jeu de données. Dans certains cas, notamment pour des images présentant une structure très similaire, c'est l'image moyenne qui est soustraite. Il est toutefois rare d'appliquer une normalisation sur la variance.

Lors de l'apprentissage, il est recommandé de mélanger les données après chaque passe sur le jeu de données d'apprentissage, afin d'éviter des cycles dans la descente de gradient~\cite{lecun_efficient_1998}. En outre, la taille des mini-lots influe sur la descente de gradient. Plus les mini-lots sont grands, plus la descente est stable. Toutefois, une taille de mini-lots faible introduit une stochasticité plus forte dans la descente de gradient et introduit un bruit qui pourrait être bénéfique pour la généralisation du modèle. Enfin, il est recommandé d'accélérer la descente de gradient en démarrant avec un taux d'apprentissage élevé, et de le réduire par la suite~\cite{bengio_practical_2012}. Les hyperparamètres de la descente de gradient sont souvent délicats à régler de façon optimale, mais il est possible de les valider sur un sous-ensemble du jeu de données restreint pour les généraliser par la suite~\cite{bottou_stochastic_2012}. L'arrêt de l'apprentissage se fait généralement quand l'erreur de validation a cessé de décroître, ou à défaut lorsque l'erreur d'apprentissage ne diminue plus, au risque d'un surapprentissage.

Les bibliothèques récentes d'apprentissage profond implémentent pour la plupart ce type de bonnes pratiques, ainsi que les régularisations, initialisations, politiques d'évolution du taux d'apprentissage et variantes de la descente de gradient. Ceci simplifie grandement le travail d'expérimentation et diminue la part d'incertitude due à des pratiques divergentes au sein de la communauté. Toutefois, le réglage des hyperparamètres conserve une influence considérable dans les performances des différentes modèles. L'absence d'études statistiques de robustesse, comme la répétition des entraînements et le moyennage des résultats, conduit parfois à conclure erronément sur les performances comparatives de différentes modèles~\cite{oliver_realistic_2018} qui deviennent dues à des aléas de l'optimisation plutôt qu'aux changements d'architectures des réseaux.

Il est important de noter que la descente de gradient minimise l'erreur sur le jeu d'entraînement, tandis que l'erreur qui nous intéresse réellement correspond à celle du modèle généralisé aux données réelles. Autrement dit, l'apprentissage minimise un risque empirique qui ne correspond pas nécessairement au risque réel. Toutefois, nous n'avons pas accès à cette erreur et devons donc nous contenter de celle disponible. Cela conduit parfois à un phénomène de surapprentissage, c'est-à-dire que le modèle va apprendre et intégrer des connaissances biaisées liées aux échantillons du jeu d'entraînement, mais dont les propriétés ne se généralisent pas aux données réelles. Par exemple, un modèle devant discriminer entre des images de chats et des images de chiens entraîné sur des photos de chats prises majoritairement de jour et des photos de chiens prises majoritairement de nuit risquerait d'apprendre des caractéristiques liées à l'illumination plutôt qu'à l'espèce de l'animal.
Pour limiter ce phénomène de surapprentissage, il est nécessaire de faire intervenir des techniques de \emph{régularisation}. Celles-ci permettent de contrebalancer le fait que la fonction objectif à optimiser n'est connue que sur un sous-ensemble restreint et nécessairement biaisé de l'ensemble des observations du monde. Une première méthode consiste à imposer une contrainte sur l'amplitude des poids des connexions du réseau. Cette méthode, appelée \emph{weight decay}, ou dégradation des pondérations, ajoute une pénalité au terme d'erreur global dépend de la norme des poids. Ainsi, la fonction de coût totale devient\,:
$$\mathcal{L}_{totale} = \mathcal{L}_{co\hat{u}t}(W, \Omega) + \lambda \sum_{w \in W} w^2~~.$$
~\citet{krogh_simple_1991} ont montré que cette simple pénalité permet de réduire l'erreur de généralisation du modèle.

Plus récemment, le Dropout~\cite{srivastava_dropout_2014} a été proposé comme méthode de régularisation pour lutter contre le surapprentissage. Les réseaux de neurones contenant de nombreux paramètres, l'idée est d'aléatoirement éteindre des neurones de certaines couches à chaque itération de la phase d'apprentissage. Toutes les connexions liées aux neurones éteints sont alors neutralisées avec une probabilité $p$ et seuls les poids du réseau réduit sont mis à jour lors de la descente de gradient pour cette itération. Lors de la phase d'inférence, toutes les activations liées à ces neurones sont pondérées par $p$ afin de conserver les amplitudes totales constantes. Les n\oe{}uds du réseau ne voient ainsi qu'une partie du jeu de données, ce qui force les représentations à contenir de la redondance pour conserver leur pouvoir discriminant. Les signaux faibles liés au biais intrinsèque du jeu de données ne peuvent alors être modélisés, palliant ainsi les problèmes de surapprentissage. Une autre façon d'envisager le Dropout est de considérer qu'il s'agit d'une méthode apte à générer un grand nombre de réseaux réduits entraînés en parallèle. En effet, si à chaque itération les neurones sont supprimés avec une probabilité $p = 0,5$, alors en pratique cela est équivalent à entraîner aléatoirement un réseau parmi les $2^n$ réseaux réduits possibles, $n$ étant le nombre de paramètres sujets au Dropout. Lors de l'inférence, un seul réseau est toutefois utilisé, qui correspond à une moyenne de ces réseaux réduits. Il s'agit ainsi d'une forme de régularisation par apprentissage par ensemble. Des variantes inspirées du Dropout ont été proposées, comme le \emph{DropConnect}~\cite{wan_regularization_2013}, supprimant des synapses plutôt que des neurones, ou l'échantillonnage stochastique~\cite{zeiler_stochastic_2013}.

Une autre méthode pour lutter contre le surapprentissage consiste à alimenter le jeu de données d'entraînement en exemples synthétiques. En augmentant artificiellement le nombre d'échantillons d'apprentissage, il est possible d'espérer augmenter de fait la variété des exemples montrés au modèle et donc de limiter l'influence des biais intrinsèques du jeu de données. On parle alors d'\emph{augmentation de données}. Dans le cas des images, cela peut ainsi passer par des transformations géométriques qui n'affectent pas leur sémantique, notamment des symétries gauche-droite, des rotations et translations légères ou encore des redimensionnements.

Enfin, la normalisation par lot~\cite{ioffe_batch_2015} est parfois présentée comme une méthode de régularisation, en ce que les moments statistiques qui y sont estimés le sont de façon stochastique, ce qui ajoute un faible bruit aux représentations internes à chaque couche.

\subsection{Réseaux de neurones convolutifs profonds}

L'idée de partager des poids pour réaliser de façon dense la même opération locale sur toute l'image remonte au \emph{Neocognitron}~\cite{fukushima_neocognitron_1980}. L'idée d'utiliser des convolutions remonte à LeCun~\cite{lecun_gradient-based_1998}. Le produit de convolution entre deux fonctions $f$ et $g$ est habituellement noté $f * g$ s'obtient par la formule suivante\,:
$$(f * g)(x) = \int_{-\infty}^{+\infty} f(t)g(x-t) \diff t$$

Ce produit est commutatif et bilinéaire.

L'intérêt principal du produit de convolution est son lien fondamental avec la transformée de Fourier~\cite{fourier_propagation_1822}. En effet, en notant $\fourier$ la transformation de Fourier, alors\,:
$$\fourier(f \ast g) = \fourier (f)\fourier (g)~.$$

Les convolutions avaient déjà été utilisées de nombreuses fois, en particulier compte-tenu des résultats de Fourier en analyse harmonique~\cite{fourier_propagation_1822}. En partiuclier, la convolution discrète permet de réaliser des calculs de gradients, utilisés dans les \gls{SIFT}~\cite{lowe_object_1999} et \gls{HOG}~\cite{dalal_histograms_2005}. Les filtres convolutifs sont également à la base de la théorie des ondelettes~\cite{mallat_exploration_2001}, utilisés notamment dans le format compressé \glssymbol{JPEG}~\cite{daubechies_ten_1992} et dans les caractéristiques de pseudo-Haar~\cite{viola_robust_2001}, dérivées des ondelettes homonyme~\cite{papageorgiou_general_1998}.
Les filtres de Gabor en particulier ont trouvé de nombreuses applications comme caractéristique d'image~\cite{pati_word_2008}. Incidemment, plusieurs travaux de neurosciences ont mis en évidence la proximité entre le modèle de filtre de Gabor et les réponses des cellules du cortex visuel chez les mammifères, notamment le chat~\cite{marcelja_mathematical_1980,jones_evaluation_1987}. La~\cref{fig:convolution_exemples} illustre le résultat de divers filtrages, soit par des noyaux de convolutions classiques comme le calcul des gradients discrets par filtre de Sobel~\cite{sobel_isotropic_2014} ou l'application d'un flou grâce à un noyau gaussien, soit des filtrages non-linéaires comme le débruitage par filtre médian~\cite{frieden_new_1976}.

L'idée de LeCun~\cite{lecun_gradient-based_1998} est de remplacer les premières couches d'un réseau de neurones par des couches convolutives. Les neurones seront donc regroupés localement et calculeront chacun une convolution sur une partie de l'image. Pour simplifier le modèle, les poids seront partagés, c'est-à-dire que tous les neurones calculeront la même convolution. Plusieurs convolutions pourront toutefois être calculées en parallèle. Les noyaux de convolution étant optimisés durant l'apprentissage, l'apprentissage par représentation va donc se faire naturellement dans le domaine image avec des opérateurs adaptés. Incidemment, il s'avère qu'en pratique, la première couche convolutive tend à approximer des filtres de Gabor~\cite{yosinski_how_2014}.

\subsubsection{Convolution}


\begin{figure}
  \captionsetup[subfigure]{justification=centering}
  \captionsetup[subfigure]{width=.9\linewidth}
  \foreach \picpath\piclegend\ker in {nobu/Image originale (identité)/\drawkernel{0}{0}{0}{0}{1}{0}{0}{0}{0},
                                      nobu_gaussian/Filtre gaussien (flou)/\drawkernel{0,25}{0,5}{0,25}{0,5}{1}{0,5}{0,25}{0,5}{0,25},
                                      nobu_sobel_h/Filtre de Sobel \cite{sobel_isotropic_2014} (contours horizontaux)/\drawkernel{-1}{0}{+1}{-2}{0}{+2}{-1}{0}{+1},
                                      nobu_sobel_v/Filtre de Sobel \cite{sobel_isotropic_2014} (contours verticaux)/\drawkernel{+1}{+2}{+1}{0}{0}{0}{-1}{-2}{-1}}{%
  \begin{subfigure}[t]{0.25\textwidth}
    \includegraphics[angle=180,width=\textwidth]{\picpath}\\
    \centering
    \resizebox{0.75\textwidth}{!}{\ker}
    \caption*{\piclegend}
  \end{subfigure}%
  }
  \caption{Exemples de filtrages par différents noyaux de convolution.}
  \label{fig:convolution_exemples}
\end{figure}


\begin{figure}
  \captionsetup[subfigure]{justification=centering}
  \foreach \picpath\piclegend in {no_padding_no_strides_00/Convolution,padding_strides_00/Convolution avec pas et padding,dilation_00/Convolution à trous,no_padding_no_strides_transposed_00/Déconvolution}{%
\begin{subfigure}[t]{0.25\textwidth}
  \includegraphics[width=\textwidth]{\picpath}
  \caption*{\piclegend}
\end{subfigure}%
}
\caption{Opérateur de convolution et variantes sur une image (figures extraites de~\cite{dumoulin_guide_2016}).}
\label{fig:convolution}
\end{figure}

Dans le cas discret, le produit de convolution se réécrit\,:
\begin{equation}
  (f * g)[n] = \sum_{k=-\infty}^{+\infty} f[n]g[k-n]
\end{equation}

Cette formulation s'intéresse toutefois aux signaux 1D, tandis que les images sont des signaux 2D par nature. La convolution discrète peut toutefois s'écrire sans problème pour des fonctions multivariées. En deux dimensions, si l'on note $I : \llbracket 1;w \rrbracket \times \llbracket 1;h \rrbracket \rightarrow \mathbb{R}$ la fonction modélisant une image $I$ de taille $w\times{}h$ et $K\,: \llbracket 1;k_w \rrbracket \times \llbracket 1;k_h \rrbracket \rightarrow \mathbb{R}$ le \emph{noyau} de convolution de dimension $p \times q$, alors on peut définir le filtre $\mathcal{K}$ tel que\,:
\begin{equation}
  \mathcal{K}(I)[m,n] = K * I [m,n] = \sum_{i=-p}^{+p} \sum_{i=-q}^{+q} I[m - i, n - j] \cdot K[i, j]~,
\end{equation}
avec $p = \frac{k_w-1}{2}$ et $q = \frac{k_h-1}{2}$. Ce calcul est illustré dans la~\cref{fig:convolution}.

Un des inconvénients de ce produit est que le noyau de convolution $K$ et l'image $I$ sont parcourus en sens inverse, les indices de l'un augmentant tandis que les indices de l'autre décroissent. En pratique, la plupart des bibliothèques implémentent l'opérateur de \emph{corrélation croisée}\,:
\begin{equation}
\mathcal{K}(I)[m,n] = K \star I [m, n] = \sum_{i=-p}^{+p} \sum_{i=-q}^{+q} I[m + i, n + j] \cdot K[i, j]~.
\end{equation}
Cette opérateur perd la commutativité mais est plus simple à programmer. Les paramètres de $K$ étant optimisables, il est équivalent en pratique d'utiliser une corrélation croisée ou une convolution, car leurs matrices sont identiques à symétrie près. Les autres opérations intervenant dans des \gls{CNN} n'étant pas commutatives, la perte de cette propriété n'a donc que peu d'importance.

La corrélation croisée et la convolution souffrent toutes deux d'une inconnue lorsque l'opérateur agit sur les bords de l'image, puis que les valeurs de $I$ sont alors indéfinies. En règle générale, on ne calcule pas ces valeurs et les lignes et colonnes pour lesquelles le produit de convolution est indéfini sont ignorées, ce qui réduit la taille effective de l'image. On parle alors de corrélation croisée \emph{valide}. Il est également possible de remplir les valeurs manquantes de $I$ par des zéros (\emph{zero-padding}) (cf.~\cref{fig:convolution_exemples}), pour un nombre de lignes et de colonnes égal à la moitié de la taille du noyau de convolution dans chaque direction. On parle alors de corrélation croisée \emph{identique}, car le résultat est de même dimension que l'image d'entrée. Enfin, il est possible de remplir les valeurs manquantes par autant de zéros que nécessaire pour que chaque élément de $I$ soit visité par chaque élément de $K$, auquel cas on parle de corrélation croisée \emph{complète}.

En pratique, une couche de convolution en dimension $n$ d'un réseau de neurones est paramétrée par\,:
\begin{itemize}
  \item Les dimensions $(k_1, \dots, k_n)$ des noyaux de convolution, généralement la même dans tous les dimensions,
  \item Le nombre $C$ de convolutions parallèles, qui définit le nombre de cartes d'activations en sortie de couche,
  \item Le pas $s$ de la convolution,
  \item Le \emph{padding} $p$.
\end{itemize}

Ainsi, une couche de convolution possède $k_1 \times \dots \times k_n \times C$ paramètres optimisables. Dans le cas le plus courant de la dimension 2, les noyaux de convolution sont généralement carrés, c'est-à-dire qu'une couche de convolution 2D contient $C k^2$ paramètres.

L'intérêt de la convolution dans les réseaux de neurones profondes est triple~\cite{goodfellow_deep_2016}\,:
\begin{itemize}
  \item Les interactions convolutives sont parcimonieuses, la taille des noyaux de convolution étant très faibles devant la taille des images,
  \item Les caractéristiques extraites par convolution sont équivariantes aux translations de l'image, c'est-à-dire qu'une translation de l'image d'entrée translate les cartes d'activation de la même façon,
  \item Les paramètres de la convolution sont partagés pour l'ensemble de l'image, ce qui permet de détecter les mêmes caractéristiques peu importe leur position dans l'image avec un très faible coût de stockage en mémoire des paramètres.
\end{itemize}
Comparé à une couche entièrement connectée, la couche convolutive n'est pas invariante à la permutation des pixels car elle possède un \emph{a priori} fort sur la structure spatiale des données. Cet \emph{a priori} est lié à la notion d'équivariance sémantique des images par rapport à diverses transformations. Néanmoins, il faut garder à l'esprit que cette connaissance structurelle n'est pas toujours respectée. Dans une série temporelle, l'apparition d'une anomalie peut avoir un sens différent en fonction du moment auquel elle se produit. À l'inverse, la convolution 1D part du principe que l'anomalie excitera les neurones de la même façon quelle que soit sa position dans le temps. Toutefois, cet \emph{a priori} fort s'applique bien sur les images, et tout particulièrement les images aériennes et satellitaires, qui présentent des régularités spécifiques qui seront détaillées plus tard. La structure même des \gls{CNN}, indépendamment de leurs poids, est particulièrement adaptée au traitement d'images, qu'ils permettent de décomposer dans un espace de représentation dotée d'une équivariance forte à diverses transformations~\cite{ulyanov_deep_2017}.

Conventionnellement, en 2D, on représente les cartes d'activation des neurones, ou cartes de caractéristiques, sous la forme de tenseurs de dimension 3 $(C, W, H)$ avec $C$ le nombre de canaux, également appelé nombre de plans de convolutions, $W$ la largeur et $H$ la hauteur des cartes.

Une couche convolutive va donc combiner les $n_{in}$ cartes d'activation d'entrée avec le $j$\ieme{} noyau de convolution $K_j$\,:
$$\forall j \in \llbracket 1;n_{out} \rrbracket,~~~o_j = b_j + \sum_{i=1}^{n_{in}} K(z_i)~,$$ c'est-à-dire\,:
$$\forall j \in \llbracket 1;n_{out} \rrbracket,~~~o_j(m, n) = b_j + \sum_{i=1}^{n_{in}} \sum_{p=-\frac{k-1}{2}}^{+\frac{k-1}{2}} \sum_{q=-\frac{k-1}{2}}^{+\frac{k-1}{2}} z_i(m - p, n - q) \cdot k_j(p, q)$$

Une convolution transforme donc un tenseur $(C_{in}, W_{in}, H_{in})$ en tenseur $(C_{out}, W_{out}, H_{out})$ avec les dimensions spatiales se calculant par\footnote{Les équations d'arithmétique des convolutions sont tirés de~\citet{dumoulin_guide_2016}.}\,:
$$\mathit{out} = \mathit{in} - \mathit{kernel} + 2\cdot \mathit{padding} + 1~.$$

\paragraph{Convolution à pas}

Une première variante du produit de convolution consiste à sous-échantillonner virtuellement la dimension des cartes d'activation produites d'un facteur $s$. Pour ce faire, il suffit de ne visiter les éléments de $I$ qu'avec un pas de $s$\,:
$$\mathcal{K}_s(I)(m,n) = K_s \star I = \sum_{i=-p}^{+p} \sum_{i=-q}^{+q} I[s \cdot m + i, s \cdot n + j] \cdot K[i, j]~.$$

Une convolution à pas transforme donc un tenseur $(C_{in}, W_{in}, H_{in})$ en tenseur $(C_{out}, W_{out}, H_{out})$ avec les dimensions spatiales se calculant par\,:
$$\mathit{out} = \left\lfloor \frac{\mathit{in} - \mathit{kernel} + 2 \cdot \mathit{padding}}{\mathit{stride}}\right\rfloor + 1~.$$

\paragraph{Convolution à trous}

La convolution à trous, ou convolution dilatée~\cite{yu_multi-scale_2015}\footnote{L'algorithme à trous~\cite{shensa_discrete_1992} applique un même filtre à plusieurs échelles en utilisant des convolutions dilatées. La différence entre les deux est subtile et ne sera pas discutée ici.}, consiste à réaliser une convolution en observant $I$ à une résolution plus faible que sa résolution réelle, en sautant certaines de ses valeurs. Le noyau de convolution est ainsi virtuellement dilaté d'un facteur $d$, les valeurs manquantes étant remplacés par des 0. En pratique, la convolution à trous se calcule par la formule\,:
$$\forall j \in \llbracket 1;n_{out} \rrbracket,~~~o_j(m, n) = b_j + \sum_{i=1}^{n_{in}} \sum_{p=-\frac{k-1}{2}}^{+\frac{k-1}{2}} \sum_{q=-\frac{k-1}{2}}^{+\frac{k-1}{2}} z_i(m - dp, n - dq) \cdot k_j(p, q)$$

À trous\,:
$$\mathit{out} = \left\lfloor \frac{\mathit{in} - \mathit{kernel} - (\mathit{kernel} -1)(\mathit{dilation} - 1) + 2 \cdot \mathit{padding}}{\mathit{stride}}\right\rfloor + 1~.$$


\paragraph{Convolution transposée}

La convolution transposée est une opération s'opposant à la convolution traditionnelle en ce qu'elle correspond à son gradient par rapport à ses entrées. Pour un noyau de convolution $k$ donné, la convolution transposée permet de reconstruire une image $I$ à partir des activations $Z$, dont les dimensions s'obtiennent par
$$\mathit{out} = (\mathit{in} - 1) \cdot \mathit{stride} + \mathit{kernel} - 2 \cdot \mathit{padding}~.$$

Plus prosaïquement, il est possible d'envisager la convolution transposée comme une convolution à pas fractionnel, c'est-à-dire une convolution de pas $s = \frac{1}{s'}$ avec $s' \in \mathbb{N}^*$.

Cette convolution est parfois appelée à tort \og déconvolution \fg dans la littérature, sans toutefois correspondre à l'opérateur mathématique de déconvolution, défini comme l'inverse de l'opérateur de convolution. La convolution transposée est particulièrement utile pour inverser les effets d'une couche convolutive, par exemple dans la phase de décodage d'un auto-encodeur convolutif~\cite{zhao_stacked_2015}.

\subsubsection{Échantillonnage}

\begin{figure}
  \resizebox{\textwidth}{!}{
  \input{Chapitre1/pooling.tikz}
  }
  \caption{Sous-échantillonnage et sur-échantillonnage par valeurs maximales en deux dimensions.}
  \label{fig:pooling}
\end{figure}

\paragraph{Sous-échantillonnage}

Afin de réduire la dimension des cartes d'activation dans le réseau, il est utile d'opérer à des sous-échantillonnages. Il s'agit généralement d'appliquer un filtre par fenêtre glissante non-recouvrante sur les données d'entrée. Ce filtre est en règle général l'opérateur $\max$ ou l'opérateur de moyenne sur une fenêtre de taille fixe. On parle alors de \emph{max pooling} ou d'\emph{average pooling}~\cite{zhou_stereo_1988}. Un exemple de sous-échantillonnage en 2D est illustré dans la~\cref{fig:pooling}.

Outre la réduction de dimension, l'intérêt de ces opérateurs est d'introduire une invariance aux déformations légères.

Par ailleurs, il est possible de réaliser un sous-échantillonnage adaptatif dans le cas du filtre moyen. Il est utilisé dans certains réseaux pour réduire brutalement la dimension des cartes d'activation lorsque l'on traite des images de taille arbitraire. Dans le cas contraire, les dimensions des couches entièrement connectées déterminent la taille des images d'entrée.
Max-pooling, average pooling, adaptative pooling

Les dimensions d'une carte d'activation en sortie d'un sous-échantillonnage sont\,:
$$\mathit{out} = \left\lfloor \frac{\mathit{in} - \mathit{kernel}}{\mathit{stride}}\right\rfloor + 1~.$$

Le sous-échantillonnage n'a pas de paramètre optimisable, il s'agit d'une fonction complètement déterminée.

\paragraph{Sur-échantillonnage}

Le sur-échantillonnage, ou \emph{unpooling}, est l'opération inverse du sous-échantillonnage et tente de reconstruire une entrée à partir de sa sortie. Le sous-échantillonnage étant une opération perdant de l'information, le sur-échantillonnage est généralement approximatif. Dans le cas du sur-échantillonnage par valeur moyenne, la même valeur sera répliquée plusieurs fois dans l'image à résolution augmentée. Dans le cas du sur-échantillonnage par valeur maximale, le maximum sera replacé à sa position initiale et les valeurs restantes complétées par des zéros, comme illustré dans la~\cref{fig:pooling}.
Les dimensions d'une carte d'activation en sortie d'un sur-échantillonnage sont\,:
$$\mathit{out} = (\mathit{in} - 1) \cdot \mathit{stride} + \mathit{kernel}~.$$

Comme pour le sous-échantillonnage dont il est la transpoée, le sur-échantillonnage ne comprend aucun paramètre optimisable.

\subsubsection{Normalisation}

Si la normalisation des données afin de faire travailler les modèles sur des distributions centrées et normalisées est une pratique ancienne, l'utilisation de couches de normalisation des activations au sein même des réseaux profonds est relativement récente. Le principe général de la normalisation est d'appliquer une transformation aux cartes d'activation afin de leur imposer des propriétés statistiques particulières pour améliorer les capacités d'apprentissage du modèle.

\citet{jarrett_what_2009} propose ainsi une normalisation locale du contraste des cartes d'activation après les couches convolutives en s'inspirant des modèles biologiques~\cite{pinto_why_2008}. Pour un tenseur de $N$ cartes d'activations $a_1,\dots,a_N$, les cartes normalisées $z_i$ s'obtiennent par d'abord par soustraction d'une valeur moyenne locale sur une fenêtre spatiale gaussienne\,:
$$b_i[x,y] = a_i[x,y] - \sum_{p,q} w_{p,q} \cdot a_i[x+p,j+q]$$ avec $w_pq$ une fenêtre gaussienne telle que $\sum_{p,q} w_{p,q} = 1$
puis par normalisation des amplitudes par l'écart-type pondéré des caractéristiques sur une fenêtre spatiale\,:
$$z_i[x,y] = b_i[x,y] / \max(\operatorname{moy}(\sigma[x,y]), \sigma[x,y])$$ avec $\sigma[x,y] = \left(\sum_{p,q} w_{p,q} \cdot b_i^2[x+p,y+q] \right)^{\frac{1}{2}}$.

L'article séminal de~\citet{krizhevsky_imagenet_2012} introduit également une couche de normalisation locale, \gls{LRN}, qui vient inhiber les activations des cartes d'activations adjacentes à celle d'un neurone fortement excité. Ce procédé inspiré de l'inhibition latérale existant dans les neurones biologiques permet d'améliorer la généralisation du modèle. Contrairement à la normalisation du contraste de \citet{jarrett_what_2009}, cette normalisation ne s'applique pas sur un voisinage spatial, mais sur les cartes d'activations filtrées voisines de celle considérée, dans la troisième direction du tenseur. Cette normalisation peut s'exprimer sous la forme d'un noyau s'appliquant aux cartes d'activation 2D. Pour un tenseur de $N$ cartes d'activations $a^1,\dots,a^N$, les cartes normalisées $z^i$ s'obtiennent par\,:
$$z^i[x,y] = \frac{a^i[x,y]}{\left(k + \alpha \sum_{j=max(0, i-n/2)}^{min(N-1, i+n/2)} (a^j[x,y])^2  \right)^\beta}~~.$$
avec $n$ la taille du voisinage à considérer (dans la direction des canaux). Il est possible d'envisager cette normalisation en considérant pour chaque position spatiale des cartes d'activation qu'il s'agit d'une normalisation de l'intensité du vecteur d'activation $(a[x,y]^1, a[x,y]^2, \dots, a[x,y]^N)$.

Toutefois, la normalisation des activations la plus courante est la normalisation par lot, ou \emph{Batch Normalization}~\cite{ioffe_batch_2015}. Ce procédé consiste à normaliser les moments statistiques des cartes d'activation plan par plan afin de les centrer et de leur conférer une variance unitaire. Il présuppose un entraînement en utilisant l'algorithme de descente de gradient stochastique par mini-lots. Pour un mini-lot de taille $N$, le réseau opère à chaque itération sur un tenseur $(N, C, W, H)$. Si l'on considère les activations pour une couche donnée, alors il est possible de considérer pour chaque élément $n$ du lot les cartes d'activation $a^{(n)}_i, i \in \llbracket 1,C \rrbracket$. L'algorithme opère de la façon suivante\,:
\begin{definition}
Algorithme de normalisation par mini-lot\,:

Durant la phase d'apprentissage, la moyenne $\mu$ et la variance $\sigma$ au sein d'un mini-lot sont calculés à la volée\,:
$$\mu_i = \frac{1}{N} \sum_{n=1}^N a_i^{(n)}~~\text{ et }~~\sigma_i = \frac{1}{N} \sum_{n=1}^N (a_i^{(n)} - \mu_i)$$

Les valeurs moyennées de $\mu$ et$\sigma$ sur l'ensemble du jeu de données sont conservées en mémoire et ré-utilisées en phase d'inférence afin de rendre les calculs indépendants de la taille du mini-lot.

Dans tous les cas, les activations normalisées $\hat{a}$ sont obtenues par\,:
$$\hat{a}_i = \frac{a_i - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}}$$.
\end{definition}

La normalisation par lot est généralement suivie d'une transformée affine $z_i = \alpha \hat{a}_i + \beta$.\footnote{Dans ce cas, la \emph{Batch Normalization} contient $2C$ paramètres optimisables.}

Cette normalisation possède une propriété cruciale pour l'optimisation de réseaux très profonds\,: elle garantit le flux du gradient couche après couche en maintenant les amplitudes des activations constantes. Les activations ne peuvent être ni trop élevées, ni trop faibles, ce qui assure la stabilité numérique de l'ensemble du réseau. En outre, la normalisation des représentations permet de limiter l'impact de perturbations non sémantiquement pertinentes dans le calcul des caractéristiques. En pratique, l'utilisation de la \emph{batch normalization} permet d'améliorer les performances de classification des modèles et accélère significativement leur vitesse de convergence en lissant la surface de la fonction de coût~\cite{santurkar_how_2018}. Cette normalisation se retrouve dans la grande majorité des architectures de réseaux profonds conçues après sa publication.

La fonction de transfert non-linéaire des neurones s'applique en sortie de convolution avant ou après la normalisation selon les modèles.

\subsubsection{Couche entièrement connectée}

Une couche entièrement connectée correspond à un graphe biparti complet, au sein duquel tous les neurones d'entrée sont connectés par des synapses à tous les neurones de sortie. Cela correspond de fait au perceptron de~\citet{rosenblatt_perceptron_1957}. La non-linéarité est appliquée sous la forme d'une fonction d'activation sur le vecteur de sortie. Une couche entièrement connectée peut se conceptualiser comme une simple multiplication matricielle, transformant un vecteur de dimensions $1\times{}N$ en vecteur de dimensions $1\times{}M$ par le biais d'une matrice de poids $N\times{}M$. Lorsqu'il est utilisé, le \emph{Dropout}~\cite{srivastava_dropout_2014} est généralement appliqué sur les poids des couches entièrement connectées. En effet, ces couches contiennent un nombre important de paramètres et sont sensibles au surapprentissage. À l'inverse, les couches convolutives sont rarement soumises au \emph{Dropout}, car la suppression stochastique de neurones risque d'avoir des effets négatifs sur la structure spatiale des cartes d'activation.

\section{Apprentissage profond pour la segmentation sémantique}

\subsection{De la classification à la segmentation}

\begin{figure}[t]
  \resizebox{\textwidth}{!}{\input{Chapitre1/classification_segmentation.tikz}}
  \caption{Exemple de classification et de segmentation sur une même image. La classification s'intéresse à la reconnaissance d'objet pour toute l'image, tandis que la segmentation opère sur chaque pixel. {\small Crédits image\,: \href{https://pixabay.com/en/balloon-hot-air-balloon-ride-mission-2331488/}{PIRO4D (CC0)}}.}
  \label{fig:classif_vs_seg}
\end{figure}

La segmentation d'image est une des premières tâches envisagée dans le cadre de la vision artificielle.
La légende veut que Minsky en 1964 ait demandé à un ses étudiants, Gerald Sussman, de \og passer l'été à connecter une caméra à un ordinateur et réussir à faire en sorte que l'ordinateur décrive ce qu'il voit \fg\footnote{``\emph{spend the summer linking a camera to a computer and getting the computer to describe what it saw}'', tel que rapporté par ~\citet{szeliski_computer_2011}, citant lui-même~\citet{boden_mind_2008} reprenant~\citet{crevier_ai_1993}.}.
La tâche envisagée par Minsky et Papert pour le \emph{Summer Vision Project} consistait notamment à \og construire un système de programmes divisant une image issue d'un tube dissecteur\footnote{En référence au \emph{vidisector}, le dissecteur d'images inventé par Philo Farnsworth en 1927 sur le principe du tube cathodique. L'appareil reçoit de la lumière, ce qui stimule une photocathode et émet des électrons, produisant un signal électrique permettant de représenter l'image. C'est l'inverse de l'écran de télévision.} en différentes régions telles que\,: plutôt des objets, plutôt l'arrière-plan, ou du chaos\fg{} avec pour objectif final \og l'identification d'objets, qui nommera chaque objet en les faisant correspondre avec un vocabulaire d'objets connus.\fg{}~\cite{papert_summer_1966}
Dès le départ, la reconnaissance de formes s'intéresse donc au découpage sémantique des images afin de comprendre les scènes visuelles qu'elles représentent.
Si cette tâche semble triviale pour un humain, elle est représente pourtant un défi considérable pour la machine et l'équipe de Minsky se heurte au paradoxe de Moravec~\cite{moravec_mind_1988}\,: \og il est relativement aisé de mettre des ordinateurs au niveau d'un humain adulte dans le cadre d'un test d'intelligence ou d'une partie de dames, mais difficile voire impossible de leur donner les capacités de perception et la mobilité d'un bébé\fg{}\footnote{\emph{``it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility''}}.

De nombreux travaux se sont dès lors penchés sur le problème de la reconnaissance d'objet, c'est-à-dire l'identification d'un objet présent dans une image. On parlera ici de classification d'images, tâche consistant à associer une classe d'objet à une image. Cette tâche a concentré la majorité des efforts de la communauté, de l'utilisation de descripteurs \emph{ad hoc}~\cite{ullman_aligning_1989} aux caractéristiques apprises~\cite{vidal-naquet_object_2003} en passant par les modèles probabilistes~\cite{schneiderman_probabilistic_1998}. L'arrivée récente de larges bases de données d'images annotées comme CIFAR-10 et CIFAR-100~\cite{krizhevsky_learning_2009}, puis ImageNet~\cite{deng_imagenet_2009,russakovsky_imagenet_2015} ont notamment permis d'aboutir au succès des réseaux convolutifs profonds en classification de chiffres sur la base de données MNIST~\cite{lecun_gradient-based_1998}, en reconnaissance de panneaux de signalisation~\cite{stallkamp_german_2011}, en identification de caractères chinois~\cite{liu_icdar_2011} et bien sûr en reconnaissance d'objets en tout genre sur ImageNet~\cite{krizhevsky_imagenet_2012}.

\begin{figure}[t]
  \resizebox{\textwidth}{!}{
    \input{Chapitre1/lenet.tikz}
  }
  \caption{Architecture LeNet-5~\cite{lecun_gradient-based_1998}}
  \label{fig:lenet}
\end{figure}

L'architecture LeNet-5, developpée par \citet{lecun_gradient-based_1998}, a posé les bases des \gls{CNN}. Elle consiste en deux couches convolutives suivies de trois couches entièrement connectées, comme illustré dans la~\cref{fig:lenet}. La partie convolutive du modèle réalise l'extraction de caractéristiques dans le domaine image, tandis que les couches entièrement connectées opèrent sur une représentation vectorielle unidimensionnelle et réalisent la classification finale. Initialement, LeNet-5 a été développé pour la reconnaissance de chiffres manuscrits dans des images en niveaux de gris de dimensions $32\times32$. En comparaison de la taille de l'image, les filtres convolutifs utilisés sont relativement grands puisque les noyaux de convolution sont de taille $5\times5$. La première couche $C1$ comporte 6 noyaux, c'est-à-dire que six cartes d'activation sont générées. Elles sont sous-échantillonnées avec un pas de 2, puis transmises à la couche convolutive suivante $C2$. Chaque carte d'activation de $C1$ est filtrée par chaque noyau de convolution de la couche $C2$, qui en comporte 16. Les cartes sont à nouveau sous-échantillonnée, ce qui produit finalement 16 cartes d'activation $5\times5$. Celles-ci sont alors aplaties en un vecteur de taille $1\times400$, entièrement connecté à une première couche cachée de dimension $120$, puis à une seconde de dimension $84$. Finalement, ce descripteur est entièrement connecté au vecteur de taille $10$, chaque activation correspondant alors à un des 10 chiffres possibles. La présence des couches entièrement connectées, dont le nombre de neurones est fixé, impose aux cartes d'activation issues des couches convolutives d'être de la bonne dimension. Cette contrainte impose en cascade que la taille des images traitées par LeNet soit toujours la même. Utiliser des images d'autres dimensions nécessiterait de ré-entraîner des couches entièrement connectées avec le nombre adéquat de neurones. Ce défaut du à la présence des couches entièrement connectées est partagé par la plupart des \gls{CNN}.

\begin{figure}[t]
  \resizebox{\textwidth}{!}{
    \input{Chapitre1/alexnet.tikz}
  }
  \caption{Architecture AlexNet~\cite{krizhevsky_imagenet_2012}}
  \label{fig:alexnet}
\end{figure}

Pour le traitement des images en couleur, le réseau AlexNet~\cite{krizhevsky_imagenet_2012} utilise une approche similaire. Ce modèle, détaillé dans la~\cref{fig:alexnet} comporte 8 couches, dont 5 convolutives, et traite des images \gls{RVB} de dimensions $224\times224$. La première convolution possède un noyau relativement grand, de dimensions $11\times11$ suivi d'un sous-échantillonnage, ce qui réduit fortement les dimensions de l'image. Ainsi, les cartes d'activation en sortie de la première couche sont de dimensions $96\times27\times27$, puis $256\times13\times13$ après la seconde. Une normalisation \gls{LRN} est utilisée après les deux premières convolutions pour favoriser la généralisation du modèle. La structure convolutive d'AlexNet est conçue de telle sorte à ce que le nombre de cartes d'activation augmente de façon inversement proportionnelle à la réduction des dimensions spatiales. Cela permet de conserver un nombre raisonnable de valeurs à calculer tout en accroissant l'expressivité de la représentation apprise. Les trois couches de convolutions suivantes sont suivies par un sous-échantillonnage final qui permet d'obtenir 256 cartes de dimensions $6\times6$, transformée en un vecteur de caractéristique de longueur $9216$. Les couches entièrement connectées le réduisent ensuite à 2048 puis le projettent dans un vecteur de classification de taille 1000, correspondant aux classes d'ImageNet~\cite{deng_imagenet_2009}. Ce modèle a permis à~\citet{krizhevsky_imagenet_2012} de remporter la compétition \gls{ILSVRC}~\cite{russakovsky_imagenet_2015} en 2012 avec un taux d'erreur de 15,3\% en considérant les 5 prédictions les plus probables.

\citet{zeiler_visualizing_2014} se sont penchés sur l'architecture AlexNet afin d'établir un diagnostic de ses points forts et de ses points faibles. En particulier, ils proposent d'inverser les opérations de convolution afin de pouvoir relier les représentations internes aux pixels de l'image originale. Ils définissent ainsi les opérations transposées de la couche de convolution et du sous-échantillonnage et construisent un \emph{Deconvnet} permettant de visualiser les caractéristiques apprises par AlexNet. En outre, ils étudient attentivement les filtres convolutifs appris par les couches basses du réseau. Leurs travaux mettent en évidence plusieurs propriétés intéressantes. La première est que les caractéristiques des couches supérieures présentent une plus grande invariance aux transformations géométriques et colorimétriques de bas niveau, traduisant ainsi un meilleur niveau d'abstraction. La seconde est que les deux premières couches convolutives d'AlexNet contienennt des filtres liés aux hautes et basses fréquences, mais conservent peu d'information des fréquences intermédiaires. Ils proposent ainsi de remplacer la première couche, dont les noyaux sont de dimension $11\times11$, par des filtres $7\times7$ appliqués sur l'image avec un pas de 2 plutôt qu'un pas de 4, afin de conserver plus d'information. Leurs filtres appris de cette façon présentent une meilleure variété et moins de filtres ``morts'' à très faible amplitude. Enfin, la visualisation des représentations internes permet de constater que celles-ci ne sont pas aléatoires, mais possèdent bien une sémantique liée à la discrimination des objets, certains neurones se focalisant par exemple sur les visages, d'autres sur les roues, etc.

\begin{figure}[t]
  \resizebox{\textwidth}{!}{
    \input{Chapitre1/vgg.tikz}
  }
  \caption{Architecture VGG-16~\cite{simonyan_very_2014}}
  \label{fig:vgg}
\end{figure}

Le modèle VGG-16 affine l'architecture en proposant notamment de réduire la dimension des noyaux de convolution. L'argument principal évoqué par~\cite{chatfield_return_2014,simonyan_very_2014} est qu'il est plus simple d'optimiser plusieurs convolutions successives de noyaux $3\times3$ qu'une seule couche convolutive avec un noyau large $11\times11$. En outre, la présence de non-linéarités supplémentaires augmente l'expressivité du modèle. L'approche utilisée pour le modèle VGG-16 est donc de remplacer les convolutions larges classiques par des blocs de 2 ou 3 convolutions $3\times3$. Le modèle comporte 16 couches dont 13 convolutives et 3 entièrement connectées, suivant l'approche canonique de LeNet et AlexNet. Elle comporte 5 blocs convolutifs $3\times3$ chacun suivi d'un sous-échantillonnage de pas 2. Les deux premiers blocs comportent 2 couches de convolution, les trois suivants en comportant 3. Les cartes d'activations finales sont de dimension $512\times7\times7$, VGG-16 réalisant ainsi une réduction de dimension d'un facteur 32. Ce vecteur de longueur 25088 est ensuite réduit à 4096, puis aux 1000 classes d'intérêt d'ImageNet. Afin de limiter le surapprentissage, les couches entièrement connectées sont soumises au \emph{Dropout}. Ces améliorations proposés à l'architecture classiques des \gls{CNN} a permis d'obtenir en 2014 un taux d'erreur de 7,4\% en reconnaissance d'objet durant la compétition \gls{ILSVRC}.

\begin{figure}[t]
  \resizebox{\textwidth}{!}{
    \input{Chapitre1/googlenet.tikz}
  }
  \caption{Architecture GoogLeNet~\cite{szegedy_going_2015}}
  \label{fig:googlenet}
\end{figure}

\begin{figure}
  \begin{minipage}{0.48\textwidth}
    \resizebox{\textwidth}{!}{
      \input{Chapitre1/inception.tikz}
    }
    \caption{Module \emph{Inception}~\cite{szegedy_going_2015}.}
    \label{fig:inception}
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\textwidth}
    \resizebox{\textwidth}{!}{
      \input{Chapitre1/depthwise.tikz}
    }
    \caption{Convolutions séparables en profondeur~\cite{chollet_xception_2017}.}
    \label{fig:depthwise}
  \end{minipage}
\end{figure}

\begin{figure}[t]
  \begin{minipage}{0.5\textwidth}
    \resizebox{\textwidth}{!}{
      \input{Chapitre1/residual.tikz}
    }
    \caption{Bloc convolutif résiduel~\cite{he_deep_2016}}
    \label{fig:residual}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
  \resizebox{\textwidth}{!}{
    \input{Chapitre1/denseblock.tikz}
  }
  \caption{Bloc convolutif dense~\cite{huang_densely_2017}}
  \label{fig:denseblock}
  \end{minipage}
\end{figure}

En parallèle, \citet{szegedy_going_2015} proposent le modèle GoogLeNet avec 22 couches. Cette architecture introduit notamment le module \emph{Inception} empilant plusieurs couches en parallèle, et plus seulement en profondeur. L'idée est de réaliser, pour une carte d'activation, l'extraction de caractéristiques à plusieurs niveaux de contextes en utilisant soit une convolution $1\times1$, c'est-à-dire une combinaison linéaire suivie d'une non-linéarité, soit un \emph{pooling}, soit des convolutions $3\times3$ ou $5\times5$. Cela permet de coupler des caractéristiques dotées de l'invariance aux translations locales (issues du sous-échantillonnage) et des caractéristiques qui ne le sont pas, permettant donc de traiter une plus grande variété de cas. Le module \emph{Inception} est illustré dans la~\cref{fig:inception}.
L'architecture complète du modèle GoogLeNet est détaillée dans la~\cref{fig:googlenet}. Compte-tenu de la profondeur du réseau (22 couches), ses auteurs proposent de faciliter l'optimisation des couches les plus basses en ajoutant un classifieur au niveau des représentations intermédiaires, après les modules \emph{Inception} (4a) et (4d), cette approche profondément supervisée ayant déjà montré son efficacité pour lutter contre les problèmes de gradients évanescents par le passé~\cite{lee_deeply-supervised_2015}. Ce modèle obtient un taux d'erreur de seulement 6,4\% en reconnaissance d'objets à l'\gls{ILSVRC}.
Ce modèle sera amélioré~\cite{szegedy_rethinking_2016} par la suite en remplaçant les convolutions $5\times5$ du module \emph{Inception} par deux convolutions $3\times3$, comme proposé dans le modèle VGG~\cite{simonyan_very_2014} et en intégrant la \emph{Batch Normalization}~\cite{ioffe_batch_2015}.

\begin{figure}[t]
  \resizebox{\textwidth}{!}{
    \input{Chapitre1/resnet.tikz}
  }
  \caption{Architecture ResNet-34~\cite{he_deep_2016}}
  \label{fig:resnet}
\end{figure}

En 2015, \citet{he_deep_2016} parviennent à obtenir un taux d'erreur de seulement 3,5\% en reconnaissance d'objet durant le \gls{ILSVRC}. Leur approche consiste en un réseau très profond comprenant plus de 100 couches convolutives. L'optimisation est rendue possible d'une part grâce à la \emph{Batch Normalization}, mais surtout grâce à l'apprentissage par résidu. L'idée est de briser la structure purement séquentielle des réseaux à propagation avant en ajoutant des connexions permettant de court-circuiter la couche suivante. Ces connexions, dites résiduelles, correspondent à une simple opération identité et permettent alors aux activations et au gradient de parcourir l'ensemble du réseau sans subir d’évanescence ou d'explosion dues à la règle de la dérivation en chaîne. Plutôt de cherche à approximer $f\,: x \rightarrow f(x)$, le bloc résiduel va approximer $\hat{f}\,: x \rightarrow f(x) - x$, plus simple car d'amplitude a priori plus faible. Le bloc de convolution résiduel est illustré dans la~\cref{fig:residual} et un exemple de modèle dit \emph{ResNet} à 34 couches est détaillé dans la~\cref{fig:resnet}. L'introduction de l'apprentissage par résidu change en partie le paradigme utilisée jusqu'alors pour la conception des \gls{CNN}. Le bloc de base constitutif du réseau passe ainsi au bloc résiduel.
Les ResNet possèdent beaucoup de couches mais comparativement peu de paramètres, car seule la dernière couche est entièrement connectée. Comme nous l'avons vu, les couches entièrement connectées concentrent en général la majorité des poids des \gls{CNN} et sont également les plus sensibles au surapprentissage, nécessitant l'intégration de régularisations comme le \emph{Dropout}~\cite{srivastava_dropout_2014}. ResNet ne contient quasiment que des convolutions $3\times3$, à l'exception de la première $7\times7$ qui permet de réduire brutalement les dimensions spatiales de l'image. Il est intéressant de constater que la réduction de dimension finale avant la couche entièrement connectée se fait à l'aide d'un sous-échantillonnage adaptatif. Ainsi, peu importe la taille de l'image de l'entrée\,: le sous-échantillonnage moyennera les activations pour produire le vecteur de caractéristiques de taille attendue par la couche entièrement connectée. Cependant, il faut noter que le nombre élevé d'activations et de gradients intermédiaires à calculer rend les ResNet coûteux en mémoire et peu pratiques sur de grandes images. L'architecture \emph{Inception} sera également améliorée par des connexions résiduelles~\cite{szegedy_inception-v4_2017}.


\begin{figure}[t]
  \resizebox{\textwidth}{!}{
    \input{Chapitre1/densenet.tikz}
  }
  \caption{Architecture DenseNet-121~\cite{huang_densely_2017}}
  \label{fig:densenet}
\end{figure}


L'utilisation de cartes d'activation intermédiaires et leur propagation aux couches supérieures permet une meilleure classification en prenant en compte plusieurs niveaux d'abstraction. En outre, plusieurs travaux suggèrent que ces approches permettent en pratique de combiner plusieurs modèles en un seul, les activations étant en mesure de suivre plusieurs chemins dans la topologie du réseau~\cite{veit_residual_2016,huang_deep_2016}. Toutefois, les connexions résiduelles ne permettent que d'accéder aux activations de la couche précédente. \citet{huang_densely_2017} ont donc proposé une architecture dite \emph{DenseNet} comportant des connexions denses, construisant un modèle au sein duquel toutes les cartes d'activation issues des couches inférieures sont transmises à toutes les couches supérieures. Pour éviter l'explosion du nombre de paramètres et d'activations, le modèle est divisé en plusieurs blocs denses, comme illustré dans la~\cref{fig:densenet}. Chaque bloc se détaille comme illustré dans la~\cref{fig:denseblock}. La présence des connexions denses permet au gradient de se propager immédiatement des couches supérieures aux couches inférieures, appliquant ainsi une forme implicite de supervision profonde~\cite{lee_deeply-supervised_2015}. Entre deux blocs, une couche convolutive de transition set appliquée pour réduire le nombre de plans et est suivie d'un \emph{max-pooling} pour réduire les dimensions spatiales. Cette architecture obtient comparativement de meilleurs résultats que les modèles ResNet sur la base de validation de l'\gls{ILSVRC}2012. Toutefois, comme pour les ResNet, si le nombre de paramètres des architectures DenseNet est faible, ils sont coûteux en espace mémoire nécessaire pour stocker les activations et gradients intermédiaires.

Plus récemment, \cite{chollet_xception_2017} introduit les convolutions séparables en profondeur ou \emph{depthwise separable convolutions}. Celles-ci opèrent un filtre par plan du tenseur d'activations et sont recombinées pixel à pixel par une convolution point à point de noyau $1\times1$. Ainsi, il s'agit d'un cas spécifique de la convolution usuelle dans laquelle chaque plan du tenseur est filtré par un et un seul noyau de convolution, comme illustré dans la~\cref{fig:depthwise} Ces convolutions sont introduites pour remplacer le module \emph{Inception} dans l'architecture éponyme et en ont amélioré les performances sur les jeux de données ImageNet et JFT, interne à Google. Un avantage notable de ces convolutions est de nécessiter moins de paramètres que la convolution classique. En effet, une convolution $k_1 \times k_2$ opérant sur $N_{in}$ cartes d'activations produisant $N_{out}$ cartes d'activations nécessite $k_1 \times k_2 \times N_{in} \times N_{out}$ paramètres. La convolution séparable en profondeur nécessite $N_{in} \times N_{in} \times k_1 \times k_2$ paramètres pour la première phase puis $N_{in} \times N_{out}$ pour la seconde, soit un total de $N_{in} \times (N_{in} \cdot k_1 \cdot k_2 + N_{out})$, ce qui est avantageux quand $N_{out} \ge N_{in}$, ce qui est le cas le plus courant.

Toutefois, la classification d'images ne donne aucune information particulière de localisation, seulement une information binaire de présence d'un objet dans une image. Les premières approches se sont intéressés au calcul de caractéristiques denses sur l'image associées à des cascades multi-échelles de classifieurs pour détecter des objets dans chaque sous-région de l'image. C'est l'approche utilisée par les descripteurs \gls{SIFT}~\cite{lowe_object_1999}, les pseudo-Haar de la méthode de Viola et Jones~\cite{viola_robust_2001} ou encore des \gls{HOG}~\cite{dalal_histograms_2005}. Des approches à base de réseaux profonds pour la détection d'objet à partir de classification de sous-régions de l'image ont rapidement vu le jour~\cite{girshick_rich_2014,liu_ssd_2016,girshick_region-based_2016} supplantant les approches traditionnelles de localisation à partir d'extraction de fenêtres candidates~\cite{gu_recognition_2009,uijlings_selective_2013}. L'idée principale est de réaliser d'une extraction dense de caractéristiques afin d'identifier les régions de l'image susceptibles de contenir un objet d'intérêt. Cependant, ces approches ne répondent pas exactement au problème décrit par Papert en 1966. La question n'est pas uniquement de pouvoir identifier des objets, mais aussi d'en estimer la forme et de les séparer du fond, comme illustré dans la~\cref{fig:classif_vs_seg}. Cette tâche consiste donc à la segmentation sémantique, c'est-à-dire à l'association d'une classe d'intérêt non pas à chaque image, mais à chaque pixel.

Plusieurs jeux de données ont été introduits dans la communauté vision par ordinateur afin d'évaluer des méthodes sur cette tâche, notamment sur des scènes de la vie quotidienne, comme PASCAL VOC~\cite{everingham_pascal_2014}, Microsoft COCO~\cite{lin_microsoft_2014} et également sur des scènes de conduite autonome avec des jeux de données tels que CamVid~\cite{brostow_semantic_2009}, Cityscapes~\cite{cordts_cityscapes_2016} ou encore Mapillary Vistas~\cite{neuhold_mapillary_2017}. Les premières approches de segmentation sémantique ont réalisé des classification à partir de caractéristiques denses calculées sur l'ensemble de l'image, regroupées \emph{a posteriori} par régions homogènes~\cite{shotton_semantic_2008,shotton_real-time_2011}. Les réseaux convolutifs profonds ont également utilisés à cette fin en calculant une classification pour chaque pixel de l'image~\cite{grangier_deep_2009,ciresan_deep_2012} ou pour chaque région~\cite{farabet_towards_2013,sermanet_overfeat_2013}. Une observation importante est que les cartes de caractéristiques issues des couches convolutives conservent la structure spatiale de l'image, c'est-à-dire qu'il est possible de faire correspondre chaque caractéristique à un ou plusieurs pixels. Cette extraction de caractéristique dense se prête particulièrement aux problématiques de localisation d'objet et est rapidement adoptée par la communauté~\cite{zou_generic_2014}. Nous détaillerons un peu plus en détail certaines de ces approches dans le~\cref{chap:cartographie}. Dans la suite de cette partie, nous nous intéressons plus particulièrement aux réseaux profonds entièrement convolutifs, destinés à la classification dense pixel à pixel. En effet, ceux-ci sont une évolution naturelle des approches d'extraction dense de caractéristique, permettant un apprentissage de bout en bout pour la segmentation sémantique.

\subsection{Approches entièrement convolutives}

\begin{figure}
  \resizebox{\textwidth}{!}{%
  \input{Chapitre1/alexnet_fcn.tikz}
  }
  \caption{AlexNet entièrement convolutif proposé par~\citet{long_fully_2015}.}
\end{figure}


La forme moderne des réseaux convolutifs profonds pour la segmentation sémantique est popularisée par~\citet{long_fully_2015}. L'idée fondamentale est de ne manipuler que des réseaux entièrement convolutifs, c'est-à-dire sans couche entièrement connectées. Dès lors, les cartes d'activation conservent leurs propriétés spatiales et peuvent être relocalisées sur l'image originale par un simple jeu de sur-échantillonnage, par exemple par interpolation bilinéaire. L'approche choisie par~\citet{long_fully_2015} consiste à transformer la première couche entièrement connectée en convolution dont le noyau recouvre l'intégralité des cartes d'activation. En effet, ces deux opérations sont mathématiquement équivalentes, mais l'expression sous forme de convolution permet de ne plus se limiter à une taille précise d'images. La première couche entièrement connectée est donc remplacée par une convolution $7\times7$ et les suivantes par des convolutions de noyaux $1\times1$. En particulier, cette transformation permet de conserver les poids de l'ensemble du réseau déjà entraîné pour la classification d'images. De fait, ils proposent ainsi d'utiliser les poids de VGG-16 entraîné pour la classification d'objets sur ImageNet en convolutionalisant ses dernières couches afin de prédire des prédictions denses à résolution $1:32$. Ce modèle peut ensuite être utilisé pour initialiser un réseau réalisant une segmentation plus fine en produisant des cartes sémantiques à résolution $1:16$ puis $1:8$ à l'aide d'un décodeur augmentant la résolution des activations. Cette approche permet immédiatement d'établir de nouveaux état de l'art sur les jeux de données de segmentation sémantique comme Pascal VOC~\cite{everingham_pascal_2014} et dominent largement les compétitions depuis 2015.

Plusieurs axes d'amélioration ont été proposés dans la litérature concernant la segmentation sémantique d'images naturelles. Tout d'abord, en conservant la structure des couches convolutives de VGG-16~\cite{simonyan_very_2014},~\citet{l._c._chen_deeplab_2018} proposent d'une part d'utiliser les convolutions à trous pour agrandir le champ réceptif du réseau tout en retirant les couches de \emph{maxpooling}, qui réduisent la résolution spatiale, et d'autre part d'utiliser un \gls{CRF} \emph{a posteriori} pour régulariser les cartes prédites. De façon similaire,~\citet{yu_multi-scale_2015} proposent l'utilisation de la convolution à trous (nommée convolution dilatée) pour agréger des cartes d'activation à plusieurs échelles, combinant ainsi l'agrandissement du champ réceptif avec le principe de convolutions parallèles du module Inception~\cite{szegedy_going_2015}. Ces modèles réalisent ainsi une extraction dense de caractéristiques sur l'ensemble de l'image, produisant des cartes d'activation à résolution réduite d'un facteur 4 ou 8.

En parallèle, une famille de \gls{FCN} dérivée des auto-encodeurs convolutifs~\cite{zhao_stacked_2015} émerge. Tandis que les modèles de \citet{long_fully_2015} consistent en un encodeur profond calqué sur la topologie d'un réseau classifieur suivi d'un décodeur constitué de peu de couches de déconvolution, ces \gls{FCN} présentent une architecture symétrique. Il s'agit alors de projeter les cartes d'activation basse résolution issues de l'encodeur dans l'espace des classes à haute résolution, soit par le biais de la convolution transposée ~\cite{nekrasov_global_2016,noh_learning_2015}, soit par un sur-échantillonnage parcimonieux~\cite{badrinarayanan_segnet_2017}. L'architecture U-Net~\cite{ronneberger_u-net_2015} utilise ainsi des \emph{skip connections} pour réinjecter les cartes d'activation des couches d'encodage dans la phase de décodage et des convolutions transposées pour reconstituer la résolution de l'image. Ces approches utilisent comme encodeur les couches convolutives de \gls{CNN} pré-entraînés pour la classification, notamment VGG-16. L'intérêt de ces approches symétriques est de générer des prédictions à la même résolution spatiale que l'image d'entrée. En effet, l'encodeur produit des cartes d'activation sous-résolues, d'un facteur 8 pour VGG-16, qui vont être reconstruites à résolution plus élevée par les couches successives du décodeur.

Toutefois, compte-tenu des performances supérieures obtenues en reconnaissance d'objet par les modèles ResNet et DenseNet, plusieurs travaux se sont donc penchés sur leur adaptation au cadre de la segmentation sémantique. Un verrou majeur de ces approches réside dans leur important coût en mémoire, compte-tenu du nombre important d'activations intermédiaires à stocker dans des réseaux aussi profonds. L'augmentation des capacités de calcul des \gls{GPU} aidant, \citet{wu_high-performance_2016} proposent ainsi une première approche pour les ResNet, qui sera également utilisée pour le modèle DeepLab~\cite{l._c._chen_deeplab_2018}. Compte-tenu du coût en mémoire, ces modèles suivent l'approche initiale de~\cite{long_fully_2015} et génèrent ainsi des cartes de prédiction à un facteur d'échelle $1:4$. Récemment, une version entièrement convolutive des DenseNet~\cite{jegou_one_2017} a également été proposée pour la segmentation sémantique en combinant une approche encodeur-décodeur avec le passage d'activations inspiré de U-Net~\cite{ronneberger_u-net_2015}.

Enfin, hormis les travaux sur l'architecture de base des \gls{FCN}, plusieurs améliorations connexes ont été proposées pour raffiner la qualité des segmentations sémantiques obtenues à partir des différents modèles. La communauté s'est ainsi penchée sur l'utilisation des modèles graphiques structurés pour la régularisation des cartes sémantiques inférées par les \gls{FCN}. En particulier, les \gls{CRF} sont reformulés de manière à s'exprimer sous forme d'un réseau récurrent optimisable conjointement avec le \gls{FCN}~\cite{zheng_conditional_2015} ou comme post-traitement~\cite{arnab_higher_2015}.

Dans la veine des \emph{skip connections}, le modèle GridNet~\cite{fourure_residual_2017} s'intéresse à des topologies de réseaux non-conventionnelles en proposant une architecture constituée de plusieurs ResNet parallèle dont les activations peuvent évoluer aussi en profondeur que latéralement. C'est également l'approche de~\citet{liu_path_2018} dont le décodeur agrège les cartes d'activation en leur permettant de prendre plusieurs chemins parmi le graphe de calcul du réseau convolutif.

Finalement, plusieurs approches multi-échelles ont été proposées. \citet{l._c._chen_deeplab_2018} intègre dans leur modèle DeepLab des prédictions à plusieurs résolutions, interpolées et moyennées en fin de traitement. D'autres approches utilisent des noyaux de convolution de différentes tailles, soit en faisant varier un facteur de dilatation~\cite{yu_multi-scale_2015}, soit en déployant au sein du réseau des modules \emph{Inception}~\cite{szegedy_going_2015,nekrasov_global_2016,zhao_pyramid_2017}. \citet{peng_large_2017} ont proposé un module global de déconvolution observant l'intégralité de l'image afin de modéliser les relations spatiales à longue distance entre les éléments constitutifs d'une scène. Ils combinent cette technique à un apprentissage résiduel permettant de raffiner les bordures des objets. Dans l'ensemble, les techniques d'inférence multi-échelles pour la segmentation sémantique sont construites en majorité sur un système de convolutions parallèles générant une pyramide de cartes d'activation à différentes résolutions.

En résumé, la segmentation sémantique d'images multimédia est une tâche fréquemment étudiée dans la littérature. Les approches par \gls{FCN} ont permis d'établir de nouveaux états de l'art sur de nombreux jeux de données\,: Microsoft COCO~\cite{lin_microsoft_2014}, Pascal VOC~\cite{everingham_pascal_2014}, Cityscapes~\cite{cordts_cityscapes_2016} ou ADE20k~\cite{zhou_scene_2017}. Toutefois, ceux-ci se focalisent sur la compréhension de scènes de la vie quotidienne\,: images d'intérieurs ou de conduite urbaine contenant de nombreux objets observés par une multitude de points de vue avec occlusions, acquises par des appareils photos ou des caméras du commerce. La contribution centrale de cette thèse consiste ainsi à comprendre dans quelle mesure la cartographie automatisée peut bénéficier des connaissances et techniques mises en place sur ces données multimédia.

\begin{figure}[h]
  \captionsetup[subfigure]{width=0.95\linewidth,justification=centering}
  \begin{subfigure}[t]{0.33\textwidth}
    \includegraphics[width=\textwidth]{lidar_carolina}
    \caption{Image \glssymbol{Lidar} de la Caroline du Nord (États-Unis). Le codage couleur correspond à l'élévation topographique.\\
    \small \href{https://commons.wikimedia.org/wiki/File:Rex,_NC_LiDAR_DEM_of_Carolina_bays.jpg}{Crédits images : Cintos (domaine public, Wikimédia Commons)}
    }
  \end{subfigure}
  \begin{subfigure}[t]{0.33\textwidth}
    \includegraphics[width=\textwidth]{Sentinel2_Viti_Levu_Fiji}
    \caption{Composition \glssymbol{RVB} d'une image multispectrale Sentinel-2 sur l'île de Viti Levu (Fiji).\\
    \small \href{https://www.esa.int/spaceinimages/Images/2017/11/Viti_Levu_Fiji}{Crédits images : données Copernicus Sentinel, traitées par l'ESA (CC BY-SA 3.0 IGO)}
    }
  \end{subfigure}
  \begin{subfigure}[t]{0.33\textwidth}
    \includegraphics[width=\textwidth]{Sentinel1_Dotson_ice_shelf}
    \caption{Image \glssymbol{SAR} Sentinel-1 de la barrière de glace de Dotson (Antarctique).\\
    \small \href{https://www.esa.int/spaceinimages/Images/2017/10/Dotson_ice_shelf_from_Sentinel-1}{Crédits images : données Copernicus Sentinel, traitées par A. Hogg/CPOM}
    }
  \end{subfigure}
  \caption{L'observation de la Terre implique une grande variété de capteurs dotés de spécificités qui leur sont propres.}
  \label{fig:earth_observation}
\end{figure}

\section{Apprentissage pour le traitement d'images de télédétection}

L'interprétation automatique d'images de télédétection mobilise des fonctions cognitives similaires à celles utilisées pour la compréhension de scènes dans des données multimédia. En particulier, la vision artificielle est très largement utilisée pour la photo-interprétation. Toutefois, l'observation de la Terre fait appel à des capteurs et à des points de vue très spécifiques. Ainsi, la cartographie automatisée d'images aériennes et satellitaires ne peut simplement se réduire à une branche de la vision artificielle. Il s'agit de l'intersection entre celle-ci et la télédétection pour l'observation de la Terre, recouvrant aussi bien des aspects d'apprentissage automatique pour la vision que des spécificités liés aux capteurs aéroportés et satellites souvent très éloignés des appareils photos du commerce.

\subsection{Différents types d'imagerie}

Comme illustré par la~\cref{fig:earth_observation}, la télédétection et en particulier l'observation de la Terre mettent à profit une grande variété de capteurs. Contrairement au multimédia, ces capteurs peuvent être de nature très variable. Si les acquisitions aéroportées se font généralement en couleurs \gls{RVB} à l'aide d'appareils photos classiques, les acquisitions satellitaires utilisent bien souvent des capteurs dotés de capacités particulière, comme de transmettre une information spectrale riche, percer la couverture nuageuse ou mesurer des propriétés physiques de la surface de la Terre.

Le cas plus simple à envisager est celui de l'infrarouge. Il est courant, y compris dans les acquisitions d'images aéroportées, d'imager aussi bien le domaine visible que le proche infrarouge, situé entre \SI{780}{\nano\meter} et \SI{2 500}{\nano\meter}, notamment car la végétation y a une réponse amplifiée par la présence de chlorophylle. Dans l'infrarouge moyen et lointain, il est également possible de mesurer indirectement la température grâce aux radiations lumineuses émises grâce à la loi de Wien. Ces caméras thermiques sont particulièrement utiles dans l'espace, car cela permet de s'affranchir du bruit ambiant causé par la chaleur naturelle terrestre.

\begin{figure}
  \resizebox{\textwidth}{!}{
  \input{Chapitre1/multispectral.tikz}
  }
  \caption{Un capteur multispectral acquiert plusieurs bandes spectrales larges réparties sur le spectre lumineux infrarouge, visible et parfois ultraviolet.}
  \label{fig:multispectral}
\end{figure}

En étendant ce principe, une caméra multispectrale ou superspectrale va imager une scène non seulement dans les trois canaux habituels \gls{RVB} du domaine visible, mais dans plusieurs bandes de longueurs d'ondes plus ou moins larges pouvant se trouver aussi bien dans le domaine visible que dans l'infrarouge ou l'ultraviolet, comme illustré dans la~\cref{fig:multispectral}. Cela conduit à des images à plusieurs canaux, généralement aux alentours d'une dizaine, qui ne sont donc pas directement visualisables par l'\oe{}il humain. Il est toutefois possible de reconstituer une image en couleurs naturelles en recomposant une image \gls{RVB} à partir des valeurs contenues dans les canaux correspondant aux longueurs d'onde du rouge, du vert et du bleu. Les acquisitions multispectrales peuvent avoir des résolutions différentes en fonction des canaux. Les satellites Sentinel-2A et Sentinel-2B produisent par exemple des images à une résolution au sol de \SI{10}{\meter/\px} dans le domaine visible, mais certaines bandes, notamment dans l'infrarouge, ont des résolutions de \SI{20}{\meter/\px} ou \SI{60}{\meter/\px}. Les acquisitions couleurs satellitaires les mieux résolues spatialement ont une résolution au sol de l'ordre de \SI{50}{\centi\meter/\px}, tandis que les images aéroportées peuvent aller jusqu'à une résolution de \SI{5}{\centi\meter/\px}. Dans le cas des acquisitions satellitaires, il est courant de réaliser en simultané une mesure multispectrale et une acquisition panchromatique, c'est-à-dire qui ne distingue pas les couleurs et produit une image en noir et blanc, à résolution supérieure. Ainsi, \gls{Pleiades} réalise à la fois une acquisition panchromatique à résolution \SI{70}{\centi\meter/\px}, ré-échantillonnée à \SI{50}{\centi\meter/\px} et multispectrale à \SI{2,8}{\meter} rééchantilonnée à \SI{2}{\meter}.

\begin{figure}
  \resizebox{\textwidth}{!}{
  \input{Chapitre1/hyperspectral.tikz}
  }
  \caption{Un capteur hyperspectral acquiert de nombreuses bandes spectrales étroites régulièrement réparties sur sa plage d'acquisition.}
  \label{fig:hyperspectral}
\end{figure}

L'imagerie hyperspectrale consiste à effectuer des acquisitions sur de nombreuses bandes étroites, toutes de même taille, afin de balayer de façon discrète l'ensemble du spectre lumineux réfléchi, comme illustré par la~\cref{fig:hyperspectral}. En fonction de la résolution spectrale -- souvent de l'ordre de \SI{10}{\nano\meter} -- et de la largeur du spectre considéré, le nombre de bandes peut varier de quelques dizaines à plusieurs centaines. L'avantage principale de ce type de caméra est qu'il est possible pour un pixel donné de reconstituer la courbe de l'intensité lumineuse réfléchie en fonction de la longueur d'onde. Chaque matériau réfléchit différemment la lumière en fonction de son albédo et cette information permet donc de caractériser finement la composition des objets d'intérêt observés. Toutefois, les caméras hyperspectrales ont également une résolution spatiale nettement plus faibles que les caméras \gls{RVB} ou multispectrales classiques, produisant des images à une résolution au sol d'environ \SI{1}{\meter/\px} en aérien.

Il est important de noter que ces capteurs optiques sont passifs. Ils ne reçoivent que l'énergie lumineuse réfléchie ou émise par le corps observé, ce qui restreint ce qu'il est possible de réaliser. Notamment, ces capteurs sont sensibles aux variations d'illumination et aux effets météorologiques, en particulier la couche nuageuse qui les rendre entièrement inopérationnels. Ainsi, de nombreux satellites sont eux munis de capteurs actifs qui émettent un signal et mesurent le retour de celui-ci. C'est cette approche qui est utilisée pour les satellites radar, et notamment \gls{SAR}, qui envoient une ou plusieurs ondes électromagnétiques et utilisent la mesure de l'onde réfléchie pour extraire des paramètres physiques de la zone observée.

\begin{figure}
  % \resizebox{\textwidth}{!}{
  % \input{Chapitre1/mnt.pdf_tex}
  % }
  \includegraphics[width=\textwidth]{mnt.pdf}
  \caption{Schéma représentatif de la différence entre \glsname{MNT} et \glsname{MNE}.}
  \label{fig:mnt}
\end{figure}

En outre, le \gls{Lidar} est un capteur actif émettant une impulsion laser dont il mesure l'écho. La position du maximum d'amplitude permet de déterminer le temps d'aller-retour du signal lumineux et donc de mesurer la distance parcourue par les photons. Ces capteurs sont très utilisés aussi bien en télédétection qu'en robotique pour effectuer des relevés topographiques et la reconstruction 3D. L'inconvénient principal du \gls{Lidar} est que la mesure laser est ponctuelle et qu'il ne permet donc que de construire des nuages de points peu denses. Dans le cas du \gls{Lidar} satellitaire, les points de mesure sont généralement espacés d'environ \SI{20}{\meter}, et \SI{10}{\centi\meter} dans le cas de l'aéroporté. Une fois le nuage de points construit, il est possible d'en extraire un maillage qui modélise la topographie de la surface observée. Il est ensuite possible de rastériser ce maillage pour obtenir un \glsfirst{MNT} ou un \glsfirst{MNE} en fonction de la résolution. Le \gls{MNE} se distingue du \gls{MNT} en ce qu'il prend en compte les objets surélevés par-dessus la surface topographique, comme illustré par la~\cref{fig:mnt}. La différence entre ces deux modèles est appelée \gls{MNH} et correspond à la hauteur normalisée des points au-dessus du sol.

Nos travaux portent principalement sur l'utilisation de données optiques pour la cartographie automatisée. Nous nous autoriserons néanmoins à faire appel à des données ancillaires, qu'elles soient dérivées d'acquisitions \gls{Lidar} ou provenant de \glspl{SIG}.

\subsection{Apprentissage et images de télédétection}

Comme nous venons de le voir, les images de télédétection peuvent se présenter sous de nombreuses formes\,: multispectrales, hyperspectrales, \gls{SAR}, \gls{Lidar}\dots De nombreuses techniques d'apprentissage automatique ont été mises en \oe{}uvre pour extraire de l'information de ces données sans intervention humaine.

\subsubsection{Extraction de caractéristiques}

Une fois les données acquises et mises en forme, il est nécessaire de choisir une méthode de représentation adaptée à la classification. En particulier, il s'agit de chercher un espace de représentation dans lequel projeter les données, de manière à ce qu'il soit aisé de partitionner l'espace afin d'y séparer les différentes classes. Il existe plusieurs approches envisageables.

La première approche consiste à simplement utiliser les données brutes, éventuellement normalisées. Le classifieur agit directement sur les valeurs de luminance ou de réflectance des pixels. Cependant, une image \gls{RVB} de dimensions $128\times128$ est alors décrite par $128\times128\times3 = 49152$ scalaires, ce qui est intractable pour la plupart des modèles statistiques usuels. Cela impose alors de traiter les pixels individuellement ou de découper l'image en régions de petite taille. De telles approches sont courantes pour le traitement de données hyperspectrales~\cite{fauvel_advances_2013,ham_investigation_2005} et multispectrales, y compris \gls{IRRVB}~\cite{dechesne_semantic_2017}.

En effet, il est possible de combiner ces données initiales à des caractéristiques expertes, comme les moments statistiques ou le gradient du signal. Dans le cas des images \gls{Lidar}, l'écart local à la hauteur moyenne permet ainsi de discriminer différents types d'objets~\cite{guo_relevance_2011,li_lidar_2017}, et l'entropie locale est une caractéristique souvent utilisée en imagerie \gls{SAR}~\cite{g._barber_sar_1991}.

En outre, les capteurs multispectraux et \gls{SAR} présentent des propriétés physiques dont la connaissance \emph{a priori} est exploitable. Des indices mesurant des ratio de réflectance dans différentes longueurs d'onde peuvent permettre de caractériser certaines surfaces, comme le \gls{NDVI}~\cite{rouse_monitoring_1974} pour la végétation et le \gls{NDWI}~\cite{xie_new_2014} pour l'eau. Ces indices sont facilement interprétables mais savoir lesquels utiliser demande une connaissance experte des phénomènes étudiés (il est impossible de détecter un phénomène que l'on ne saurait pas caractériser physiquement, au moins partiellement) et un effort systématique d'ingénierie, les indices à utiliser dépendants de chaque problème.

Comme pour le traitement d'images multimédia, il est ainsi intéressant de rechercher des caractéristiques génériques. Les histogrammes de couleurs peuvent par exemple se généraliser à des images multispectrales ou hyperspectrales de la même façon qu'ils s'appliquent aux images \gls{RVB}. De tels histogrammes sont invariants aux rotations et aux translations locales, ainsi qu'aux changements d'échelle. Toutefois, ils sont fortement susceptibles aux changements radiométriques induits par l'environnement, comme la variation de la luminosité extérieure. En outre, la discrétisation des valeurs dans l'histogramme ajoute une robustesse au bruit, mais fait perdre la précision des valeurs, notamment dans le cas multispectral où des pics d'absorption caractéristiques peuvent alors disparaître. D'autres histogrammes usuels, comme les \gls{HOG}~\cite{dalal_histograms_2005} s'appliquent également aux images de télédétection.

En pratique, il est courant d'utiliser une combinaison de ces différentes caractéristiques expertes, considérées à différentes échelles. Une fois les vecteurs de caractéristiques générés, de nombreux modèles statistiques peuvent alors être appliqués.

\subsubsection{Modèles statistiques usuels}

Une fois le vecteur de caractéristiques extrait pour un échantillon, celui-ci sert alors d'entrée à un classifieur. Le classifieur est un modèle statistique de décision pouvant prendre plusieurs formes. Cette partie ne traite que des classifieurs sans apprentissage de représentation, excluant par conséquent les réseaux de neurones profonds.

La littérature en apprentissage automatique pour la télédétection a longtemps plébiscité les arbres de décision sous forme de forêts aléatoires~\cite{breiman_random_2001} et les \gls{SVM}~\cite{boser_training_1992,cortes_support-vector_1995}.

Les arbres de décision~\cite{breiman_classification_2017} forment un ensemble de modèles statistiques représentant les variables sous forme de n\oe{}ud intérieur, chaque arête correspondant à un ensemble de valeurs possibles pour la variable associée au n\oe{}ud. L'ensemble des arêtes partant d'un n\oe{}ud donné couvre l'ensemble des valeurs que peut prendre la variable qui lui est associée. Durant la phase d'apprentissage, l'arbre est construit par partitionnement récursif, divisant l'ensemble des données en fonction d'une première variable, puis de la seconde et ainsi de suite jusqu'à ce que l'ajout de variable n'améliore plus la prédiction, ou que tous les sous-ensemble aboutissent au même résultat.

Les arbres de décision sont couramment utilisés sous forme de forêts aléatoires~\cite{breiman_random_2001}. Une forêt aléatoire est en réalité un ensemble d'arbres décisions construits à partir de sous-ensembles aléatoires des variables d'entrée. Chaque arbre réalise sa prédiction indépendamment des autres, et la prédiction finale est celle ayant obtenu le plus de votes. Réaliser un tel apprentissage par ensemble permet d'obtenir un classifieur dont la variance est plus faible que chaque arbre individuel. Les arbres de décision ont l'avantage d'être simples à interpréter, puisqu'une décision (un n\oe{}ud) est associé à un test sur une caractéristique précise. Les forêts aléatoires ont ainsi été abondamment utilisées pour la télédétection pour la cartographie d'occupation des sols à partir d'images \gls{Landsat}~\cite{pal_random_2005} ou la prédiction météorologique~\cite{lary_machine_2016}. Les ensembles d'arbres de décision ont également été utilisés sur le principe du \emph{gradient boosting}, permettant d'exploiter un ensemble de modèles de prédiction faibles pour les combiner et renforcer leur pouvoir de prédiction~\cite{friedman_greedy_2001}. Ce type de classifieurs est moins courant en télédétection mais se retrouve néanmoins dans certaines applications~\cite{lawrence_classification_2004}.

Les \gls{SVM}~\cite{boser_training_1992,cortes_support-vector_1995} sont des classifieurs partitionnant l'espace de telle sorte que la distance entre la frontière et l'échantillon le plus proche (la marge) soit maximale. La frontière se représente sous la forme d'un hyperplan dans l'espace des données d'entrée dans le cas du noyau linéaire, ou d'un hyperplan dans un espace de représentation de grande dimension (possiblement infinie) en utilisant l'astuce du noyau~\cite{boser_training_1992}.

Dans le cas où la dimension des données d'entrée est de grande taille, le calcul exact des hyperplans à marge maximale n'est pas nécessairement réalisable en temps raisonnable. Dans ce cas, il est possible de faire appel à des algorithmes d'approximation reposant sur une optimisation par descente de gradient~\cite{bottou_large-scale_2010}.

Les \gls{SVM} ont trouvé de nombreuses applications en télédétection, notamment pour l'occupation des sols à partir d'images multispectrales~\cite{pal_support_2005} et hyperspectrales~\cite{melgani_classification_2004}.

Enfin, on retrouve également des réseaux de neurones de type perceptron multi-couches dans la littérature pour le traitement d'images multispectrales~\cite{benediktsson_neural_1990} et hyperspectrales~\cite{goel_classification_2003}.

\subsubsection{Caractéristiques spatiales et caractéristiques spectrales}

Comme nous l'avons vu, les caractéristiques expertes de la littérature en télédétection s'intéressent majoritairement au contenu radiométrique des pixels considérés. La plupart des méthodes réalisent ainsi une classification pixel à pixel, c'est-à-dire que les classifieurs ne réalisent qu'une seule prédiction à la fois, généralement pour le pixel central d'une région considérée.

L'avantage de cette méthode est qu'elle garantit une résolution identique à celle de la donnée. Néanmoins, cela ne permet pas nécessairement modéliser de façon satisfaisante les relations spatiales pouvant exister entre les objets. Compte-tenu de l'augmentation continue de la résolution des capteurs, les objets d'intérêt s'étendent souvent sur plusieurs pixels et présentent des propriétés géométriques particulières de connexité et de convexité. Ainsi, un pixel particulier soumis à un bruit extrême (suite à une défaillance du capteur ou à un matériau particulier) peut être mal classifié s'il est considéré isolément. Un classifieur prenant en compte son appartenance à un voisinage spatial particulier pourrait contourner cette erreur en se basant sur des critères d'homogénéité. Ainsi la classification pixellique tend à produire des cartes exhibant un comportement de bruit poivre-et-sel. Celles-ci sont alors régularisées \emph{a posteri} en utilisant des modèles graphiques, comme les \gls{CRF}.

Des méthodes de classification dites par \emph{patch} sont apparues afin de prendre en compte le contexte spatial d'un pixel. Notamment, ces méthodes réalisent une classification pour chaque voisinage carré extrait autour d'un pixel par fenêtre glissante. Ces approches ont connu un succès considérable grâce à l'apprentissage de représentation à l'aide de réseaux convolutifs pour la classification d'images. Si originellement des descripteurs experts à la fois spatiaux et spectraux ont été développées~\cite{fauvel_advances_2013}, les caractéristiques spatiales-spectrales automatiquement apprises par les réseaux profonds sont souvent plus performantes~\cite{nogueira_learning_2016,chen_deep_2016}. Divers travaux utilisent ainsi des \gls{CNN} via une fenêtre glissante pour prédire la classe du pixel central des régions considérées pour la détection de bâtiment~\cite{vakalopoulou_building_2015} et l'étude de l'occupation des sols~\cite{papadomanolaki_patch-based_2017}.

\begin{figure}
\resizebox{\textwidth}{!}{%
\input{Chapitre1/framework.tikz}
}
\caption{Segmentation sémantique par régions d'une image aérienne. Chaque région de l'image segmentée est classifiée à partir de caractéristiques profondes extraites d'un réseau convolutif pré-entraîné.}
\label{fig:framework}
\end{figure}

Néanmoins, le nombre de pixels croissant quadratiquement avec la taille de l'image, ces approches passent difficilement à l'échelle, notamment en imagerie \gls{HR}, \gls{THR} et \gls{EHR}. Une alternative proposée consiste à utiliser le classifieur réaliser non pas une prédiction pour un unique pixel, mais pour une région toute entière.

En particulier, l'approche de classification par régions consiste à regrouper ensemble des pixels similaires dans des régions homogènes. Le critère de similarité dans ce cas dépend principalement des valeurs des pixels et de leurs positions relatives. Le classifieur réalisera ensuite une prédiction unique pour l'ensemble des pixels d'une même région, en faisant l'hypothèse que des pixels spatialement et spectralement proches possèdent la même sémantique, c'est-à-dire que l'on puisse les associer à la même classe d'intérêt. Dans ce cas, plutôt que de réaliser la classification pixel par pixel, il est possible de ne réaliser qu'une seule extraction d'attributs pour l'ensemble de la région homogène et de ne faire qu'une unique prédiction. Ainsi, dans le cas d'une image de dimensions $1 500\times1 500$ segmentée en \nombre{20000} régions, on peut alors cartographier l'image en classifiant \nombre{20000} régions plutôt qu'en classifiant \nombre{2250000} pixels.

De nombreux algorithmes de segmentation ont été proposés, aussi bien dans la communauté télédétection que dans la communauté vision par ordinateur. Ces algorithmes de segmentation viennent partitionner l'ensemble des pixels de façon non-supervisée. Une fois ce partitionnement effectué, il est possible d'extraire des attributs pour chaque région et d'entraîner un classifieur de la façon usuelle, l'inférence se faisant de façon analogue. Ce procédé est illustré dans la~\cref{fig:framework}.

L'avantage principal de ces méthodes est de grandement réduire la complexité calculatoire. Notamment, lorsque l'image augmente de résolution spatiale, de nombreuses régions conservent leur homogénéité et il est ainsi très avantageux de les conserver regroupées\,: l'augmentation de la résolution ne nécessite pas obligatoirement une évolution quadratique du nombre de régions segmentées. Depuis les premiers travaux de~\citet{mnih_machine_2013} utilisant les \gls{CNN} pour l'extraction de routes et de bâtiments dans des images aériennes à partir d'imagettes, ces approches ont ainsi été utilisées avec succès sur de nombreux jeux de données \gls{THR}~\cite{lagrange_benchmarking_2015,vargas_superpixel-based_2014}.

Il existe par ailleurs des approches nativement multi-échelles utilisant une segmentation sous forme d'arbre hiérarchique. Il est alors possible d'extraire des profils d'attributs à différents niveaux de l'arbre et de les utiliser comme caractéristiques pour une classification supervisée~\cite{cui_combining_2016,pham_feature_2018}.

On peut noter que traiter l'image par une fenêtre glissante ou même par une grille de pixels correspond à des cas particuliers de classification par région. Les méthodes de classification par \emph{patch} entrent ainsi dans cette famille de méthodes~\cite{papadomanolaki_patch-based_2017,vakalopoulou_building_2015}.

En parallèle de cette thèse, les approches utilisant les \glsfirst{FCN} pour la segmentation sémantique d'images de télédétection se sont nettement développées. Ces approches réalisent une prédiction pixellique en s'affranchissant du principal problème de la classification par \emph{patch}. Plutôt que de réaliser une prédiction unique pour toute une image, les \gls{FCN} permettent de classifier l'ensemble des pixels considérés en une seule passe d'inférence, réduisant ainsi drastiquement les temps de calcul, sans pour autant nécessiter d'une segmentation préalable non supervisée. Les premières applications de \gls{FCN} sur des données aériennes optiques apparaissent en 2015 chez \citet{paisitkriangkrai_effective_2015,sherrah_fully_2016} en utilisant tout d'abord les architectures classiques de~\citet{long_fully_2015}. Les modèles encodeur-décodeurs symétriques suivent rapidement~\cite{volpi_dense_2017,audebert_semantic_2016} incluant différentes variantes, comme l'utilisation de~\gls{CRF}~\cite{liu_dense_2017} pour la fusion de données ou la régularisation des bordures par contrainte explicite~\cite{marmanis_classification_2017}. Les images satellites subissent le même traitement~\cite{fu_classification_2017}.

De nouveaux jeux de données comme l'Inria Aerial Image Labeling Dataset sont en particulier un terrain de jeu propice aux réseaux entièrement convolutifs qui dominent très largement les classements~\cite{huang_large-scale_2018}. Dans l'ensemble, les méthodes par apprentissage profond utilisant les réseaux entièrement convolutifs se sont imposées comme le nouvel état de l'art sur de nombreuses tâches d'interprétation d'images de télédétection~\cite{liu_comparing_2018}. L'apparition de nombreux jeux de données annotés de télédétection, détaillés en~\cref{chap:datasets}, a notamment permis d'entraîner des modèles profonds sur une large variété d'applications.

\bibliographystyle{plainnat}
\bibliography{Chapitre1/Historique}
