
@article{nekrasov_global_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.03930},
  primaryClass = {cs},
  title = {Global {{Deconvolutional Networks}} for {{Semantic Segmentation}}},
  abstract = {Semantic image segmentation is an important low-level computer vision problem aimed to correctly classify each individual pixel of the image. Recent empirical improvements achieved in this area have primarily been motivated by successful exploitation of Convolutional Neural Networks (CNNs) pre-trained for image classification and object recognition tasks. However, the pixel-wise labeling with CNNs has its own unique challenges: (1) an accurate deconvolution, or upsampling, of low-resolution output into a higher-resolution segmentation mask and (2) an inclusion of global information, or context, within locally extracted features. To address these issues, we propose a novel architecture to conduct the deconvolution operation and acquire dense predictions, and an additional refinement, which allows to incorporate global information into the network. We demonstrate that these alterations lead to improved performance of state-of-the-art semantic segmentation models on the PASCAL VOC 2012 benchmark.},
  journal = {arXiv:1602.03930 [cs]},
  author = {Nekrasov, Vladimir and Ju, Janghoon and Choi, Jaesik},
  month = feb,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//arXiv1602.03930 [cs]/2016/Nekrasov et al 2016 - Global Deconvolutional Networks for Semantic Segmentation.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/RMQZVX7S/1602.html}
}

@inproceedings{zhao_stacked_2015,
  title = {Stacked {{What}}-{{Where Auto}}-Encoders},
  abstract = {We present a novel architecture, the "stacked what-where auto-encoders" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the "what" which are fed to the next layer, and its complementary variable "where" that are fed to the corresponding layer in the generative decoder.},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}}},
  author = {Zhao, Junbo and Mathieu, Michael and Goroshin, Ross and LeCun, Yann},
  month = jun,
  year = {2015},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Learning,Statistics - Machine Learning},
  file = {/home/naudeber/Bibliographie//undefined/2015/Zhao et al 2015 - Stacked What-Where Auto-encoders.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/297AMMBV/1506.html}
}

@inproceedings{audebert_semantic_2016,
  title = {Semantic {{Segmentation}} of {{Earth Observation Data Using Multimodal}} and {{Multi}}-Scale {{Deep Networks}}},
  doi = {10.1007/978-3-319-54181-5_12},
  abstract = {This work investigates the use of deep fully convolutional neural networks (DFCNN) for pixel-wise scene labeling of Earth Observation images. Especially, we train a variant of the SegNet architecture on remote sensing data over an urban area and study different strategies for performing accurate semantic segmentation. Our contributions are the following: (1) we transfer efficiently a DFCNN from generic everyday images to remote sensing images; (2) we introduce a multi-kernel convolutional layer for fast aggregation of predictions at multiple scales; (3) we perform data fusion from heterogeneous sensors (optical and laser) using residual correction. Our framework improves state-of-the-art accuracy on the ISPRS Vaihingen 2D Semantic Labeling dataset.},
  booktitle = {Computer {{Vision}} \textendash{} {{ACCV}} 2016},
  publisher = {{Springer, Cham}},
  author = {Audebert, Nicolas and Le Saux, Bertrand and Lef{\`e}vre, S{\'e}bastien},
  month = nov,
  year = {2016},
  pages = {180-196},
  file = {/home/naudeber/Bibliographie//Springer, Cham/2016/Audebert et al 2016 - Semantic Segmentation of Earth Observation Data Using Multimodal and.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/394SFXNP/Audebert et al_2016_Semantic Segmentation of Earth Observation Data Using Multimodal and.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/PC7RWZAX/978-3-319-54181-5_12.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/TJHU6H38/978-3-319-54181-5_12.html}
}

@article{simonyan_very_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.1556},
  primaryClass = {cs},
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  journal = {arXiv:1409.1556 [cs]},
  author = {Simonyan, Karen and Zisserman, Andrew},
  month = sep,
  year = {2014},
  keywords = {À lire,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//arXiv1409.1556 [cs]/2014/Simonyan Zisserman 2014 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8VM3BPQT/1409.html}
}

@incollection{zeiler_visualizing_2014,
  title = {Visualizing and Understanding Convolutional Networks},
  booktitle = {Computer {{Vision}}\textendash{{ECCV}} 2014},
  publisher = {{Springer}},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  year = {2014},
  pages = {818--833},
  file = {/home/naudeber/Bibliographie//Springer/2014/Zeiler Fergus 2014 - Visualizing and understanding convolutional networks.pdf}
}

@article{russakovsky_imagenet_2015,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  volume = {115},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-015-0816-y},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
  language = {en},
  number = {3},
  journal = {International Journal of Computer Vision},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and {Fei-Fei}, Li},
  month = apr,
  year = {2015},
  pages = {211-252},
  file = {/home/naudeber/Bibliographie//International Journal of Computer Vision/2015/Russakovsky et al 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/3VH7ZBJ3/s11263-015-0816-y.html}
}

@article{lecun_gradient-based_1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  volume = {86},
  issn = {0018-9219},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day},
  number = {11},
  journal = {Proceedings of the IEEE},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  month = nov,
  year = {1998},
  keywords = {2D shape variability,back-propagation,backpropagation,Character recognition,cheque reading,complex decision surface synthesis,convolution,convolutional neural network character recognizers,document recognition,document recognition systems,feature extraction,Feature extraction,field extraction,gradient based learning technique,gradient-based learning,graph transformer networks,GTN,handwritten character recognition,handwritten digit recognition task,Hidden Markov models,high-dimensional patterns,language modeling,Machine learning,Multi-layer neural network,multilayer neural networks,multilayer perceptrons,multimodule systems,Neural networks,optical character recognition,Optical character recognition software,Optical computing,Pattern recognition,Pattern Recognition,performance measure minimization,principal component analysis,Principal component analysis,segmentation recognition},
  pages = {2278-2324},
  file = {/home/naudeber/Bibliographie//Proceedings of the IEEE/1998/Lecun et al 1998 - Gradient-based learning applied to document recognition.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/ANY9HKIA/726791.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HUI6PF8F/abs_all.html}
}

@inproceedings{yosinski_how_2014,
  title = {How Transferable Are Features in Deep Neural Networks?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  year = {2014},
  pages = {3320--3328},
  file = {/home/naudeber/Bibliographie//undefined/2014/Yosinski et al 2014 - How transferable are features in deep neural networks.pdf}
}

@article{stewart_local_2012,
  title = {Local {{Climate Zones}} for {{Urban Temperature Studies}}},
  volume = {93},
  issn = {0003-0007},
  doi = {10.1175/BAMS-D-11-00019.1},
  abstract = {The effect of urban development on local thermal climate is widely documented in scientific literature. Observations of urban\textendash{}rural air temperature differences\textemdash{}or urban heat islands (UHIs)\textemdash{}have been reported for cities and regions worldwide, often with local field sites that are extremely diverse in their physical and climatological characteristics. These sites are usually described only as ``urban'' or ``rural,'' leaving much uncertainty about the actual exposure and land cover of the sites. To address the inadequacies of urban\textendash{}rural description, the ``local climate zone'' (LCZ) classification system has been developed. The LCZ system comprises 17 zone types at the local scale (102 to 104 m). Each type is unique in its combination of surface structure, cover, and human activity. Classification of sites into appropriate LCZs requires basic metadata and surface characterization. The zone definitions provide a standard framework for reporting and comparing field sites and their temperature observations. The LCZ system is designed primarily for urban heat island researchers, but it has derivative uses for city planners, landscape ecologists, and global climate change investigators.},
  number = {12},
  journal = {Bulletin of the American Meteorological Society},
  author = {Stewart, I. D. and Oke, T. R.},
  month = may,
  year = {2012},
  pages = {1879-1900},
  file = {/home/naudeber/Bibliographie//Bulletin of the American Meteorological Society/2012/Stewart Oke 2012 - Local Climate Zones for Urban Temperature Studies.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/AEEBUUNM/BAMS-D-11-00019.html}
}

@article{rottensteiner_isprs_2012,
  title = {The {{ISPRS}} Benchmark on Urban Object Classification and {{3D}} Building Reconstruction},
  volume = {1},
  journal = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  author = {Rottensteiner, Franz and Sohn, Gunho and Jung, Jaewook and Gerke, Markus and Baillard, Caroline and Benitez, Sebastien and Breitkopf, Uwe},
  year = {2012},
  pages = {3},
  file = {/home/naudeber/Bibliographie//ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci/2012/Rottensteiner et al 2012 - The ISPRS benchmark on urban object classification and 3D building.pdf}
}

@article{srivastava_dropout_2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  volume = {15},
  shorttitle = {Dropout},
  journal = {Journal of Machine Learning Research},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  pages = {1929-1958},
  file = {/home/naudeber/Bibliographie//Journal of Machine Learning Research/2014/Srivastava et al 2014 - Dropout.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/CDBFKSEG/srivastava14a.html}
}

@inproceedings{sutskever_importance_2013,
  title = {On the Importance of Initialization and Momentum in Deep Learning},
  author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  year = {2013},
  pages = {1139-1147},
  file = {/home/naudeber/Bibliographie//undefined/2013/Sutskever et al 2013 - On the importance of initialization and momentum in deep learning.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/ER599R43/momentum.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/NZEUJWS4/sutskever13.html}
}

@article{zeiler_adadelta_2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1212.5701},
  primaryClass = {cs},
  title = {{{ADADELTA}}: {{An Adaptive Learning Rate Method}}},
  shorttitle = {{{ADADELTA}}},
  abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
  journal = {arXiv:1212.5701 [cs]},
  author = {Zeiler, Matthew D.},
  month = dec,
  year = {2012},
  keywords = {Computer Science - Learning},
  file = {/home/naudeber/Bibliographie//arXiv1212.5701 [cs]/2012/Zeiler 2012 - ADADELTA_4.pdf;/home/naudeber/Bibliographie//arXiv1212.5701 [cs]/2012/Zeiler 2012 - ADADELTA.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/2M53HISV/1212.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/P4SSV3BD/1212.html}
}

@article{kingma_adam_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6980},
  primaryClass = {cs},
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  journal = {arXiv:1412.6980 [cs]},
  author = {Kingma, Diederik and Ba, Jimmy},
  month = dec,
  year = {2014},
  keywords = {Computer Science - Learning},
  file = {/home/naudeber/Bibliographie//arXiv1412.6980 [cs]/2014/Kingma Ba 2014 - Adam_3.pdf;/home/naudeber/Bibliographie//arXiv1412.6980 [cs]/2014/Kingma Ba 2014 - Adam.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/2ZWBWNCV/1412.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/ERWP2VRK/1412.html}
}

@inproceedings{yu_multi-scale_2015,
  title = {Multi-{{Scale Context Aggregation}} by {{Dilated Convolutions}}},
  abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}}},
  author = {Yu, Fisher and Koltun, Vladlen},
  month = nov,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//undefined/2015/Yu Koltun 2015 - Multi-Scale Context Aggregation by Dilated Convolutions_2.pdf;/home/naudeber/Bibliographie//undefined/2015/Yu Koltun 2015 - Multi-Scale Context Aggregation by Dilated Convolutions.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/M4N6U752/1511.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WT6KRJ7K/1511.html}
}

@article{everingham_pascal_2014,
  title = {The {{Pascal Visual Object Classes Challenge}}: {{A Retrospective}}},
  volume = {111},
  issn = {0920-5691, 1573-1405},
  shorttitle = {The {{Pascal Visual Object Classes Challenge}}},
  doi = {10.1007/s11263-014-0733-5},
  abstract = {The Pascal Visual Object Classes (VOC) challenge consists of two components: (i) a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii) an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008\textendash{}2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community's progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges.},
  language = {en},
  number = {1},
  journal = {International Journal of Computer Vision},
  author = {Everingham, Mark and Eslami, S. M. Ali and Gool, Luc Van and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
  month = jun,
  year = {2014},
  keywords = {Image Processing and Computer Vision,Computer Imaging; Vision; Pattern Recognition and Graphics,Artificial Intelligence (incl. Robotics),Pattern Recognition,segmentation,object detection,object recognition,Database,Benchmark},
  pages = {98-136},
  file = {/home/naudeber/Bibliographie//International Journal of Computer Vision/2014/Everingham et al 2014 - The Pascal Visual Object Classes Challenge.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/4CFZXJMT/10.html}
}

@article{razakarivony_vehicle_2016,
  title = {Vehicle {{Detection}} in {{Aerial Imagery}}: {{A}} Small Target Detection Benchmark},
  volume = {34},
  shorttitle = {Vehicle {{Detection}} in {{Aerial Imagery}}},
  journal = {Journal of Visual Communication and Image Representation},
  author = {Razakarivony, S{\'e}bastien and Jurie, Fr{\'e}d{\'e}ric},
  year = {2016},
  pages = {187--203},
  file = {/home/naudeber/Bibliographie//Journal of Visual Communication and Image Representation/2016/Razakarivony Jurie 2016 - Vehicle Detection in Aerial Imagery.pdf}
}

@article{dumoulin_guide_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.07285},
  primaryClass = {cs, stat},
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
  journal = {arXiv:1603.07285 [cs, stat]},
  author = {Dumoulin, Vincent and Visin, Francesco},
  month = mar,
  year = {2016},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Learning,Statistics - Machine Learning},
  file = {/home/naudeber/Bibliographie//arXiv1603.07285 [cs, stat]/2016/Dumoulin Visin 2016 - A guide to convolution arithmetic for deep learning_3.pdf;/home/naudeber/Bibliographie//arXiv1603.07285 [cs, stat]/2016/Dumoulin Visin 2016 - A guide to convolution arithmetic for deep learning.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/RURBKXLY/Dumoulin et Visin - 2016 - A guide to convolution arithmetic for deep learnin.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/3Q8459H9/1603.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/IPC7JT57/1603.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/Q97C8IHK/1603.html}
}

@article{arnab_higher_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.08119},
  primaryClass = {cs},
  title = {Higher {{Order Conditional Random Fields}} in {{Deep Neural Networks}}},
  abstract = {We address the problem of semantic segmentation using deep learning. Most segmentation systems include a Conditional Random Field (CRF) to produce a structured output that is consistent with the image's visual features. Recent deep learning approaches have incorporated CRFs into Convolutional Neural Networks (CNNs), with some even training the CRF end-to-end with the rest of the network. However, these approaches have not employed higher order potentials, which have previously been shown to significantly improve segmentation performance. In this paper, we demonstrate that two types of higher order potential, based on object detections and superpixels, can be included in a CRF embedded within a deep network. We design these higher order potentials to allow inference with the differentiable mean field algorithm. As a result, all the parameters of our richer CRF model can be learned end-to-end with our pixelwise CNN classifier. We achieve state-of-the-art segmentation performance on the PASCAL VOC benchmark with these trainable higher order potentials.},
  journal = {arXiv:1511.08119 [cs]},
  author = {Arnab, Anurag and Jayasumana, Sadeep and Zheng, Shuai and Torr, Philip},
  month = nov,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//arXiv1511.08119 [cs]/2015/Arnab et al 2015 - Higher Order Conditional Random Fields in Deep Neural Networks.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/MEJRWJHW/1511.html}
}

@article{wu_high-performance_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1604.04339},
  primaryClass = {cs},
  title = {High-Performance {{Semantic Segmentation Using Very Deep Fully Convolutional Networks}}},
  abstract = {We propose a method for high-performance semantic image segmentation (or semantic pixel labelling) based on very deep residual networks, which achieves the state-of-the-art performance. A few design factors are carefully considered to this end. We make the following contributions. (i) First, we evaluate different variations of a fully convolutional residual network so as to find the best configuration, including the number of layers, the resolution of feature maps, and the size of field-of-view. Our experiments show that further enlarging the field-of-view and increasing the resolution of feature maps are typically beneficial, which however inevitably leads to a higher demand for GPU memories. To walk around the limitation, we propose a new method to simulate a high resolution network with a low resolution network, which can be applied during training and/or testing. (ii) Second, we propose an online bootstrapping method for training. We demonstrate that online bootstrapping is critically important for achieving good accuracy. (iii) Third we apply the traditional dropout to some of the residual blocks, which further improves the performance. (iv) Finally, our method achieves the currently best mean intersection-over-union 78.3$\backslash$\% on the PASCAL VOC 2012 dataset, as well as on the recent dataset Cityscapes.},
  journal = {arXiv:1604.04339 [cs]},
  author = {Wu, Zifeng and Shen, Chunhua and Van Den Hengel, Anton},
  month = apr,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//arXiv1604.04339 [cs]/2016/Wu et al 2016 - High-performance Semantic Segmentation Using Very Deep Fully Convolutional.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/TVNJGJ62/1604.html}
}

@article{marmanis_classification_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.01337},
  title = {Classification {{With}} an {{Edge}}: {{Improving Semantic Image Segmentation}} with {{Boundary Detection}}},
  shorttitle = {Classification {{With}} an {{Edge}}},
  abstract = {We present an end-to-end trainable deep convolutional neural network (DCNN) for semantic segmentation with built-in awareness of semantically meaningful boundaries. Semantic segmentation is a fundamental remote sensing task, and most state-of-the-art methods rely on DCNNs as their workhorse. A major reason for their success is that deep networks learn to accumulate contextual information over very large windows (receptive fields). However, this success comes at a cost, since the associated loss of effecive spatial resolution washes out high-frequency details and leads to blurry object boundaries. Here, we propose to counter this effect by combining semantic segmentation with semantically informed edge detection, thus making class-boundaries explicit in the model, First, we construct a comparatively simple, memory-efficient model by adding boundary detection to the Segnet encoder-decoder architecture. Second, we also include boundary detection in FCN-type models and set up a high-end classifier ensemble. We show that boundary detection significantly improves semantic segmentation with CNNs. Our high-end ensemble achieves $>$ 90\% overall accuracy on the ISPRS Vaihingen benchmark.},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  author = {Marmanis, Dimitrios and Schindler, Konrad and Wegner, Jan Dirk and Galliani, Silvano and Datcu, Mihai and Stilla, Uwe},
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//arXiv1612.01337 [cs]/2016/Marmanis et al 2016 - Classification With an Edge.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/XBNT2Q32/Marmanis et al_2016_Classification With an Edge.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VEPZ68S6/1612.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/Z3Z4WVNI/1612.html}
}

@inproceedings{clevert_fast_2015,
  title = {Fast and {{Accurate Deep Network Learning}} by {{Exponential Linear Units}} ({{ELUs}})},
  abstract = {We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10\% classification error for a single crop, single model network.},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}}},
  author = {Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  month = nov,
  year = {2015},
  keywords = {Computer Science - Learning},
  file = {/home/naudeber/Bibliographie//undefined/2015/Clevert et al 2015 - Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)_2.pdf;/home/naudeber/Bibliographie//undefined/2015/Clevert et al 2015 - Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/DJF95A6V/1511.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VGGRFHEP/1511.html}
}

@article{marmanis_deep_2016,
  title = {Deep {{Learning Earth Observation Classification Using ImageNet Pretrained Networks}}},
  volume = {13},
  issn = {1545-598X},
  doi = {10.1109/LGRS.2015.2499239},
  abstract = {Deep learning methods such as convolutional neural networks (CNNs) can deliver highly accurate classification results when provided with large enough data sets and respective labels. However, using CNNs along with limited labeled data can be problematic, as this leads to extensive overfitting. In this letter, we propose a novel method by considering a pretrained CNN designed for tackling an entirely different classification problem, namely, the ImageNet challenge, and exploit it to extract an initial set of representations. The derived representations are then transferred into a supervised CNN classifier, along with their class labels, effectively training the system. Through this two-stage framework, we successfully deal with the limited-data problem in an end-to-end processing scheme. Comparative results over the UC Merced Land Use benchmark prove that our method significantly outperforms the previously best stated results, improving the overall accuracy from 83.1\% up to 92.4\%. Apart from statistical improvements, our method introduces a novel feature fusion algorithm that effectively tackles the large data dimensionality by using a simple and computationally efficient approach.},
  number = {1},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  author = {Marmanis, D. and Datcu, M. and Esch, T. and Stilla, U.},
  month = jan,
  year = {2016},
  keywords = {Adaptation models,Arrays,convolutional neural networks,Convolutional neural networks (CNNs),Data models,deep learning (DL),deep learning earth observation classification,deep learning method,end-to-end processing scheme,feature extraction,Feature extraction,fusion algorithm,geophysical techniques,ImageNet pretrained networks,land-use classification,limited labeled data,limited-data problem,neural nets,Neural networks,pretrained network,remote sensing,Remote sensing,remote sensing (RS),supervised CNN classifier,Training,UC Merced Land Use benchmark},
  pages = {105-109},
  file = {/home/naudeber/Bibliographie//IEEE Geoscience and Remote Sensing Letters/2016/Marmanis et al 2016 - Deep Learning Earth Observation Classification Using ImageNet Pretrained.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/6NHD6Y5U/Marmanis et al. - 2016 - Deep Learning Earth Observation Classification Usi.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/CDKGES4I/7342907.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/H2VNFIIW/login.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/T3TS89A6/7342907.html}
}

@inproceedings{szegedy_going_2015,
  title = {Going Deeper with Convolutions},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2015},
  pages = {1--9},
  file = {/home/naudeber/Bibliographie//undefined/2015/Szegedy et al 2015 - Going deeper with convolutions_2.pdf;/home/naudeber/Bibliographie//undefined/2015/Szegedy et al 2015 - Going deeper with convolutions.pdf}
}

@inproceedings{ioffe_batch_2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  pages = {448-456},
  file = {/home/naudeber/Bibliographie//undefined/2015/Ioffe Szegedy 2015 - Batch Normalization.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/SZXJDI3V/ioffe15.html}
}

@incollection{lin_microsoft_2014,
  series = {Lecture Notes in Computer Science},
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  copyright = {\textcopyright{}2014 Springer International Publishing Switzerland},
  isbn = {978-3-319-10601-4 978-3-319-10602-1},
  shorttitle = {Microsoft {{COCO}}},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  language = {en},
  number = {8693},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  publisher = {{Springer International Publishing}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  month = sep,
  year = {2014},
  keywords = {Artificial Intelligence (incl. Robotics),Computer Graphics,Image Processing and Computer Vision,Pattern Recognition},
  pages = {740-755},
  file = {/home/naudeber/Bibliographie//Springer International Publishing/2014/Lin et al 2014 - Microsoft COCO.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/27T2J7FZ/978-3-319-10602-1_48.html},
  doi = {10.1007/978-3-319-10602-1_48}
}

@inproceedings{chatfield_return_2014,
  title = {Return of the {{Devil}} in the {{Details}}: {{Delving Deep}} into {{Convolutional Nets}}},
  isbn = {978-1-901725-52-0},
  shorttitle = {Return of the {{Devil}} in the {{Details}}},
  doi = {10.5244/C.28.6},
  language = {en},
  booktitle = {Proceedings of the {{British Machine Vision Conference}}},
  publisher = {{British Machine Vision Association}},
  author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2014},
  pages = {6.1-6.12}
}

@article{cramer_dgpf_2010,
  title = {The {{DGPF}} Test on Digital Aerial Camera Evaluation \textendash{} Overview and Test Design},
  volume = {2},
  journal = {Photogrammetrie \textendash{} Fernerkundung \textendash{} Geoinformation},
  author = {Cramer, M.},
  year = {2010},
  pages = {73--82}
}

@inproceedings{maggiori_fully_2016,
  title = {Fully Convolutional Neural Networks for Remote Sensing Image Classification},
  doi = {10.1109/IGARSS.2016.7730322},
  abstract = {We propose a convolutional neural network (CNN) model for remote sensing image classification. Using CNNs provides us with a means of learning contextual features for large-scale image labeling. Our network consists of four stacked convolutional layers that downsample the image and extract relevant features. On top of these, a deconvolutional layer upsamples the data back to the initial resolution, producing a final dense image labeling. Contrary to previous frameworks, our network contains only convolution and deconvolution operations. Experiments on aerial images show that our network produces more accurate classifications in lower computational time.},
  booktitle = {2016 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}} ({{IGARSS}})},
  author = {Maggiori, E. and Tarabalka, Y. and Charpiat, G. and Alliez, P.},
  month = jul,
  year = {2016},
  keywords = {Context,Labeling,neural nets,learning (artificial intelligence),geophysical image processing,aerial images,remote sensing image classification,Neural networks,convolutional neural networks,classification,convolution,computational time,contextual feature learning,convolutional neural network model,final dense image labeling,initial resolution,large-scale image labeling,stacked deconvolutional layers,Image resolution,Remote sensing images,Training,deep learning,image classification,remote sensing},
  pages = {5071-5074},
  file = {/home/naudeber/Bibliographie//undefined/2016/Maggiori et al 2016 - Fully convolutional neural networks for remote sensing image classification.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/TEW7NM87/7730322.html}
}

@article{campos-taberner_processing_2016,
  title = {Processing of {{Extremely High}}-{{Resolution LiDAR}} and {{RGB Data}}: {{Outcome}} of the 2015 {{IEEE GRSS Data Fusion Contest Part A}}: 2-{{D Contest}}},
  volume = {9},
  issn = {1939-1404},
  shorttitle = {Processing of {{Extremely High}}-{{Resolution LiDAR}} and {{RGB Data}}},
  doi = {10.1109/JSTARS.2016.2569162},
  abstract = {In this paper, we discuss the scientific outcomes of the 2015 data fusion contest organized by the Image Analysis and Data Fusion Technical Committee (IADF TC) of the IEEE Geoscience and Remote Sensing Society (IEEE GRSS). As for previous years, the IADF TC organized a data fusion contest aiming at fostering new ideas and solutions for multisource studies. The 2015 edition of the contest proposed a multiresolution and multisensorial challenge involving extremely high-resolution RGB images and a three-dimensional (3-D) LiDAR point cloud. The competition was framed in two parallel tracks, considering 2-D and 3-D products, respectively. In this paper, we discuss the scientific results obtained by the winners of the 2-D contest, which studied either the complementarity of RGB and LiDAR with deep neural networks (winning team) or provided a comprehensive benchmarking evaluation of new classification strategies for extremely high-resolution multimodal data (runner-up team). The data and the previously undisclosed ground truth will remain available for the community and can be obtained at http://www.grss-ieee.org/community/technical-committees/data-fusion/2015-ieee-grss-data-fusion-contest/. The 3-D part of the contest is discussed in the Part-B paper [1].},
  number = {12},
  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  author = {{Campos-Taberner}, M. and {Romero-Soriano}, A. and Gatta, C. and {Camps-Valls}, G. and Lagrange, A. and Le Saux, B. and Beaup{\`e}re, A. and Boulch, A. and {Chan-Hon-Tong}, A. and Herbin, S. and Randrianarivo, H. and Ferecatu, M. and Shimoni, M. and Moser, G. and Tuia, D.},
  month = dec,
  year = {2016},
  keywords = {deep neural networks,Laser radar,Data integration,Earth,Spatial resolution,Three-dimensional displays,LiDAR,extremely high spatial resolution,image analysis and data fusion (IADF),landcover classification,multimodal-data fusion,multiresolution-,multisource-,remote sensing},
  pages = {5547-5559},
  file = {/home/naudeber/Bibliographie//IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing/2016/Campos-Taberner et al 2016 - Processing of Extremely High-Resolution LiDAR and RGB Data.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FC7U9666/articleDetails.html}
}

@article{sherrah_fully_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.02585},
  primaryClass = {cs},
  title = {Fully {{Convolutional Networks}} for {{Dense Semantic Labelling}} of {{High}}-{{Resolution Aerial Imagery}}},
  abstract = {The trend towards higher resolution remote sensing imagery facilitates a transition from land-use classification to object-level scene understanding. Rather than relying purely on spectral content, appearance-based image features come into play. In this work, deep convolutional neural networks (CNNs) are applied to semantic labelling of high-resolution remote sensing data. Recent advances in fully convolutional networks (FCNs) are adapted to overhead data and shown to be as effective as in other domains. A full-resolution labelling is inferred using a deep FCN with no downsampling, obviating the need for deconvolution or interpolation. To make better use of image features, a pre-trained CNN is fine-tuned on remote sensing data in a hybrid network context, resulting in superior results compared to a network trained from scratch. The proposed approach is applied to the problem of labelling high-resolution aerial imagery, where fine boundary detail is important. The dense labelling yields state-of-the-art accuracy for the ISPRS Vaihingen and Potsdam benchmark data sets.},
  journal = {arXiv:1606.02585 [cs]},
  author = {Sherrah, Jamie},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//arXiv1606.02585 [cs]/2016/Sherrah 2016 - Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/R8U6HVPF/1606.html}
}

@phdthesis{mnih_machine_2013,
  title = {Machine {{Learning}} for {{Aerial Image Labeling}}},
  school = {University of Toronto},
  author = {Mnih, Volodymyr},
  year = {2013}
}

@article{volpi_dense_2017,
  title = {Dense {{Semantic Labeling}} of {{Subdecimeter Resolution Images With Convolutional Neural Networks}}},
  volume = {55},
  issn = {0196-2892},
  doi = {10.1109/TGRS.2016.2616585},
  abstract = {Semantic labeling (or pixel-level land-cover classification) in ultrahigh-resolution imagery ($<$;10 cm) requires statistical models able to learn high-level concepts from spatial data, with large appearance variations. Convolutional neural networks (CNNs) achieve this goal by learning discriminatively a hierarchy of representations of increasing abstraction. In this paper, we present a CNN-based system relying on a downsample-then-upsample architecture. Specifically, it first learns a rough spatial map of high-level representations by means of convolutions and then learns to upsample them back to the original resolution by deconvolutions. By doing so, the CNN learns to densely label every pixel at the original resolution of the image. This results in many advantages, including: 1) the state-of-the-art numerical accuracy; 2) the improved geometric accuracy of predictions; and 3) high efficiency at inference time. We test the proposed system on the Vaihingen and Potsdam subdecimeter resolution data sets, involving the semantic labeling of aerial images of 9- and 5-cm resolution, respectively. These data sets are composed by many large and fully annotated tiles, allowing an unbiased evaluation of models making use of spatial information. We do so by comparing two standard CNN architectures with the proposed one: standard patch classification, prediction of local label patches by employing only convolutions, and full patch labeling by employing deconvolutions. All the systems compare favorably or outperform a state-of-the-art baseline relying on superpixels and powerful appearance descriptors. The proposed full patch labeling CNN outperforms these models by a large margin, also showing a very appealing inference time.},
  number = {2},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  author = {Volpi, M. and Tuia, D.},
  month = feb,
  year = {2017},
  keywords = {Labeling,neural nets,convolutional neural network,geophysical image processing,aerial images,semantic labeling,classification,Data models,Convolutional neural networks (CNNs),Image resolution,Machine learning,Semantics,deep learning,feature extraction,image classification,remote sensing,land cover,semantic Web,CNN-based system,Potsdam subdecimeter resolution dataset,Vaihingen subdecimeter resolution dataset,patch labeling,pixel-level land cover classification,standard patch classification,statistical model,subdecimeter resolution images,ultrahigh-resolution imagery,deconvolution networks,subdecimeter resolution},
  pages = {881-893},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/UVV6QJZ8/7725499.html}
}

@inproceedings{he_deep_2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\texttimes{} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month = jun,
  year = {2016},
  keywords = {neural nets,object detection,learning (artificial intelligence),Image recognition,Visualization,Neural networks,Training,image classification,image segmentation,CIFAR-10,COCO object detection dataset,COCO segmentation,ILSVRC \& COCO 2015 competitions,ILSVRC 2015 classification task,ImageNet dataset,ImageNet localization,ImageNet test set,VGG nets,deep residual learning,deep residual nets,deeper neural network training,residual function learning,residual nets,visual recognition tasks,Complexity theory,Degradation},
  pages = {770-778},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/KUCF9Q86/7780459.html}
}

@inproceedings{long_fully_2015,
  title = {Fully Convolutional Networks for Semantic Segmentation},
  doi = {10.1109/CVPR.2015.7298965},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build ``fully convolutional'' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  month = jun,
  year = {2015},
  keywords = {Adaptation models,Computer architecture,contemporary classification networks,convolution,Deconvolution,fully convolutional networks,image classification,image segmentation,inference,inference mechanisms,learning,learning (artificial intelligence),NYUDv2,Pascal VOC,pixels-to-pixels,semantic segmentation,Semantics,SIFT flow,Training,visual models},
  pages = {3431-3440},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/BGDX4IA8/7298965.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VAQSQ2JQ/7298965.html}
}

@inproceedings{paisitkriangkrai_effective_2015,
  title = {Effective Semantic Pixel Labelling with Convolutional Networks and {{Conditional Random Fields}}},
  doi = {10.1109/CVPRW.2015.7301381},
  abstract = {Large amounts of available training data and increasing computing power have led to the recent success of deep convolutional neural networks (CNN) on a large number of applications. In this paper, we propose an effective semantic pixel labelling using CNN features, hand-crafted features and Conditional Random Fields (CRFs). Both CNN and hand-crafted features are applied to dense image patches to produce per-pixel class probabilities. The CRF infers a labelling that smooths regions while respecting the edges present in the imagery. The method is applied to the ISPRS 2D semantic labelling challenge dataset with competitive classification accuracy.},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Paisitkriangkrai, Sakrapee and Sherrah, Jamie and Janney, Pranam and Van Den Hengel, Anton},
  month = jun,
  year = {2015},
  keywords = {Accuracy,classification accuracy,CNN features,computing power,conditional random fields,Convolutional networks,CRF,deep-convolutional neural networks,dense-image patches,edge detection,feature extraction,hand-crafted features,image classification,Image edge detection,image region smoothing,ISPRS 2D semantic labelling challenge dataset,Labeling,neural nets,per-pixel class probabilities,probability,semantic pixel labelling,Semantics,smoothing methods,Training,training data,Visualization},
  pages = {36-43},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/5WKQRPFF/7301381.html}
}

@inproceedings{penatti_deep_2015,
  title = {Do Deep Features Generalize from Everyday Objects to Remote Sensing and Aerial Scenes Domains?},
  doi = {10.1109/CVPRW.2015.7301382},
  abstract = {In this paper, we evaluate the generalization power of deep features (ConvNets) in two new scenarios: aerial and remote sensing image classification. We evaluate experimentally ConvNets trained for recognizing everyday objects for the classification of aerial and remote sensing images. ConvNets obtained the best results for aerial images, while for remote sensing, they performed well but were outperformed by low-level color descriptors, such as BIC. We also present a correlation analysis, showing the potential for combining/fusing different ConvNets with other descriptors or even for combining multiple ConvNets. A preliminary set of experiments fusing ConvNets obtains state-of-the-art results for the well-known UCMerced dataset.},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Penatti, Ot{\'a}vio A. B. Penatti Ot{\'a}vio A. B. and Nogueira, Keiller and Santos, Jefersson A.},
  month = jun,
  year = {2015},
  keywords = {Accuracy,aerial images,aerial scenes domains,BIC,Correlation,correlation analysis,correlation methods,deep features,feature extraction,generalization power,geophysical image processing,Histograms,image classification,Image color analysis,image colour analysis,low-level color descriptors,multiple ConvNets,object recognition,remote sensing,remote sensing image classification,UCMerced dataset,Visualization},
  pages = {44-51},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/AHDUQXD8/7301382.html}
}

@inproceedings{lagrange_benchmarking_2015,
  title = {Benchmarking Classification of Earth-Observation Data: {{From}} Learning Explicit Features to Convolutional Networks},
  shorttitle = {Benchmarking Classification of Earth-Observation Data},
  doi = {10.1109/IGARSS.2015.7326745},
  abstract = {In this paper, we address the task of semantic labeling of multisource earth-observation (EO) data. Precisely, we benchmark several concurrent methods of the last 15 years, from expert classifiers, spectral support-vector classification and high-level features to deep neural networks. We establish that (1) combining multisensor features is essential for retrieving some specific classes, (2) in the image domain, deep convolutional networks obtain significantly better overall performances and (3) transfer of learning from large generic-purpose image sets is highly effective to build EO data classifiers.},
  booktitle = {2015 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}} ({{IGARSS}})},
  author = {Lagrange, Adrien and Le Saux, Bertrand and Beaup{\`e}re, Anne and Boulch, Alexandre and {Chan-Hon-Tong}, Adrien and Herbin, St{\'e}phane and Randrianarivo, Hicham and Ferecatu, Marin},
  month = jul,
  year = {2015},
  keywords = {Buildings,deep convolutional networks,deep neural networks,expert classifiers,feature extraction,generic-purpose image sets,geophysical image processing,high-level features,image classification,image domain,Laser radar,learning explicit features,multisensor features,multisource earth-observation data benchmarking classification,neural nets,Neural networks,Pattern analysis,remote sensing,semantic labeling,Semantics,spectral support-vector classification,support vector machines,terrain mapping},
  pages = {4173-4176},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WXE5EVSM/7326745.html}
}

@inproceedings{noh_learning_2015,
  title = {Learning {{Deconvolution Network}} for {{Semantic Segmentation}}},
  doi = {10.1109/ICCV.2015.178},
  abstract = {We propose a novel semantic segmentation algorithm by learning a deep deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixelwise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction, our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5\%) among the methods trained without using Microsoft COCO dataset through ensemble with the fully convolutional network.},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
  month = dec,
  year = {2015},
  keywords = {neural nets,convolutional neural network,learning (artificial intelligence),Visualization,convolution,Semantics,Shape,feature extraction,image segmentation,CNN,Deconvolution,prediction theory,semantic networks,deconvolution network learning,proposal-wise prediction,semantic segmentation algorithm,Image reconstruction},
  pages = {1520-1528},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/UMRAS2II/7410535.html}
}

@inproceedings{he_delving_2015,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human}}-{{Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  doi = {10.1109/ICCV.2015.123},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1\%, [26]) on this dataset.},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month = dec,
  year = {2015},
  keywords = {Adaptation models,Biological neural networks,Computational modeling,Gaussian distribution,human-level performance,ILSVRC 2014 winner,image classification,ImageNet 2012 classification dataset,ImageNet classification,model fitting,network architectures,neural nets,overfitting risk,parametric rectified linear unit,PReLU,rectified activation units,rectifier neural networks,rectifier nonlinearities,robust initialization method,state-of-the-art neural networks,Testing,Training},
  pages = {1026-1034},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/NXDQWID9/7410480.html}
}

@article{badrinarayanan_segnet_2017,
  title = {{{SegNet}}: {{A Deep Convolutional Encoder}}-{{Decoder Architecture}} for {{Scene Segmentation}}},
  volume = {39},
  issn = {0162-8828},
  shorttitle = {{{SegNet}}},
  doi = {10.1109/TPAMI.2016.2644615},
  abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.a- .uk/projects/segnet/.},
  number = {12},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  month = dec,
  year = {2017},
  keywords = {Computer architecture,Decoder,Decoding,Deep Convolutional Neural Networks,Encoder,image segmentation,Indoor Scenes,Neural networks,Pooling,Road Scenes,Roads,Semantic Pixel-Wise Segmentation,Semantics,Training,Upsampling},
  pages = {2481-2495},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9SZV73FK/7803544.html}
}

@article{goel_classification_2003,
  title = {Classification of Hyperspectral Data by Decision Trees and Artificial Neural Networks to Identify Weed Stress and Nitrogen Status of Corn},
  volume = {39},
  issn = {0168-1699},
  doi = {10.1016/S0168-1699(03)00020-6},
  abstract = {This study evaluates the potential of decision tree classification algorithms for the classification of hyperspectral data, with the goal of discriminating between different growth scenarios in a cornfield. A comparison was also made between decision tree and artificial neural networks (ANNs) classification accuracies. In the summer of the year 2000, a two-factor field experiment representing different crop conditions was carried out. Corn was grown under four weed management strategies: no weed control, control of grasses, control of broadleaf weeds, and full weed control with nitrogen levels of 60, 120, and 250 N kg/ha. Hyperspectral data using a Compact Airborne Spectrographic Imager were acquired three times during the entire growing season. Decision tree technology was applied to classify different treatments based on the hyperspectral data. Various tree-growing mechanisms were used to improve the accuracy of classification. Misclassification rates of detecting all the combinations of different nitrogen and weed categories were 43, 32, and 40\% for hyperspectral data sets obtained at the initial growth, the tasseling and the full maturity stages, respectively. However, satisfactory classification results were obtained when one factor (nitrogen or weed) was considered at a time. In this case, misclassification rates were only 22 and 18\% for nitrogen and weeds, respectively, for the data obtained at the tasseling stage. Slightly better results were obtained by following the ANN approach. However, the advantage with the decision tree was the formulation of simple and clear classification rules. The highest accuracy was obtained for the data acquired at tasseling stage. The results indicate the potential of decision tree classification algorithms and ANN usage in the classification of hyperspectral data for crop condition assessment.},
  number = {2},
  journal = {Computers and Electronics in Agriculture},
  author = {Goel, P. K and Prasher, S. O and Patel, R. M and Landry, J. A and Bonnell, R. B and Viau, A. A},
  month = may,
  year = {2003},
  keywords = {Artificial neural networks,classification,Classification,Corn,Decision tree,Hyperspectral,Nitrogen,remote sensing,Remote sensing,Weeds},
  pages = {67-93},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/R8QTFWNE/S0168169903000206.html}
}

@article{fauvel_advances_2013,
  title = {Advances in {{Spectral}}-{{Spatial Classification}} of {{Hyperspectral Images}}},
  volume = {101},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2012.2197589},
  abstract = {Recent advances in spectral-spatial classification of hyperspectral images are presented in this paper. Several techniques are investigated for combining both spatial and spectral information. Spatial information is extracted at the object (set of pixels) level rather than at the conventional pixel level. Mathematical morphology is first used to derive the morphological profile of the image, which includes characteristics about the size, orientation, and contrast of the spatial structures present in the image. Then, the morphological neighborhood is defined and used to derive additional features for classification. Classification is performed with support vector machines (SVMs) using the available spectral information and the extracted spatial information. Spatial postprocessing is next investigated to build more homogeneous and spatially consistent thematic maps. To that end, three presegmentation techniques are applied to define regions that are used to regularize the preliminary pixel-wise thematic map. Finally, a multiple-classifier (MC) system is defined to produce relevant markers that are exploited to segment the hyperspectral image with the minimum spanning forest algorithm. Experimental results conducted on three real hyperspectral images with different spatial and spectral resolutions and corresponding to various contexts are presented. They highlight the importance of spectral-spatial strategies for the accurate classification of hyperspectral images and validate the proposed methods.},
  number = {3},
  journal = {Proceedings of the IEEE},
  author = {Fauvel, M. and Tarabalka, Y. and Benediktsson, J. A. and Chanussot, J. and Tilton, J. C.},
  month = mar,
  year = {2013},
  keywords = {classification,Classification algorithms,conventional pixel level,feature extraction,geophysical image processing,homogeneous thematic maps,hyperspectral image,hyperspectral image segment,hyperspectral imaging,image classification,image morphological profile,Image resolution,image segmentation,Kernel,kernel methods,Mathematical morphology,morphological neighborhood,multiple-classifier system,Nearest neighbor searches,object level,pixel-wise thematic map,presegmentation techniques,remote sensing,segmentation,spanning forest algorithm,spatial information,spatial postprocessing,Spatial resolution,spatial structure contrast,spatial structure orientation,spatial structure size,spatially consistent thematic maps,Spectral analysis,spectral information,spectral-spatial classification,spectral–spatial classifier,support vector machines,vegetation mapping},
  pages = {652-675},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8XPE5X8G/Fauvel et al_2013_Advances in Spectral-Spatial Classification of Hyperspectral Images.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/D7SGGI9Z/6297992.html}
}

@article{chen_deep_2016,
  title = {Deep {{Feature Extraction}} and {{Classification}} of {{Hyperspectral Images Based}} on {{Convolutional Neural Networks}}},
  volume = {54},
  issn = {0196-2892},
  doi = {10.1109/TGRS.2016.2584107},
  abstract = {Due to the advantages of deep learning, in this paper, a regularized deep feature extraction (FE) method is presented for hyperspectral image (HSI) classification using a convolutional neural network (CNN). The proposed approach employs several convolutional and pooling layers to extract deep features from HSIs, which are nonlinear, discriminant, and invariant. These features are useful for image classification and target detection. Furthermore, in order to address the common issue of imbalance between high dimensionality and limited availability of training samples for the classification of HSI, a few strategies such as L2 regularization and dropout are investigated to avoid overfitting in class data modeling. More importantly, we propose a 3-D CNN-based FE model with combined regularization to extract effective spectral-spatial features of hyperspectral imagery. Finally, in order to further improve the performance, a virtual sample enhanced method is proposed. The proposed approaches are carried out on three widely used hyperspectral data sets: Indian Pines, University of Pavia, and Kennedy Space Center. The obtained results reveal that the proposed models with sparse constraints provide competitive results to state-of-the-art methods. In addition, the proposed deep FE opens a new window for further research.},
  number = {10},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  author = {Chen, Y. and Jiang, H. and Li, C. and Jia, X. and Ghamisi, P.},
  month = oct,
  year = {2016},
  keywords = {class data modeling,CNN-based FE model,convolutional neural network (CNN),convolutional neural networks,data mining,deep learning,feature extraction,feature extraction (FE),geophysical techniques,hyperspectral image (HSI) classification,hyperspectral images,hyperspectral imaging,Indian Pines,Iron,Kennedy Space Center,Machine learning,neural nets,pooling layers,regularized deep feature extraction,state-of-the-art methods,Training,University of Pavia},
  pages = {6232-6251},
  file = {/home/naudeber/Bibliographie//IEEE Transactions on Geoscience and Remote Sensing/2016/Chen et al 2016 - Deep Feature Extraction and Classification of Hyperspectral Images Based on.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/43SS934F/7514991.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/6ZGJ3ZHM/7514991.html}
}

@inproceedings{zheng_conditional_2015,
  title = {Conditional {{Random Fields}} as {{Recurrent Neural Networks}}},
  doi = {10.1109/ICCV.2015.179},
  abstract = {Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate Conditional Random Fields with Gaussian pairwise potentials and mean-field approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Zheng, S. and Jayasumana, S. and {Romera-Paredes}, B. and Vineet, V. and Su, Z. and Du, D. and Huang, C. and Torr, P. H. S.},
  month = dec,
  year = {2015},
  keywords = {back-propagation algorithm,backpropagation,CNN,computer vision,conditional random field,convolutional neural network,CRF,deep learning technique,Gaussian pairwise potential,Gaussian processes,Graphical models,image segmentation,image understanding,Labeling,Machine learning,mean-field approximate inference,neural nets,pixel-level labelling task,probabilistic graphical modelling,probability,random processes,recurrent neural network,semantic image segmentation,Semantics,Training},
  pages = {1529-1537},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VS4NUXVR/CRFasRNN.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/UERRSF27/7410536.html}
}

@inproceedings{zhao_pyramid_2017,
  title = {Pyramid {{Scene Parsing Network}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  year = {2017},
  pages = {2881-2890},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/PIQ3MQHG/Zhao_Pyramid_Scene_Parsing_CVPR_2017_paper.html}
}

@inproceedings{cordts_cityscapes_2016,
  title = {The {{Cityscapes Dataset}} for {{Semantic Urban Scene Understanding}}},
  doi = {10.1109/CVPR.2016.350},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Cordts, M. and Omran, M. and Ramos, S. and Rehfeld, T. and Enzweiler, M. and Benenson, R. and Franke, U. and Roth, S. and Schiele, B.},
  month = jun,
  year = {2016},
  keywords = {Benchmark testing,Cityscapes dataset,Complexity theory,computer vision,image sequences,object detection,semantic urban scene understanding,Semantics,stereo image processing,stereo video sequence,Training,Urban areas,vehicles,video signal processing,Visualization},
  pages = {3213-3223},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/QVHASJX3/7780719.html}
}

@inproceedings{maggiori_can_2017,
  title = {Can {{Semantic Labeling Methods Generalize}} to {{Any City}}? {{The Inria Aerial Image Labeling Benchmark}}},
  shorttitle = {Can {{Semantic Labeling Methods Generalize}} to {{Any City}}?},
  abstract = {New challenges in remote sensing impose the necessity of designing pixel classification methods that, once trained on a certain dataset, generalize to other areas of the earth. This may include regions where the appearance of the same type of objects is significantly different. In the literature it is common to use a single image and split it into training and test sets to train a classifier and assess its performance, respectively. However, this does not prove the generalization capabilities to other inputs. In this paper, we propose an aerial image labeling dataset that covers a wide range of urban settlement appearances, from different geographic locations. Moreover, the cities included in the test set are different from those of the training set. We also experiment with convolutional neural networks on our dataset.},
  language = {en},
  booktitle = {Proceedings of the {{IEEE International Symposium}} on {{Geoscience}} and {{Remote Sensing}} ({{IGARSS}})},
  author = {Maggiori, Emmanuel and Tarabalka, Yuliya and Charpiat, Guillaume and Alliez, Pierre},
  month = jul,
  year = {2017},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/UV49DI3R/Maggiori et al_2017_Can Semantic Labeling Methods Generalize to Any City.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/K7CHRHV4/hal-01468452.html}
}

@article{brostow_semantic_2009,
  series = {Video-based Object and Event Analysis},
  title = {Semantic Object Classes in Video: {{A}} High-Definition Ground Truth Database},
  volume = {30},
  issn = {0167-8655},
  shorttitle = {Semantic Object Classes in Video},
  doi = {10.1016/j.patrec.2008.04.005},
  number = {2},
  journal = {Pattern Recognition Letters},
  author = {Brostow, Gabriel J. and Fauqueur, Julien and Cipolla, Roberto},
  month = jan,
  year = {2009},
  keywords = {Label propagation,object recognition,semantic segmentation,Video database,Video understanding},
  pages = {88-97},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/I8FJM7EM/Brostow et al_2009_Semantic object classes in video.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FTC9XV9V/S0167865508001220.html}
}

@inproceedings{jegou_one_2017,
  title = {The {{One Hundred Layers Tiramisu}}: {{Fully Convolutional DenseNets}} for {{Semantic Segmentation}}},
  shorttitle = {The {{One Hundred Layers Tiramisu}}},
  doi = {10.1109/CVPRW.2017.156},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {J{\'e}gou, S. and Drozdzal, M. and Vazquez, D. and Romero, A. and Bengio, Y.},
  month = jul,
  year = {2017},
  keywords = {Benchmark testing,Computer architecture,convolution,convolutional DenseNets,convolutional neural networks,feature extraction,image classification,Image resolution,image segmentation,learning (artificial intelligence),semantic image segmentation,Semantics,Spatial resolution,Standards,Tiramisu layers,upsampling path training},
  pages = {1175-1183},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/MHA9Z9FV/8014890.html}
}

@inproceedings{deng_imagenet_2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  doi = {10.1109/CVPR.2009.5206848},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, J. and Dong, W. and Socher, R. and Li, L. J. and Li, Kai and {Fei-Fei}, Li},
  month = jun,
  year = {2009},
  keywords = {computer vision,Explosions,Image databases,Image resolution,image retrieval,ImageNet database,Information retrieval,Internet,large-scale hierarchical image database,large-scale ontology,Large-scale systems,multimedia computing,multimedia data,Multimedia databases,Ontologies,ontologies (artificial intelligence),Robustness,Spine,subtree,trees (mathematics),very large databases,visual databases,wordNet structure},
  pages = {248-255},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FFXN3NBD/5206848.html}
}

@incollection{ronneberger_u-net_2015,
  address = {Cham},
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  isbn = {978-3-319-24574-4},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at                                           http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net                                                          .},
  booktitle = {Medical {{Image Computing}} and {{Computer}}-{{Assisted Intervention}} \textendash{} {{MICCAI}} 2015: 18th {{International Conference}}, {{Munich}}, {{Germany}}, {{October}} 5-9, 2015, {{Proceedings}}, {{Part III}}},
  publisher = {{Springer International Publishing}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  pages = {234-241},
  doi = {10.1007/978-3-319-24574-4_28}
}

@article{l._c._chen_deeplab_2018,
  title = {{{DeepLab}}: {{Semantic Image Segmentation}} with {{Deep Convolutional Nets}}, {{Atrous Convolution}}, and {{Fully Connected CRFs}}},
  volume = {40},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2017.2699184},
  number = {4},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {{L. C. Chen} and {G. Papandreou} and {I. Kokkinos} and {K. Murphy} and {A. L. Yuille}},
  month = apr,
  year = {2018},
  keywords = {Context,Neural networks,Image resolution,Computational modeling,Semantics,Image segmentation,Atrous Convolution,Conditional Random Fields,Convolution,Convolutional Neural Networks,Semantic Segmentation},
  pages = {834-848}
}

@inproceedings{maas_rectifier_2013,
  title = {Rectifier Nonlinearities Improve Neural Network Acoustic Models},
  abstract = {Deep neural network acoustic models pro-duce substantial gains in large vocabu-lary continuous speech recognition systems. Emerging work with rectified linear (ReL) hidden units demonstrates additional gains in final system performance relative to more commonly used sigmoidal nonlinearities. In this work, we explore the use of deep rectifier networks as acoustic models for the 300 hour Switchboard conversational speech recogni-tion task. Using simple training procedures without pretraining, networks with rectifier nonlinearities produce 2 \% absolute reduc-tions in word error rates over their sigmoidal counterparts. We analyze hidden layer repre-sentations to quantify differences in how ReL units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further im-prove deep rectifier networks. 1.},
  booktitle = {{{ICML Workshop}} on {{Deep Learning}} for {{Audio}}, {{Speech}} and {{Language Processing}}},
  author = {Maas, Andrew L. and Hannun, Awni Y. and Ng, Andrew Y.},
  year = {2013},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/66LFT82M/relu_hybrid_icml2013_final.pdf}
}

@inproceedings{krizhevsky_imagenet_2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Proceedings of the {{Neural Information Processing Systems}} ({{NIPS}})},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  pages = {1097--1105},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/CPT7II8U/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.html}
}

@inproceedings{rouse_monitoring_1974,
  title = {Monitoring Vegetation Systems in the {{Great Plains}} with {{ERTS}}},
  abstract = {The Great Plains Corridor rangeland project utilizes natural vegetation systems as phenological indicators of seasonal development and climatic effects upon regional growth conditions. A method has been developed for quantitative measurement of vegetation conditions over broad regions using ERTS-1 MSS data. Radiance values recorded in ERTS-1 spectral bands 5 and 7, corrected for sun angle, are used to compute a band ratio parameter which is shown to be correlated with aboveground green biomass on rangelands.},
  author = {Rouse, J. W.},
  month = jan,
  year = {1974},
  keywords = {cattle,earth resources program,grains,grasslands,great plains corridor,multispectral photography,rangelands,vegetation},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FGXEKISM/Rouse - 1974 - Monitoring vegetation systems in the Great Plains .pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/D2KK3G7R/search.html}
}

@article{xie_new_2014,
  title = {New Hyperspectral Difference Water Index for the Extraction of Urban Water Bodies by the Use of Airborne Hyperspectral Images},
  volume = {8},
  issn = {1931-3195, 1931-3195},
  doi = {10.1117/1.JRS.8.085098},
  abstract = {Extracting surface land-cover types and analyzing changes are among the most common applications of remote sensing. One of the most basic tasks is to identify and map surface water boundaries. Spectral water indexes have been successfully used in the extraction of water bodies in multispectral images. However, directly applying a water index method to hyperspectral images disregards the abundant spectral information and involves difficulty in selecting appropriate spectral bands. It is also a challenge for a spectral water index to distinguish water from shadowed regions. The purpose of this study is therefore to develop an index that is suitable for water extraction by the use of hyperspectral images, and with the capability to mitigate the effects of shadow and low-albedo surfaces, especially in urban areas. Thus, we introduce a new hyperspectral difference water index (HDWI) to improve the water classification accuracy in areas that include shadow over water, shadow over other ground surfaces, and low-albedo ground surfaces. We tested the new method using PHI-2, HyMAP, and ROSIS hyperspectral images of Shanghai, Munich, and Pavia. The performance of the water index was compared with the normalized difference water index (NDWI) and the Mahalanobis distance classifier (MDC). With all three test images, the accuracy of HDWI was significantly higher than that of NDWI and MDC. Therefore, HDWI can be used for extracting water with a high degree of accuracy, especially in urban areas, where shadow caused by high buildings is an important source of classification error.},
  number = {1},
  journal = {Journal of Applied Remote Sensing},
  author = {Xie, Huan and Luo, Xin and Xu, Xiong and Tong, Xiaohua and Jin, Yanmin and Pan, Haiyan and Zhou, Bingzhong},
  month = jul,
  year = {2014},
  pages = {085098},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/LRCJ2SK9/Xie et al. - 2014 - New hyperspectral difference water index for the e.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/ERGIGNSZ/1.JRS.8.085098.html}
}

@article{breiman_random_2001,
  title = {Random {{Forests}}},
  volume = {45},
  issn = {0885-6125, 1573-0565},
  doi = {10.1023/A:1010933404324},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\textendash{}156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  language = {en},
  number = {1},
  journal = {Machine Learning},
  author = {Breiman, Leo},
  month = oct,
  year = {2001},
  pages = {5-32},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/DNCGLG9M/Breiman - 2001 - Random Forests.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/EVMJYKUX/A1010933404324.html}
}

@article{friedman_greedy_2001,
  title = {Greedy Function Approximation: {{A}} Gradient Boosting Machine.},
  volume = {29},
  issn = {0090-5364, 2168-8966},
  shorttitle = {Greedy Function Approximation},
  doi = {10.1214/aos/1013203451},
  abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent ``boosting'' paradigm is developed for additive expansions based on any fitting criterion.Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such ``TreeBoost'' models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
  language = {en},
  number = {5},
  journal = {The Annals of Statistics},
  author = {Friedman, Jerome H.},
  month = oct,
  year = {2001},
  keywords = {decision trees,boosting,Function estimation,robust nonparametric regression},
  pages = {1189-1232},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/R7ZESN7H/1013203451.html}
}

@inproceedings{bottou_large-scale_2010,
  title = {Large-Scale Machine Learning with Stochastic Gradient Descent},
  abstract = {Abstract. During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.},
  booktitle = {In {{COMPSTAT}}},
  author = {Bottou, L{\'e}on},
  year = {2010},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/KUDZJ6EU/Bottou - 2010 - Large-scale machine learning with stochastic gradi.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/IIFEA7IM/summary.html}
}

@article{cortes_support-vector_1995,
  title = {Support-Vector Networks},
  volume = {20},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00994018},
  abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
  language = {en},
  number = {3},
  journal = {Machine Learning},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  month = sep,
  year = {1995},
  pages = {273-297},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/DI8NHFDJ/Cortes et Vapnik - 1995 - Support-vector networks.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/MPIR7B89/10.html}
}

@inproceedings{boser_training_1992,
  address = {New York, NY, USA},
  series = {COLT '92},
  title = {A {{Training Algorithm}} for {{Optimal Margin Classifiers}}},
  isbn = {978-0-89791-497-0},
  doi = {10.1145/130385.130401},
  abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
  booktitle = {Proceedings of the {{Fifth Annual Workshop}} on {{Computational Learning Theory}}},
  publisher = {{ACM}},
  author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
  year = {1992},
  pages = {144--152},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VJXG6SYQ/Boser et al. - 1992 - A Training Algorithm for Optimal Margin Classifier.pdf}
}

@book{breiman_classification_2017,
  title = {Classification and Regression Trees},
  publisher = {{Routledge}},
  author = {Breiman, Leo},
  year = {2017},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9MV6GN35/9781351460491.html}
}

@inproceedings{dalal_histograms_2005,
  title = {Histograms of Oriented Gradients for Human Detection},
  volume = {1},
  doi = {10.1109/CVPR.2005.177},
  abstract = {We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
  booktitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  author = {Dalal, N. and Triggs, B.},
  month = jun,
  year = {2005},
  keywords = {Image edge detection,object detection,support vector machines,object recognition,Histograms,feature extraction,Robustness,Testing,Image databases,coarse spatial binning,contrast normalization,edge based descriptors,fine orientation binning,fine-scale gradients,gradient based descriptors,gradient methods,High performance computing,histograms of oriented gradients,human detection,Humans,linear SVM,Object detection,Object recognition,overlapping descriptor,pedestrian database,robust visual object recognition,Support vector machines},
  pages = {886-893 vol. 1},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/5DZC6F52/9411012.html}
}

@inproceedings{lowe_object_1999,
  title = {Object Recognition from Local Scale-Invariant Features},
  volume = {2},
  doi = {10.1109/ICCV.1999.790410},
  abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
  booktitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  author = {Lowe, D. G.},
  year = {1999},
  keywords = {Image recognition,object recognition,Computer science,feature extraction,Layout,computational geometry,Object recognition,3D projection,blurred image gradients,candidate object matches,cluttered partially occluded images,computation time,Electrical capacitance tomography,Filters,image matching,inferior temporal cortex,least squares approximations,Lighting,local geometric deformations,local image features,local scale-invariant features,low residual least squares solution,multiple orientation planes,nearest neighbor indexing method,Neurons,primate vision,Programmable logic arrays,Reactive power,robust object recognition,staged filtering approach,unknown model parameters},
  pages = {1150-1157 vol.2},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9MPEA4TN/790410.html}
}

@inproceedings{nogueira_learning_2016,
  title = {Learning to Semantically Segment High-Resolution Remote Sensing Images},
  doi = {10.1109/ICPR.2016.7900187},
  abstract = {Land cover classification is a task that requires methods capable of learning high-level features while dealing with high volume of data. Overcoming these challenges, Convolutional Networks (ConvNets) can learn specific and adaptable features depending on the data while, at the same time, learn classifiers. In this work, we propose a novel technique to automatically perform pixel-wise land cover classification. To the best of our knowledge, there is no other work in the literature that perform pixel-wise semantic segmentation based on data-driven feature descriptors for high-resolution remote sensing images. The main idea is to exploit the power of ConvNet feature representations to learn how to semantically segment remote sensing images. First, our method learns each label in a pixel-wise manner by taking into account the spatial context of each pixel. In a predicting phase, the probability of a pixel belonging to a class is also estimated according to its spatial context and the learned patterns. We conducted a systematic evaluation of the proposed algorithm using two remote sensing datasets with very distinct properties. Our results show that the proposed algorithm provides improvements when compared to traditional and state-of-the-art methods that ranges from 5 to 15\% in terms of accuracy.},
  booktitle = {2016 23rd {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Nogueira, K. and Mura, M. Dalla and Chanussot, J. and Schwartz, W. R. and dos Santos, J. A.},
  month = dec,
  year = {2016},
  keywords = {Context,neural nets,image representation,learning (artificial intelligence),Visualization,geophysical image processing,convolution,Machine learning,Semantics,feature extraction,image classification,image segmentation,remote sensing,land cover,Image segmentation,Feature extraction,Remote sensing,Semantic Segmentation,classifier learning,ConvNet feature representation,convolutional network,Deep Learning,feature descriptor,Feature Learning,High-resolution Images,high-resolution remote sensing image,image resolution,land cover classification,Land-cover Mapping,Pixel-wise Classification,pixel-wise semantic segmentation,Remote Sensing},
  pages = {3566-3571},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/GNPR43FA/7900187.html}
}

@article{santos_multiscale_2012,
  title = {Multiscale {{Classification}} of {{Remote Sensing Images}}},
  volume = {50},
  issn = {0196-2892},
  doi = {10.1109/TGRS.2012.2186582},
  abstract = {A huge effort has been applied in image classification to create high-quality thematic maps and to establish precise inventories about land cover use. The peculiarities of remote sensing images (RSIs) combined with the traditional image classification challenges made RSI classification a hard task. Our aim is to propose a kind of boost-classifier adapted to multiscale segmentation. We use the paradigm of boosting, whose principle is to combine weak classifiers to build an efficient global one. Each weak classifier is trained for one level of the segmentation and one region descriptor. We have proposed and tested weak classifiers based on linear support vector machines (SVM) and region distances provided by descriptors. The experiments were performed on a large image of coffee plantations. We have shown in this paper that our approach based on boosting can detect the scale and set of features best suited to a particular training set. We have also shown that hierarchical multiscale analysis is able to reduce training time and to produce a stronger classifier. We compare the proposed methods with a baseline based on SVM with radial basis function kernel. The results show that the proposed methods outperform the baseline.},
  number = {10},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  author = {dos Santos, J. A. and Gosselin, P. H. and {Philipp-Foliguet}, S. and Torres, R. da S. and Falao, A. X.},
  month = oct,
  year = {2012},
  keywords = {Vectors,support vector machines,Image color analysis,geophysical image processing,Histograms,terrain mapping,Training,image classification,image segmentation,remote sensing,Image segmentation,Feature extraction,Support vector machines,boost-classifier,Boosting,coffee plantations,hierarchical multiscale analysis,high-quality thematic maps,image descriptors,land cover use,linear support vector machines,multiscale classification,multiscale segmentation,radial basis function kernel,region descriptor,region distances,remote sensing image (RSI),remote sensing image multiscale classification,support vector machines (SVM),training time,weak classifier},
  pages = {3764-3775},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/KT46YPUT/6170888.html}
}

@inproceedings{girshick_rich_2014,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}}},
  doi = {10.1109/CVPR.2014.81},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012 \textendash{} achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Girshick, R. and Donahue, J. and Darrell, T. and Malik, J.},
  month = jun,
  year = {2014},
  keywords = {Vectors,neural nets,object detection,Proposals,Visualization,Training,image segmentation,semantic segmentation,Feature extraction,Object detection,Support vector machines,auxiliary task,bottom-up region proposal,canonical PASCAL VOC dataset,detection algorithm,domain-specific fine-tuning,high-capacity convolutional neural network,image features,labeled training data,low-level image feature,mAP,mean average precision,object detection performance,performance boost,R-CNN,rich feature hierarchy,segment objects,source code,supervised pretraining},
  pages = {580-587},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/QF6TL58Y/6909475.html}
}

@inproceedings{vargas_superpixel-based_2014,
  title = {Superpixel-{{Based Interactive Classification}} of {{Very High Resolution Images}}},
  doi = {10.1109/SIBGRAPI.2014.49},
  abstract = {Very high resolution (VHR) images are large datasets for pixel annotation \textendash{} a process that has depended on the supervised training of an effective pixel classifier. Active learning techniques have mitigated this problem, but pixel descriptors are limited to local image information and the large number of pixels makes the response time to the user's actions impractical, during active learning. To circumvent the problem, we present an active learning strategy that relies on superpixel descriptors and a priori dataset reduction. Firstly, we compare VHR image annotation using superpixel- and pixel-based classifiers, as designed by the same state-of-the-art active learning technique \textendash{} Multi-Class Level Uncertainty (MCLU). Even with the dataset reduction provided by the superpixel representation, MCLU remains unfeasible for user interaction. Therefore, we propose a technique to considerably reduce the superpixel dataset for active learning. Moreover, we subdivide the reduced dataset into a list of subsets with random sample rearrangement to gain both speed and sample diversity during the active learning process.},
  booktitle = {2014 27th {{SIBGRAPI Conference}} on {{Graphics}}, {{Patterns}} and {{Images}}},
  author = {Vargas, J. E. and Saito, P. T. M. and Falc{\~a}o, A. X. and d Rezende, P. J. and d Santos, J. A.},
  month = aug,
  year = {2014},
  keywords = {Accuracy,Image color analysis,image representation,learning (artificial intelligence),Histograms,Training,image classification,supervised classification,Support vector machines,image resolution,a priori dataset reduction,active learning,active learning techniques,Clustering algorithms,dataset reduction,interactive systems,local image information,MCLU,multiclass level uncertainty,pixel annotation,pixel classifier,superpixel dataset,superpixel descriptors,superpixel representation,superpixel-based interactive classification,supervised training,Uncertainty,user interaction,Very high resolution image processing,very high resolution images,VHR image annotation},
  pages = {173-179},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HKQN4H75/6915305.html}
}

@article{mountrakis_support_2011,
  title = {Support Vector Machines in Remote Sensing: {{A}} Review},
  volume = {66},
  issn = {0924-2716},
  shorttitle = {Support Vector Machines in Remote Sensing},
  doi = {10.1016/j.isprsjprs.2010.11.001},
  abstract = {A wide range of methods for analysis of airborne- and satellite-derived imagery continues to be proposed and assessed. In this paper, we review remote sensing implementations of support vector machines (SVMs), a promising machine learning methodology. This review is timely due to the exponentially increasing number of works published in recent years. SVMs are particularly appealing in the remote sensing field due to their ability to generalize well even with limited training samples, a common limitation for remote sensing applications. However, they also suffer from parameter assignment issues that can significantly affect obtained results. A summary of empirical results is provided for various applications of over one hundred published works (as of April, 2010). It is our hope that this survey will provide guidelines for future applications of SVMs and possible areas of algorithm enhancement.},
  number = {3},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  author = {Mountrakis, Giorgos and Im, Jungho and Ogole, Caesar},
  month = may,
  year = {2011},
  keywords = {SVM,Remote sensing,Support vector machines,Review,SVMs},
  pages = {247-259},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/CAKTDBGN/Mountrakis et al. - 2011 - Support vector machines in remote sensing A revie.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/AA6GW96W/S0924271610001140.html}
}

@incollection{bengio_practical_2012,
  series = {Lecture Notes in Computer Science},
  title = {Practical {{Recommendations}} for {{Gradient}}-{{Based Training}} of {{Deep Architectures}}},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyperparameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
  language = {en},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Bengio, Yoshua},
  year = {2012},
  pages = {437-478},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/MCFQUWFJ/Bengio - 2012 - Practical Recommendations for Gradient-Based Train.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/E38VEJWM/978-3-642-35289-8_26.html},
  doi = {10.1007/978-3-642-35289-8_26}
}

@article{benediktsson_advances_2013,
  title = {Advances in {{Very}}-{{High}}-{{Resolution Remote Sensing}} [{{Scanning}} the {{Issue}}]},
  volume = {101},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2012.2237076},
  abstract = {The articles in special issue focus on advancements in very high resolution remote sensing technologies and applications.},
  number = {3},
  journal = {Proceedings of the IEEE},
  author = {Benediktsson, J. A. and Chanussot, J. and Moon, W. M.},
  month = mar,
  year = {2013},
  keywords = {Machine learning,Remote sensing,Hyperspectral imaging,Microwave communication,Optical fiber communication,Signal processing,Special issues and sections,Synthetic aperture radar},
  pages = {566-569},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/7DWFSJXJ/6461961.html}
}

@book{goodfellow_deep_2016,
  title = {Deep {{Learning}}},
  publisher = {{MIT Press}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  note = {$\backslash$url\{http://www.deeplearningbook.org\}}
}

@article{le_saux_2018_2018,
  title = {2018 {{IEEE GRSS Data Fusion Contest}}: {{Multimodal Land Use Classification}} [{{Technical Committees}}]},
  volume = {6},
  issn = {2473-2397},
  shorttitle = {2018 {{IEEE GRSS Data Fusion Contest}}},
  doi = {10.1109/MGRS.2018.2798161},
  abstract = {Presents information on the 2018 IEEE GRSS Data Fusion Contest.},
  number = {1},
  journal = {IEEE Geoscience and Remote Sensing Magazine},
  author = {Le Saux, B and Yokoya, N. and Hansch, R. and Prasad, S.},
  month = mar,
  year = {2018},
  pages = {52-54},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/UTBE7ZSW/cookiedetectresponse.html}
}

@article{tuia_2017_2017,
  title = {The 2017 {{IEEE Geoscience}} and {{Remote Sensing Society Data Fusion Contest}}: {{Open Data}} for {{Global Multimodal Land Use Classification}} [{{Technical Committees}}]},
  volume = {5},
  issn = {2473-2397},
  shorttitle = {The 2017 {{IEEE Geoscience}} and {{Remote Sensing Society Data Fusion Contest}}},
  doi = {10.1109/MGRS.2017.2760346},
  abstract = {Presents information on the 2017 IEEE Geoscience and Remote Sensing Society Data Fusion Contest.},
  number = {4},
  journal = {IEEE Geoscience and Remote Sensing Magazine},
  author = {Tuia, D. and Moser, G. and Saux, B. Le and Bechtel, B. and See, L.},
  month = dec,
  year = {2017},
  pages = {110-114},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/S5KYRQTF/cookiedetectresponse.html}
}

@article{marquez-neila_morphological_2014,
  title = {A {{Morphological Approach}} to {{Curvature}}-{{Based Evolution}} of {{Curves}} and {{Surfaces}}},
  volume = {36},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2013.106},
  abstract = {We introduce new results connecting differential and morphological operators that provide a formal and theoretically grounded approach for stable and fast contour evolution. Contour evolution algorithms have been extensively used for boundary detection and tracking in computer vision. The standard solution based on partial differential equations and level-sets requires the use of numerical methods of integration that are costly computationally and may have stability issues. We present a morphological approach to contour evolution based on a new curvature morphological operator valid for surfaces of any dimension. We approximate the numerical solution of the curve evolution PDE by the successive application of a set of morphological operators defined on a binary level-set and with equivalent infinitesimal behavior. These operators are very fast, do not suffer numerical stability issues, and do not degrade the level set function, so there is no need to reinitialize it. Moreover, their implementation is much easier since they do not require the use of sophisticated numerical algorithms. We validate the approach providing a morphological implementation of the geodesic active contours, the active contours without borders, and turbopixels. In the experiments conducted, the morphological implementations converge to solutions equivalent to those achieved by traditional numerical solutions, but with significant gains in simplicity, speed, and stability.},
  number = {1},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {{M{\'a}rquez-Neila}, P. and Baumela, L. and Alvarez, L.},
  month = jan,
  year = {2014},
  keywords = {computer vision,Three-dimensional displays,Computer vision,Surface morphology,Active contours,Approximation methods,boundary detection,curvature based evolution,curvature morphological operator,curve evolution,fast contour evolution algorithms,geodesic active contours,Level set,level set function,level-sets,Mathematical model,mathematical morphology,morphological snakes,numerical analysis,numerical solutions,numerical stability,Numerical stability,partial differential equations,sophisticated numerical algorithms,stability,tracking,turbopixels},
  pages = {2-17},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/TUT83J8I/6529072.html}
}

@article{bengio_representation_2013,
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  volume = {35},
  issn = {0162-8828},
  shorttitle = {Representation {{Learning}}},
  doi = {10.1109/TPAMI.2013.50},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
  number = {8},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Bengio, Y. and Courville, A. and Vincent, P.},
  month = aug,
  year = {2013},
  keywords = {neural nets,probability,Neural networks,Machine learning,feature learning,Learning systems,autoencoders,Feature extraction,Deep learning,unsupervised learning,Humans,Speech recognition,Abstracts,AI,Algorithms,artificial intelligence,Artificial Intelligence,autoencoder,Boltzmann machine,data representation,data structures,density estimation,geometrical connections,machine learning algorithms,manifold learning,Manifolds,Neural Networks (Computer),probabilistic models,representation learning,unsupervised feature learning},
  pages = {1798-1828},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VVVC9RRN/6472238.html}
}

@article{dechesne_semantic_2017,
  title = {Semantic Segmentation of Forest Stands of Pure Species Combining Airborne Lidar Data and Very High Resolution Multispectral Imagery},
  volume = {126},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2017.02.011},
  abstract = {Forest stands are the basic units for forest inventory and mapping. Stands are defined as large forested areas (e.g., $\geqslant$2ha) of homogeneous tree species composition and age. Their accurate delineation is usually performed by human operators through visual analysis of very high resolution (VHR) infra-red images. This task is tedious, highly time consuming, and should be automated for scalability and efficient updating purposes. In this paper, a method based on the fusion of airborne lidar data and VHR multispectral images is proposed for the automatic delineation of forest stands containing one dominant species (purity superior to 75\%). This is the key preliminary task for forest land-cover database update. The multispectral images give information about the tree species whereas 3D lidar point clouds provide geometric information on the trees and allow their individual extraction. Multi-modal features are computed, both at pixel and object levels: the objects are individual trees extracted from lidar data. A supervised classification is then performed at the object level in order to coarsely discriminate the existing tree species in each area of interest. The classification results are further processed to obtain homogeneous areas with smooth borders by employing an energy minimum framework, where additional constraints are joined to form the energy function. The experimental results show that the proposed method provides very satisfactory results both in terms of stand labeling and delineation (overall accuracy ranges between 84\% and 99\%).},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  author = {Dechesne, Cl{\'e}ment and Mallet, Cl{\'e}ment and Le Bris, Arnaud and {Gouet-Brunet}, Val{\'e}rie},
  month = apr,
  year = {2017},
  keywords = {Feature selection,Lidar,Energy minimization,Forest stand delineation,Fusion,Multispectral imagery,Regularisation,Supervised classification,Tree species},
  pages = {129-145},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WPIRC3P4/Dechesne et al. - 2017 - Semantic segmentation of forest stands of pure spe.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/E4W5V7GP/S0924271616302763.html}
}

@article{guo_relevance_2011,
  title = {Relevance of Airborne Lidar and Multispectral Image Data for Urban Scene Classification Using {{Random Forests}}},
  volume = {66},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2010.08.007},
  abstract = {Airborne lidar systems have become a source for the acquisition of elevation data. They provide georeferenced, irregularly distributed 3D point clouds of high altimetric accuracy. Moreover, these systems can provide for a single laser pulse, multiple returns or echoes, which correspond to different illuminated objects. In addition to multi-echo laser scanners, full-waveform systems are able to record 1D signals representing a train of echoes caused by reflections at different targets. These systems provide more information about the structure and the physical characteristics of the targets. Many approaches have been developed, for urban mapping, based on aerial lidar solely or combined with multispectral image data. However, they have not assessed the importance of input features. In this paper, we focus on a multi-source framework using aerial lidar (multi-echo and full waveform) and aerial multispectral image data. We aim to study the feature relevance for dense urban scenes. The Random Forests algorithm is chosen as a classifier: it runs efficiently on large datasets, and provides measures of feature importance for each class. The margin theory is used as a confidence measure of the classifier, and to confirm the relevance of input features for urban classification. The quantitative results confirm the importance of the joint use of optical multispectral and lidar data. Moreover, the relevance of full-waveform lidar features is demonstrated for building and vegetation area discrimination.},
  number = {1},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  author = {Guo, Li and Chehata, Nesrine and Mallet, Cl{\'e}ment and Boukir, Samia},
  month = jan,
  year = {2011},
  keywords = {Lidar,Multispectral image,Random forests,Urban,Variable importance},
  pages = {56-66},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/XXRR6Q59/Guo et al. - 2011 - Relevance of airborne lidar and multispectral imag.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WJWZ7ZUQ/S0924271610000705.html}
}

@article{ham_investigation_2005,
  title = {Investigation of the Random Forest Framework for Classification of Hyperspectral Data},
  volume = {43},
  issn = {0196-2892},
  doi = {10.1109/TGRS.2004.842481},
  abstract = {Statistical classification of byperspectral data is challenging because the inputs are high in dimension and represent multiple classes that are sometimes quite mixed, while the amount and quality of ground truth in the form of labeled data is typically limited. The resulting classifiers are often unstable and have poor generalization. This work investigates two approaches based on the concept of random forests of classifiers implemented within a binary hierarchical multiclassifier system, with the goal of achieving improved generalization of the classifier in analysis of hyperspectral data, particularly when the quantity of training data is limited. A new classifier is proposed that incorporates bagging of training samples and adaptive random subspace feature selection within a binary hierarchical classifier (BHC), such that the number of features that is selected at each node of the tree is dependent on the quantity of associated training data. Results are compared to a random forest implementation based on the framework of classification and regression trees. For both methods, classification results obtained from experiments on data acquired by the National Aeronautics and Space Administration (NASA) Airborne Visible/Infrared Imaging Spectrometer instrument over the Kennedy Space Center, Florida, and by Hyperion on the NASA Earth Observing 1 satellite over the Okavango Delta of Botswana are superior to those from the original best basis BHC algorithm and a random subspace extension of the BHC.},
  number = {3},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  author = {Ham, J. and Chen, Yangchi and Crawford, M. M. and Ghosh, J.},
  month = mar,
  year = {2005},
  keywords = {trees (mathematics),hyperspectral data classification,classification tree,random processes,feature extraction,image classification,vegetation mapping,hyperspectral data analysis,Kennedy Space Center,geophysical signal processing,Infrared imaging,Infrared spectra,Spectroscopy,Airborne Visible/Infrared Imaging Spectrometer,Hyperspectral imaging,data acquisition,forestry,adaptive random subspace feature selection,Bagging,binary hierarchical classifier,binary hierarchical multiclassifier system,Botswana,Classification tree analysis,Data analysis,Florida,Hyperion,multidimensional signal processing,NASA,NASA Earth Observing 1 satellite,Okavango Delta,random forest,random subspace extension,regression tree,Regression tree analysis,statistical classification,Training data,tree node,USA},
  pages = {492-501},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/YD633IEG/1396322.html}
}

@article{li_review_2014,
  title = {A {{Review}} of {{Remote Sensing Image Classification Techniques}}: The {{Role}} of {{Spatio}}-Contextual {{Information}}},
  volume = {47},
  issn = {2279-7254},
  shorttitle = {A {{Review}} of {{Remote Sensing Image Classification Techniques}}},
  doi = {10.5721/EuJRS20144723},
  language = {en},
  number = {1},
  journal = {European Journal of Remote Sensing},
  author = {Li, Miao and Zang, Shuying and Zhang, Bing and Li, Shanshan and Wu, Changshan},
  month = jan,
  year = {2014},
  pages = {389-411},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/UV78NXV3/A Review of Remote Sensing Image Classification Techniques the Role of Spatio contextual Information.pdf}
}

@book{rosenblatt_perceptron_1957,
  title = {The {{Perceptron}}: {{A Probabilistic Model}} for {{Information Storage}} and {{Organization In The Brain}}},
  shorttitle = {The {{Perceptron}}},
  abstract = {If we are eventually to understand the capability of higher organisms for perceptual recognition, generalization, recall, and thinking, we must first have answers to three fundamental questions: 1. How is information about the physical world sensed, or detected, by the biological system? 2. In what form is information stored, or remembered? 3. How does information contained in storage, or in memory, influence recognition and behavior? The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus 1 The development of this theory has been carried out at the Cornell Aeronautical Laboratory, Inc., under the sponsorship of the Office of Naval Research, Contract Nonr-2381(00). This article is primarily'an adaptation of material reported in Ref. IS, which constitutes the first full report on the program.},
  author = {Rosenblatt, Frank},
  year = {1957},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/GXFF5DH9/Brain et Rosenblatt - The Perceptron A Probabilistic Model for Informat.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/LP2KSK8W/14849-66902-1-PB.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/F2JJY9W3/summary.html}
}

@incollection{bengio_greedy_2007,
  title = {Greedy {{Layer}}-{{Wise Training}} of {{Deep Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 19},
  publisher = {{MIT Press}},
  author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  editor = {Sch{\"o}lkopf, B. and Platt, J. C. and Hoffman, T.},
  year = {2007},
  pages = {153--160},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/PPSXPXV3/Bengio et al. - 2007 - Greedy Layer-Wise Training of Deep Networks.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/68AJKV2Q/3048-greedy-layer-wise-training-of-deep-networks.html}
}

@article{fukushima_neocognitron_1980,
  title = {Neocognitron: {{A}} Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  volume = {36},
  issn = {0340-1200, 1432-0770},
  shorttitle = {Neocognitron},
  doi = {10.1007/BF00344251},
  abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by ``learning without a teacher'', and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname ``neocognitron''. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of ``S-cells'', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of ``C-cells'' similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any ``teacher'' during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
  language = {en},
  number = {4},
  journal = {Biological Cybernetics},
  author = {Fukushima, Kunihiko},
  month = apr,
  year = {1980},
  pages = {193-202},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/TC5ATREG/Fukushima - 1980 - Neocognitron A self-organizing neural network mod.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/UI4D83BI/BF00344251.html}
}

@inproceedings{nair_rectified_2010,
  title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning ({{ICML}}-10)},
  author = {Nair, Vinod and Hinton, Geoffrey E.},
  year = {2010},
  pages = {807--814},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/U4T6HPKD/reluICML.pdf}
}

@article{klein_second-harmonic_2006,
  title = {Second-{{Harmonic Generation}} from {{Magnetic Metamaterials}}},
  volume = {313},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1129198},
  language = {en},
  number = {5786},
  journal = {Science},
  author = {Klein, M. W.},
  month = jul,
  year = {2006},
  pages = {502-504},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/Z8S7XIFA/science.pdf}
}

@article{bengio_learning_2009,
  title = {Learning Deep Architectures for {{AI}}},
  volume = {2},
  number = {1},
  journal = {Foundations and trends\textregistered{} in Machine Learning},
  author = {Bengio, Yoshua},
  year = {2009},
  pages = {1--127},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/65AS4KWA/TR1312.pdf}
}

@book{turing_computing_1950,
  title = {Computing Machinery and Intelligence},
  author = {Turing, A. M.},
  year = {1950},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/K56NIUPT/turing.pdf}
}

@inproceedings{glorot_deep_2011,
  title = {Deep {{Sparse Rectifier Neural Networks}}},
  abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neu...},
  language = {en},
  booktitle = {Proceedings of the {{Fourteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  month = jun,
  year = {2011},
  pages = {315-323},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HA4QKRXE/Glorot et al. - 2011 - Deep Sparse Rectifier Neural Networks.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/KGVEV8GK/glorot11a.html}
}

@book{hebb_organization_1949,
  title = {The {{Organization}} of {{Behavior}}},
  language = {eng},
  author = {Hebb, Donald O.},
  year = {1949},
  keywords = {Animals,Behavior; Animal,Cognitive Science,History; 20th Century,Neurosciences,Publishing},
  pmid = {10643472}
}

@article{mcculloch_logical_1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  volume = {5},
  journal = {Bulletin of Mathematical Biophysics},
  author = {McCulloch, Warren S. and Pitts, Walter H.},
  year = {1943},
  pages = {115-133},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/A8VXF2EC/mcp.pdf}
}

@article{kleene_representation_1956,
  title = {Representation of {{Events}} in {{Nerve Nets}} and {{Finite Automata}}},
  journal = {Automata Studies},
  author = {Kleene, S. C.},
  year = {1956},
  pages = {3-42},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/T8R2QULB/RM704.pdf}
}

@incollection{rumelhart_learning_1986,
  address = {Cambridge, MA, USA},
  title = {Learning Internal Representations by Error Propagation},
  isbn = {978-0-262-68053-0},
  shorttitle = {Parallel {{Distributed Processing}}},
  publisher = {{MIT Press}},
  author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
  editor = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
  year = {1986},
  pages = {318--362}
}

@book{minsky_perceptrons_1969,
  title = {Perceptrons},
  abstract = {It is the author's view that although the time is not yet ripe for developing a really general theory of automata and computation, it is now possible and desirable to move more explicitly in this direction. This can be done by studying in an extremely thorough way well-chosen particular situations that embody the basic concepts. This is the aim of the present book, which seeks general results from the close study of abstract versions of devices known as perceptrons.A perceptron is a parallel computer containing a number of readers that scan a field independently and simultaneously, and it makes decisions by linearly combining the local and partial data gathered, weighing the evidence, and deciding if events fit a given ``pattern,'' abstract or geometric. The rigorous and systematic study of the perceptron undertaken here convincingly demonstrates the authors' contention that there is both a real need for a more basic understanding of computation and little hope of imposing one from the top, as opposed to working up such an understanding from the detailed consideration of a limited but important class of concepts, such as those underlying perceptron operations. ``Computer science,'' the authors suggest, is beginning to learn more and more just how little it really knows. Not only does science not know much about how brains compute thoughts or how the genetic code computes organisms, it also has no very good idea about how computers compute, in terms of such basic principles as how much computation a problem of what degree of complexity is most suitable to deal with it. Even the language in which the questions are formulated is imprecise, including for example the exact nature of the opposition or complementarity implicit in the distinction ``analogue'' vs. ``digital,'' ``local'' vs. ``global,'' ``parallel'' vs. ``serial,'' ``addressed'' vs. ``associative.'' Minsky and Papert strive to bring these concepts into a sharper focus insofar as they apply to the perceptron. They also question past work in the field, which too facilely assumed that perceptronlike devices would, automatically almost, evolve into universal ``pattern recognizing,'' ``learning,'' or ``self-organizing'' machines. The work recognizes fully the inherent impracticalities, and proves certain impossibilities, in various system configurations. At the same time, the real and lively prospects for future advance are accentuated.The book divides in a natural way into three parts -- the first part is ``algebraic'' in character, since it considers the general properties of linear predicate families which apply to all perceptrons, independently of the kinds of patterns involved; the second part is ``geometric'' in that it looks more narrowly at various interesting geometric patterns and derives theorems that are sharper than those of Part One, if thereby less general; and finally the third part views perceptrons as practical devices, and considers the general questions of pattern recognition and learning by artificial systems.},
  language = {en},
  publisher = {{MIT Press}},
  author = {Minsky, Marvin and Papert, Seymour A.},
  year = {1969},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/W4V4U4AX/perceptrons.html}
}

@phdthesis{werbos_beyond_1975,
  title = {Beyond {{Regression}}: {{New Tools}} for {{Prediction}} and {{Analysis}} in the {{Behavioral Sciences}}},
  shorttitle = {Beyond {{Regression}}},
  language = {en},
  school = {Harvard University},
  author = {Werbos, Paul John},
  year = {1975}
}

@incollection{lecun_learning_1986,
  series = {NATO ASI Series},
  title = {Learning {{Process}} in an {{Asymmetric Threshold Network}}},
  isbn = {978-3-642-82659-7 978-3-642-82657-3},
  abstract = {Threshold functions and related operators are widely used as basic elements of adaptive and associative networks [Nakano 72, Amari 72, Hopfield 82]. There exist numerous learning rules for finding a set of weights to achieve a particular correspondence between input-output pairs. But early works in the field have shown that the number of threshold functions (or linearly separable functions) in N binary variables is small compared to the number of all possible boolean mappings in N variables, especially if N is large. This problem is one of the main limitations of most neural networks models where the state is fully specified by the environment during learning: they can only learn linearly separable functions of their inputs. Moreover, a learning procedure which requires the outside world to specify the state of every neuron during the learning session can hardly be considered as a general learning rule because in real-world conditions, only a partial information on the ``ideal'' network state for each task is available from the environment. It is possible to use a set of so-called ``hidden units'' [Hinton,Sejnowski,Ackley. 84], without direct interaction with the environment, which can compute intermediate predicates. Unfortunately, the global response depends on the output of a particular hidden unit in a highly non-linear way, moreover the nature of this dependence is influenced by the states of the other cells.},
  language = {en},
  booktitle = {Disordered {{Systems}} and {{Biological Organization}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {LeCun, Yann},
  year = {1986},
  pages = {233-240},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WWMLA557/978-3-642-82657-3_24.html},
  doi = {10.1007/978-3-642-82657-3_24}
}

@article{hornik_approximation_1991,
  title = {Approximation Capabilities of Multilayer Feedforward Networks},
  volume = {4},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(91)90009-T},
  abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp($\mu$) performance criteria, for arbitrary finite input environment measures $\mu$, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
  number = {2},
  journal = {Neural Networks},
  author = {Hornik, Kurt},
  month = jan,
  year = {1991},
  keywords = {() approximation,Activation function,Input environment measure,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities},
  pages = {251-257},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/38AI5HHM/Hornik - 1991 - Approximation capabilities of multilayer feedforwa.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/F72Z6USA/089360809190009T.html}
}

@article{cybenko_approximation_1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  volume = {2},
  issn = {0932-4194, 1435-568X},
  doi = {10.1007/BF02551274},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  language = {en},
  number = {4},
  journal = {Mathematics of Control, Signals and Systems},
  author = {Cybenko, G.},
  month = dec,
  year = {1989},
  pages = {303-314},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/GK4G5ZRI/Cybenko - 1989 - Approximation by superpositions of a sigmoidal fun.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/RDR7DZHS/BF02551274.html}
}

@article{hubel_receptive_1959,
  title = {Receptive Fields of Single Neurones in the Cat's Striate Cortex},
  volume = {148},
  issn = {0022-3751},
  number = {3},
  journal = {The Journal of Physiology},
  author = {Hubel, D. H. and Wiesel, T. N.},
  month = oct,
  year = {1959},
  pages = {574-591},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/XHQFL544/Hubel et Wiesel - 1959 - Receptive fields of single neurones in the cat's s.pdf},
  pmid = {14403679},
  pmcid = {PMC1363130}
}

@article{hubel_receptive_1968,
  title = {Receptive Fields and Functional Architecture of Monkey Striate Cortex},
  volume = {195},
  issn = {0022-3751},
  abstract = {1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded.2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent.3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other.4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two-thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and VI contain mostly complex and hypercomplex cells, binocularly driven.5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer IV, the higher orders in the upper and lower layers.},
  language = {eng},
  number = {1},
  journal = {The Journal of Physiology},
  author = {Hubel, D. H. and Wiesel, T. N.},
  month = mar,
  year = {1968},
  keywords = {Animals,Color Perception,Evoked Potentials,Haplorhini,Light,Motion Perception,Occipital Lobe,Retina,Vision; Ocular,Visual Fields},
  pages = {215-243},
  pmid = {4966457},
  pmcid = {PMC1557912}
}

@inproceedings{glorot_understanding_2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental resul...},
  language = {en},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  month = mar,
  year = {2010},
  pages = {249-256},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9Z8Z4TUV/glorot10a.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HSHJ3227/Glorot et Bengio - 2010 - Understanding the difficulty of training deep feed.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/NBNQZI9J/glorot10a.html}
}

@article{lecun_backpropagation_1989,
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  volume = {1},
  issn = {0899-7667},
  doi = {10.1162/neco.1989.1.4.541},
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  number = {4},
  journal = {Neural Computation},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  month = dec,
  year = {1989},
  pages = {541-551},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8A9ZNCQK/6795724.html}
}

@article{ackley_learning_1985,
  title = {A Learning Algorithm for Boltzmann Machines},
  volume = {9},
  issn = {0364-0213},
  doi = {10.1016/S0364-0213(85)80012-4},
  abstract = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.},
  number = {1},
  journal = {Cognitive Science},
  author = {Ackley, David H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
  month = jan,
  year = {1985},
  pages = {147-169},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/R78GWUDI/Ackley et al. - 1985 - A learning algorithm for boltzmann machines.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/EPPCYCJD/S0364021385800124.html}
}

@article{hinton_fast_2006,
  title = {A {{Fast Learning Algorithm}} for {{Deep Belief Nets}}},
  volume = {18},
  issn = {0899-7667},
  doi = {10.1162/neco.2006.18.7.1527},
  abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
  number = {7},
  journal = {Neural Comput.},
  author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
  month = jul,
  year = {2006},
  pages = {1527--1554}
}

@article{hinton_reducing_2006,
  title = {Reducing the Dimensionality of Data with Neural Networks},
  volume = {313},
  issn = {1095-9203},
  doi = {10.1126/science.1127647},
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  language = {eng},
  number = {5786},
  journal = {Science (New York, N.Y.)},
  author = {Hinton, G. E. and Salakhutdinov, R. R.},
  month = jul,
  year = {2006},
  pages = {504-507},
  pmid = {16873662}
}

@inproceedings{salakhutdinov_deep_2009,
  title = {Deep {{Boltzmann Machines}}},
  abstract = {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to fo...},
  language = {en},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
  month = apr,
  year = {2009},
  pages = {448-455},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/94955MT6/Salakhutdinov et Hinton - 2009 - Deep Boltzmann Machines.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/AUEH38MG/salakhutdinov09a.html}
}

@inproceedings{lecun_learning_2004,
  title = {Learning Methods for Generic Object Recognition with Invariance to Pose and Lighting},
  volume = {2},
  doi = {10.1109/CVPR.2004.1315150},
  abstract = {We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13\% for SVM and 7\% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7\% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.},
  booktitle = {Proceedings of the 2004 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}, 2004. {{CVPR}} 2004.},
  author = {LeCun, Y. and Huang, Fu Jie and Bottou, L.},
  month = jun,
  year = {2004},
  keywords = {object detection,support vector machines,principal component analysis,computer vision,Airplanes,feature extraction,image segmentation,Testing,Support vector machine classification,Learning systems,visual databases,stereo image processing,very large databases,Humans,Object recognition,Support vector machines,Animals,Azimuth,convolutional networks,generic object recognition,Gray-scale,learning methods,lighting invariance,low-resolution grayscale images,nearest neighbor methods,pose invariance,test error rates},
  pages = {II-97-104 Vol.2},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/TQED6IXB/1315150.html}
}

@inproceedings{serre_object_2005,
  title = {Object Recognition with Features Inspired by Visual Cortex},
  volume = {2},
  doi = {10.1109/CVPR.2005.254},
  abstract = {We introduce a novel set of features for robust object recognition. Each element of this set is a complex feature obtained by combining position- and scale-tolerant edge-detectors over neighboring positions and multiple orientations. Our system's architecture is motivated by a quantitative model of visual cortex. We show that our approach exhibits excellent recognition performance and outperforms several state-of-the-art systems on a variety of image datasets including many different object categories. We also demonstrate that our system is able to learn from very few examples. The performance of the approach constitutes a suggestive plausibility proof for a class of feedforward models of object recognition in cortex.},
  booktitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  author = {Serre, T. and Wolf, L. and Poggio, T.},
  month = jun,
  year = {2005},
  keywords = {Image recognition,object recognition,edge detection,Shape,feature extraction,Robustness,Geometry,Object detection,Object recognition,Biology computing,Brain modeling,Face detection,image dataset,position-tolerant edge detector,scale-tolerant edge detector,Target recognition,visual cortex},
  pages = {994-1000 vol. 2},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/U6I5F7QE/1467551.html}
}

@inproceedings{huang_large-scale_2006,
  title = {Large-Scale {{Learning}} with {{SVM}} and {{Convolutional}} for {{Generic Object Categorization}}},
  volume = {1},
  doi = {10.1109/CVPR.2006.164},
  abstract = {The detection and recognition of generic object categories with invariance to viewpoint, illumination, and clutter requires the combination of a feature extractor and a classifier. We show that architectures such as convolutional networks are good at learning invariant features, but not always optimal for classification, while Support Vector Machines are good at producing decision surfaces from wellbehaved feature vectors, but cannot learn complicated invariances. We present a hybrid system where a convolutional network is trained to detect and recognize generic objects, and a Gaussian-kernel SVM is trained from the features learned by the convolutional network. Results are given on a large generic object recognition task with six categories (human figures, four-legged animals, airplanes, trucks, cars, and "none of the above"), with multiple instances of each object category under various poses, illuminations, and backgrounds. On the test set, which contains different object instances than the training set, an SVM alone yields a 43.3\% error rate, a convolutional net alone yields 7.2\% and an SVM on top of features produced by the convolutional net yields 5.9\%.},
  booktitle = {2006 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'06)},
  author = {Huang, Fu Jie and LeCun, Y.},
  month = jun,
  year = {2006},
  keywords = {Machine learning,Gaussian processes,Support vector machine classification,Large-scale systems,Feature extraction,Humans,Object detection,Object recognition,Support vector machines,Lighting},
  pages = {284-291},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/7XIXPLDS/icp.html}
}

@inproceedings{chellapilla_high_2006,
  title = {High Performance Convolutional Neural Networks for Document Processing},
  booktitle = {Tenth {{International Workshop}} on {{Frontiers}} in {{Handwriting Recognition}}},
  publisher = {{Suvisoft}},
  author = {Chellapilla, Kumar and Puri, Sidd and Simard, Patrice},
  year = {2006},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/Z6TCEL73/document.pdf}
}

@inproceedings{liu_icdar_2011,
  title = {{{ICDAR}} 2011 {{Chinese Handwriting Recognition Competition}}},
  doi = {10.1109/ICDAR.2011.291},
  abstract = {In the Chinese handwriting recognition competition organized with the ICDAR 2011, four tasks were evaluated: offline and online isolated character recognition, offline and online handwritten text recognition. To enable the training of recognition systems, we announced the large databases CASIA-HWDB/OLHWDB. The submitted systems were evaluated on un-open datasets to report character-level correct rates. In total, we received 25 systems submitted by eight groups. On the test datasets, the best results (correct rates) are 92.18\% for offline character recognition, 95.77\% for online character recognition, 77.26\% for offline text recognition, and 94.33\% for online text recognition, respectively. In addition to the evaluation results, we provide short descriptions of the recognition methods and have brief discussions.},
  booktitle = {2011 {{International Conference}} on {{Document Analysis}} and {{Recognition}}},
  author = {Liu, C. L. and Yin, F. and Wang, Q. F. and Wang, D. H.},
  month = sep,
  year = {2011},
  keywords = {handwritten character recognition,Character recognition,Training,Support vector machine classification,Feature extraction,Databases,CASIA-HWDB database,character-level correct rate,Chinese handwriting recognition competition,handwriting recognition,Handwriting recognition,handwritten text recognition,ICDAR 2011,isolated character recongition,offline,offline handwritten text recognition,offline isolated character recognition,OLHWDB database,online,online handwritten text recognition,online isolated character recognition,text analysis,Text recognition},
  pages = {1464-1469},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/M5YSUKPP/6065551.html}
}

@inproceedings{stallkamp_german_2011,
  title = {The {{German Traffic Sign Recognition Benchmark}}: {{A}} Multi-Class Classification Competition},
  shorttitle = {The {{German Traffic Sign Recognition Benchmark}}},
  doi = {10.1109/IJCNN.2011.6033395},
  abstract = {The ``German Traffic Sign Recognition Benchmark'' is a multi-category classification competition held at IJCNN 2011. Automatic recognition of traffic signs is required in advanced driver assistance systems and constitutes a challenging real-world computer vision and pattern recognition problem. A comprehensive, lifelike dataset of more than 50,000 traffic sign images has been collected. It reflects the strong variations in visual appearance of signs due to distance, illumination, weather conditions, partial occlusions, and rotations. The images are complemented by several precomputed feature sets to allow for applying machine learning algorithms without background knowledge in image processing. The dataset comprises 43 classes with unbalanced class frequencies. Participants have to classify two test sets of more than 12,500 images each. Here, the results on the first of these sets, which was used in the first evaluation stage of the two-fold challenge, are reported. The methods employed by the participants who achieved the best results are briefly described and compared to human traffic sign recognition performance and baseline results.},
  booktitle = {The 2011 {{International Joint Conference}} on {{Neural Networks}}},
  author = {Stallkamp, J. and Schlipsing, M. and Salmen, J. and Igel, C.},
  month = jul,
  year = {2011},
  keywords = {Image color analysis,learning (artificial intelligence),Histograms,computer vision,Image resolution,Training,image classification,Benchmark testing,image processing,Humans,driver assistance system,driver information systems,German Traffic Sign Recognition Benchmark,Lead,machine learning algorithm,multiclass classification competition,pattern recognition problem,traffic engineering computing},
  pages = {1453-1460},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/DICIBBY6/6033395.html}
}

@article{lettvin_what_1959,
  title = {What the {{Frog}}'s {{Eye Tells}} the {{Frog}}'s {{Brain}}},
  volume = {47},
  issn = {0096-8390},
  doi = {10.1109/JRPROC.1959.287207},
  abstract = {In this paper, we analyze the activity of single fibers in the optic nerve of a frog. Our method is to find what sort of stimulus causes the largest activity in one nerve fiber and then what is the exciting aspect of that stimulus such that variations in everything else cause little change in the response. It has been known for the past 20 years that each fiber is connected not to a few rods and cones in the retina but to very many over a fair area. Our results show that for the most part within that area, it is not the light intensity itself but rather the pattern of local variation of intensity that is the exciting factor. There are four types of fibers, each type concerned with a different sort of pattern. Each type is uniformly distributed over the whole retina of the frog. Thus, there are four distinct parallel distributed channels whereby the frog's eye informs his brain about the visual image in terms of local pattern independent of average illumination. We describe the patterns and show the functional and anatomical separation of the channels. This work has been done on the frog, and our interpretation applies only to the frog.},
  number = {11},
  journal = {Proceedings of the IRE},
  author = {Lettvin, J. Y. and Maturana, H. R. and McCulloch, W. S. and Pitts, W. H.},
  month = nov,
  year = {1959},
  keywords = {Lighting,Retina,Cerebral cortex,Eyes,Gravity,Nerve fibers,Optical fibers,Relays,Senior members,Visual system},
  pages = {1940-1951},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/X8ZQ4GDY/4065609.html}
}

@article{serre_quantitative_2007,
  title = {A Quantitative Theory of Immediate Visual Recognition},
  volume = {165},
  issn = {0079-6123},
  doi = {10.1016/S0079-6123(06)65004-8},
  abstract = {Human and non-human primates excel at visual recognition tasks. The primate visual system exhibits a strong degree of selectivity while at the same time being robust to changes in the input image. We have developed a quantitative theory to account for the computations performed by the feedforward path in the ventral stream of the primate visual cortex. Here we review recent predictions by a model instantiating the theory about physiological observations in higher visual areas. We also show that the model can perform recognition tasks on datasets of complex natural images at a level comparable to psychophysical measurements on human observers during rapid categorization tasks. In sum, the evidence suggests that the theory may provide a framework to explain the first 100-150 ms of visual object recognition. The model also constitutes a vivid example of how computational models can interact with experimental observations in order to advance our understanding of a complex phenomenon. We conclude by suggesting a number of open questions, predictions, and specific experiments for visual physiology and psychophysics.},
  language = {eng},
  journal = {Progress in Brain Research},
  author = {Serre, Thomas and Kreiman, Gabriel and Kouh, Minjoon and Cadieu, Charles and Knoblich, Ulf and Poggio, Tomaso},
  year = {2007},
  keywords = {Humans,Animals,Computer Simulation,Field Dependence-Independence,Models; Biological,Pattern Recognition; Visual,Photic Stimulation,Psychophysics,Visual Cortex},
  pages = {33-56},
  pmid = {17925239}
}

@book{hochreiter_gradient_2001,
  title = {Gradient {{Flow}} in {{Recurrent Nets}}: The {{Difficulty}} of {{Learning Long}}-{{Term Dependencies}}},
  shorttitle = {Gradient {{Flow}} in {{Recurrent Nets}}},
  abstract = {Recurrent networks (crossreference Chapter 12) can, in principle, use their feedback connections to store representations of recent input events in the form of activations. The most widely used algorithms for learning what to put in short-term memory, however, take too much time to be feasible or do not work well at all, especially when minimal time lags between inputs and corresponding teacher signals are long. Although theoretically fascinating, they do not provide clear practical advantages over, say, backprop in feedforward networks with limited time windows (see crossreference Chapters 11 and 12). With conventional "algorithms based on the computation of the complete gradient", such as "Back-Propagation Through Time" (BPTT, e.g., [22, 27, 26]) or "Real-Time Recurrent Learning" (RTRL, e.g., [21]) error signals "flowing backwards in time" tend to either (1) blow up or (2) vanish: the temporal evolution of the backpropagated error ex},
  author = {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, J{\"u}rgen},
  year = {2001},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/BVN852HR/Hochreiter et al. - 2001 - Gradient Flow in Recurrent Nets the Difficulty of.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/NWB9R498/summary.html}
}

@incollection{lecun_efficient_1998,
  series = {Lecture Notes in Computer Science},
  title = {Efficient {{BackProp}}},
  isbn = {978-3-540-65311-0 978-3-540-49430-0},
  abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most ``classical'' second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
  language = {en},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {LeCun, Yann and Bottou, Leon and Orr, Genevieve B. and M{\"u}ller, Klaus-Robert},
  year = {1998},
  pages = {9-50},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/M5LR2BBH/LeCun et al. - 1998 - Efficient BackProp.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/BQIRL2KK/3-540-49430-8_2.html},
  doi = {10.1007/3-540-49430-8_2}
}

@article{ramachandran_searching_2018,
  title = {Searching for {{Activation Functions}}},
  abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the...},
  author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
  month = feb,
  year = {2018},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/B8TPXEPV/Ramachandran et al. - 2018 - Searching for Activation Functions.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/NUSI62IX/forum.html}
}

@article{sonoda_neural_2017,
  title = {Neural Network with Unbounded Activation Functions Is Universal Approximator},
  volume = {43},
  issn = {1063-5203},
  doi = {10.1016/j.acha.2015.12.005},
  abstract = {This paper presents an investigation of the approximation property of neural networks with unbounded activation functions, such as the rectified linear unit (ReLU), which is the new de-facto standard of deep learning. The ReLU network can be analyzed by the ridgelet transform with respect to Lizorkin distributions. By showing three reconstruction formulas by using the Fourier slice theorem, the Radon transform, and Parseval's relation, it is shown that a neural network with unbounded activation functions still satisfies the universal approximation property. As an additional consequence, the ridgelet transform, or the backprojection filter in the Radon domain, is what the network learns after backpropagation. Subject to a constructive admissibility condition, the trained network can be obtained by simply discretizing the ridgelet transform, without backpropagation. Numerical examples not only support the consistency of the admissibility condition but also imply that some non-admissible cases result in low-pass filtering.},
  number = {2},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Sonoda, Sho and Murata, Noboru},
  month = sep,
  year = {2017},
  keywords = {Admissibility condition,Backprojection filter,Bounded extension to,Integral representation,Lizorkin distribution,Neural network,Radon transform,Rectified linear unit (ReLU),Ridgelet transform,Universal approximation},
  pages = {233-268},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FQK2ABLS/Sonoda et Murata - 2017 - Neural network with unbounded activation functions.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/ZFNGVMR4/S1063520315001748.html}
}

@inproceedings{mhaskar_when_2017,
  title = {When and Why Are Deep Networks Better than Shallow Ones?},
  booktitle = {{{AAAI}}},
  author = {Mhaskar, Hrushikesh and Liao, Qianli and Poggio, Tomaso A.},
  year = {2017},
  pages = {2343--2349},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VE8DTPUE/14849-66902-1-PB.pdf}
}

@article{poggio_why_2017,
  title = {Why and When Can Deep-but Not Shallow-Networks Avoid the Curse of Dimensionality: {{A}} Review},
  volume = {14},
  issn = {1476-8186, 1751-8520},
  shorttitle = {Why and When Can Deep-but Not Shallow-Networks Avoid the Curse of Dimensionality},
  doi = {10.1007/s11633-017-1054-2},
  abstract = {The paper reviews and extends an emerging body of theoretical results on deep learning including the conditions under which it can be exponentially better than shallow learning. A class of deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. Implications of a few key theorems are discussed, together with new results, open problems and conjectures.},
  language = {en},
  number = {5},
  journal = {International Journal of Automation and Computing},
  author = {Poggio, Tomaso and Mhaskar, Hrushikesh and Rosasco, Lorenzo and Miranda, Brando and Liao, Qianli},
  month = oct,
  year = {2017},
  pages = {503-519},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/59299T2I/Poggio et al. - 2017 - Why and when can deep-but not shallow-networks avo.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/AUZY2F5T/s11633-017-1054-2.html}
}

@article{bianchini_complexity_2014,
  title = {On the {{Complexity}} of {{Neural Network Classifiers}}: {{A Comparison Between Shallow}} and {{Deep Architectures}}},
  volume = {25},
  issn = {2162-237X},
  shorttitle = {On the {{Complexity}} of {{Neural Network Classifiers}}},
  doi = {10.1109/TNNLS.2013.2293637},
  abstract = {Recently, researchers in the artificial neural network field have focused their attention on connectionist models composed by several hidden layers. In fact, experimental results and heuristic considerations suggest that deep architectures are more suitable than shallow ones for modern applications, facing very complex problems, e.g., vision and human language understanding. However, the actual theoretical results supporting such a claim are still few and incomplete. In this paper, we propose a new approach to study how the depth of feedforward neural networks impacts on their ability in implementing high complexity functions. First, a new measure based on topological concepts is introduced, aimed at evaluating the complexity of the function implemented by a neural network, used for classification purposes. Then, deep and shallow neural architectures with common sigmoidal activation functions are compared, by deriving upper and lower bounds on their complexity, and studying how the complexity depends on the number of hidden units and the used activation function. The obtained results seem to support the idea that deep networks actually implements functions of higher complexity, so that they are able, with the same number of resources, to address more difficult problems.},
  number = {8},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  author = {Bianchini, M. and Scarselli, F.},
  month = aug,
  year = {2014},
  keywords = {deep neural networks,Computer architecture,classification,Complexity theory,feedforward neural nets,Biological neural networks,deep network,computational complexity,Neurons,hidden layers,artificial neural network,Betti numbers,deep architecture,feedforward neural network,function approximation,function complexity evaluation,high complexity functions,human language understanding,neural network classifiers,pattern classification,Polynomials,shallow architecture,sigmoidal activation function,topological complexity,topological concepts,topology,Upper bound,Vapnik–Chervonenkis dimension (VC-dim),Vapnik–Chervonenkis dimension (VC-dim).,vision},
  pages = {1553-1565},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/J99VEKHQ/6697897.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/M6WJRD7N/6697897.html}
}

@article{zoph_neural_2016,
  title = {Neural {{Architecture Search}} with {{Reinforcement Learning}}},
  abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are...},
  author = {Zoph, Barret and Le, Quoc},
  month = nov,
  year = {2016},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/D228RPJX/Zoph et Le - 2016 - Neural Architecture Search with Reinforcement Lear.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/TPS62V46/forum.html}
}

@book{lhospital_analyse_1716,
  title = {{Analyse des infiniment petits, pour l'intelligence des lignes courbes}},
  language = {French},
  publisher = {{Paris : Montalant}},
  author = {de L'Hospital, marquis},
  collaborator = {{University of Ottawa}},
  year = {1716},
  keywords = {Calcul diffeÌrentiel}
}

@book{lagrange_theorie_1797,
  title = {{Th{\'e}orie des fonctions analytiques, contenant les principes du calcul diff{\'e}rentiel , d{\'e}gag{\'e}s de toute consid{\'e}ration d'infiniment petits ou d'{\'e}vanouissans, de limites ou de fluxions, et r{\'e}duits a l'analyse alg{\'e}brique des quantit{\'e}s finies ; par J. L. Lagrange, de l'Institut national.}},
  copyright = {domaine public},
  abstract = {Th{\'e}orie des fonctions analytiques, contenant les principes du calcul diff{\'e}rentiel , d{\'e}gag{\'e}s de toute consid{\'e}ration d'infiniment petits ou d'{\'e}vanouissans, de limites ou de fluxions, et r{\'e}duits a l'analyse alg{\'e}brique des quantit{\'e}s finies ; par J. L. Lagrange, de l'Institut national. -- 1797 -- livre},
  language = {french},
  publisher = {{A Paris, de l'Imprimerie de la R{\'e}publique. Prairial an V.}},
  author = {Lagrange, Joseph-Louis (1736-1813)},
  year = {1797},
  keywords = {Calcul différentiel -- Ouvrages avant 1800,Fonctions analytiques -- Ouvrages avant 1800},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/GQ38UARE/bpt6k86263h.html},
  note = {ark:/12148/bpt6k86263h}
}

@book{cauchy_comptes_1847,
  address = {Paris},
  title = {{Comptes rendus hebdomadaires des s{\'e}ances de l'Acad{\'e}mie des sciences}},
  copyright = {domaine public},
  abstract = {Comptes rendus hebdomadaires des s{\'e}ances de l'Acad{\'e}mie des sciences / publi{\'e}s... par MM. les secr{\'e}taires perp{\'e}tuels -- 1847-07 -- periodiques},
  language = {language.label.fran{\c c}ais},
  publisher = {{Gauthier-Villars}},
  author = {Cauchy, Augustin Louis},
  month = jul,
  year = {1847},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/3U63RZPH/f540.html},
  note = {ark:/12148/bpt6k2982c}
}

@inproceedings{srivastava_training_2015,
  title = {Training Very Deep Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Srivastava, Rupesh K. and Greff, Klaus and Schmidhuber, J{\"u}rgen},
  year = {2015},
  pages = {2377--2385},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/SSYTTD3E/5850-training-very-deep-networks.pdf}
}

@techreport{krizhevsky_learning_2009,
  title = {Learning Multiple Layers of Features from Tiny Images},
  institution = {{CIFAR}},
  author = {Krizhevsky, Alex},
  year = {2009},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/RDUUTJLM/learning-features-2009-TR.pdf}
}

@inproceedings{neuhold_mapillary_2017,
  title = {The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes},
  booktitle = {Proceedings of the {{International Conference}} on {{Computer Vision}} ({{ICCV}}), {{Venice}}, {{Italy}}},
  author = {Neuhold, Gerhard and Ollmann, Tobias and Bul{\`o}, S. Rota and Kontschieder, Peter},
  year = {2017},
  pages = {22--29},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/SFRHVK47/ICCV17a.pdf}
}

@inproceedings{huang_densely_2017,
  title = {Densely Connected Convolutional Networks},
  volume = {1},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q. and {van der Maaten}, Laurens},
  year = {2017},
  pages = {3},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/BUE4EVCI/densely-connected.pdf}
}

@article{cheng_remote_2017,
  title = {Remote {{Sensing Image Scene Classification}}: {{Benchmark}} and {{State}} of the {{Art}}},
  volume = {105},
  issn = {0018-9219},
  shorttitle = {Remote {{Sensing Image Scene Classification}}},
  doi = {10.1109/JPROC.2017.2675998},
  abstract = {Remote sensing image scene classification plays an important role in a wide range of applications and hence has been receiving remarkable attention. During the past years, significant efforts have been made to develop various data sets or present a variety of approaches for scene classification from remote sensing images. However, a systematic review of the literature concerning data sets and methods for scene classification is still lacking. In addition, almost all existing data sets have a number of limitations, including the small scale of scene classes and the image numbers, the lack of image variations and diversity, and the saturation of accuracy. These limitations severely limit the development of new approaches especially deep learning-based methods. This paper first provides a comprehensive review of the recent progress. Then, we propose a large-scale data set, termed ``NWPU-RESISC45,'' which is a publicly available benchmark for REmote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This data set contains 31 500 images, covering 45 scene classes with 700 images in each class. The proposed NWPU-RESISC45 1) is large-scale on the scene classes and the total image number; 2) holds big variations in translation, spatial resolution, viewpoint, object pose, illumination, background, and occlusion; and 3) has high within-class diversity and between-class similarity. The creation of this data set will enable the community to develop and evaluate various data-driven algorithms. Finally, several representative methods are evaluated using the proposed data set, and the results are reported as a useful baseline for future research.},
  number = {10},
  journal = {Proceedings of the IEEE},
  author = {Cheng, G. and Han, J. and Lu, X.},
  month = oct,
  year = {2017},
  keywords = {learning (artificial intelligence),geophysical image processing,Unsupervised learning,Satellites,Spatial resolution,scene classification,Machine learning,deep learning,remote sensing,Benchmark testing,Remote sensing,Classification,remote sensing image,Image analysis,unsupervised feature learning,Benchmark data set,data sets,data-driven algorithms,handcrafted features,image diversity,image numbers,image variations,learning based methods,Northwestern Polytechnical University,NWPU,NWPU-RESISC45,remote sensing image scene classification,representative methods,RESISC,Social network services,various data sets},
  pages = {1865-1883},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/LKU9QIWL/7891544.html}
}

@inproceedings{basu_deepsat_2015,
  address = {New York, NY, USA},
  series = {SIGSPATIAL '15},
  title = {{{DeepSat}}: {{A Learning Framework}} for {{Satellite Imagery}}},
  isbn = {978-1-4503-3967-4},
  shorttitle = {{{DeepSat}}},
  doi = {10.1145/2820783.2820816},
  abstract = {Satellite image classification is a challenging problem that lies at the crossroads of remote sensing, computer vision, and machine learning. Due to the high variability inherent in satellite data, most of the current object classification approaches are not suitable for handling satellite datasets. The progress of satellite image analytics has also been inhibited by the lack of a single labeled high-resolution dataset with multiple class labels. The contributions of this paper are twofold -- (1) first, we present two new satellite datasets called SAT-4 and SAT-6, and (2) then, we propose a classification framework that extracts features from an input image, normalizes them and feeds the normalized feature vectors to a Deep Belief Network for classification. On the SAT-4 dataset, our best network produces a classification accuracy of 97.95\% and outperforms three state-of-the-art object recognition algorithms, namely - Deep Belief Networks, Convolutional Neural Networks and Stacked Denoising Autoencoders by \textasciitilde{}11\%. On SAT-6, it produces a classification accuracy of 93.9\% and outperforms the other algorithms by \textasciitilde{}15\%. Comparative studies with a Random Forest classifier show the advantage of an unsupervised learning approach over traditional supervised learning techniques. A statistical analysis based on Distribution Separability Criterion and Intrinsic Dimensionality Estimation substantiates the effectiveness of our approach in learning better representations for satellite imagery.},
  booktitle = {Proceedings of the 23rd {{SIGSPATIAL International Conference}} on {{Advances}} in {{Geographic Information Systems}}},
  publisher = {{ACM}},
  author = {Basu, Saikat and Ganguly, Sangram and Mukhopadhyay, Supratik and DiBiano, Robert and Karki, Manohar and Nemani, Ramakrishna},
  year = {2015},
  keywords = {deep learning,high resolution,satellite imagery},
  pages = {37:1--37:10},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/GY9QWZDV/Basu et al. - 2015 - DeepSat A Learning Framework for Satellite Imager.pdf}
}

@inproceedings{yang_bag--visual-words_2010,
  address = {New York, NY, USA},
  series = {GIS '10},
  title = {Bag-of-Visual-Words and {{Spatial Extensions}} for {{Land}}-Use {{Classification}}},
  isbn = {978-1-4503-0428-3},
  doi = {10.1145/1869790.1869829},
  abstract = {We investigate bag-of-visual-words (BOVW) approaches to land-use classification in high-resolution overhead imagery. We consider a standard non-spatial representation in which the frequencies but not the locations of quantized image features are used to discriminate between classes analogous to how words are used for text document classification without regard to their order of occurrence. We also consider two spatial extensions, the established spatial pyramid match kernel which considers the absolute spatial arrangement of the image features, as well as a novel method which we term the spatial co-occurrence kernel that considers the relative arrangement. These extensions are motivated by the importance of spatial structure in geographic data. The methods are evaluated using a large ground truth image dataset of 21 land-use classes. In addition to comparisons with standard approaches, we perform extensive evaluation of different configurations such as the size of the visual dictionaries used to derive the BOVW representations and the scale at which the spatial relationships are considered. We show that even though BOVW approaches do not necessarily perform better than the best standard approaches overall, they represent a robust alternative that is more effective for certain land-use classes. We also show that extending the BOVW approach with our proposed spatial co-occurrence kernel consistently improves performance.},
  booktitle = {Proceedings of the 18th {{SIGSPATIAL International Conference}} on {{Advances}} in {{Geographic Information Systems}}},
  publisher = {{ACM}},
  author = {Yang, Yi and Newsam, Shawn},
  year = {2010},
  keywords = {bag-of-visual-words,land-use classification,local invariant features},
  pages = {270--279},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/GFUS52MJ/Yang et Newsam - 2010 - Bag-of-visual-words and Spatial Extensions for Lan.pdf}
}

@article{jones_evaluation_1987,
  title = {An Evaluation of the Two-Dimensional {{Gabor}} Filter Model of Simple Receptive Fields in Cat Striate Cortex},
  volume = {58},
  issn = {0022-3077},
  doi = {10.1152/jn.1987.58.6.1233},
  abstract = {1. Using the two-dimensional (2D) spatial and spectral response profiles described in the previous two reports, we test Daugman's generalization of Marcelja's hypothesis that simple receptive fields belong to a class of linear spatial filters analogous to those described by Gabor and referred to here as 2D Gabor filters. 2. In the space domain, we found 2D Gabor filters that fit the 2D spatial response profile of each simple cell in the least-squared error sense (with a simplex algorithm), and we show that the residual error is devoid of spatial structure and statistically indistinguishable from random error. 3. Although a rigorous statistical approach was not possible with our spectral data, we also found a Gabor function that fit the 2D spectral response profile of each simple cell and observed that the residual errors are everywhere small and unstructured. 4. As an assay of spatial linearity in two dimensions, on which the applicability of Gabor theory is dependent, we compare the filter parameters estimated from the independent 2D spatial and spectral measurements described above. Estimates of most parameters from the two domains are highly correlated, indicating that assumptions about spatial linearity are valid. 5. Finally, we show that the functional form of the 2D Gabor filter provides a concise mathematical expression, which incorporates the important spatial characteristics of simple receptive fields demonstrated in the previous two reports. Prominent here are 1) Cartesian separable spatial response profiles, 2) spatial receptive fields with staggered subregion placement, 3) Cartesian separable spectral response profiles, 4) spectral response profiles with axes of symmetry not including the origin, and 5) the uniform distribution of spatial phase angles. 6. We conclude that the Gabor function provides a useful and reasonably accurate description of most spatial aspects of simple receptive fields. Thus it seems that an optimal strategy has evolved for sampling images simultaneously in the 2D spatial and spatial frequency domains.},
  language = {eng},
  number = {6},
  journal = {Journal of Neurophysiology},
  author = {Jones, J. P. and Palmer, L. A.},
  month = dec,
  year = {1987},
  keywords = {Animals,Visual Fields,Visual Cortex,Cats,Mathematics,Models; Neurological,Visual Perception},
  pages = {1233-1258},
  pmid = {3437332}
}

@article{pati_word_2008,
  title = {Word Level Multi-Script Identification},
  volume = {29},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2008.01.027},
  abstract = {We report an algorithm to identify the script of each word in a document image. We start with a bi-script scenario which is later extended to tri-script and then to eleven-script scenarios. A database of 20,000 words of different font styles and sizes has been collected and used for each script. Effectiveness of Gabor and discrete cosine transform (DCT) features has been independently evaluated using nearest neighbor, linear discriminant and support vector machines (SVM) classifiers. The combination of Gabor features with nearest neighbor or SVM classifier shows promising results; i.e., over 98\% for bi-script and tri-script cases and above 89\% for the eleven-script scenario.},
  number = {9},
  journal = {Pattern Recognition Letters},
  author = {Pati, Peeta Basa and Ramakrishnan, A. G.},
  month = jul,
  year = {2008},
  keywords = {DCT,Gabor filter,Script identification},
  pages = {1218-1229},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/N9DBFCYQ/Pati et Ramakrishnan - 2008 - Word level multi-script identification.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/M38DUHLS/S0167865508000354.html}
}

@book{mallat_exploration_2001,
  address = {Palaiseau},
  title = {{Une exploration des signaux en ondelettes}},
  isbn = {978-2-7302-0733-1},
  language = {Fran{\c c}ais},
  publisher = {{{\'E}ditions de l'{\'E}cole polytechnique}},
  author = {Mallat, St{\'e}phane},
  month = sep,
  year = {2001}
}

@book{daubechies_ten_1992,
  address = {Philadelphia, PA, USA},
  title = {Ten {{Lectures}} on {{Wavelets}}},
  isbn = {978-0-89871-274-2},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Daubechies, Ingrid},
  year = {1992}
}

@incollection{fourier_propagation_1822,
  title = {{Propagation de la chaleur dans un solide rectangulaire infini}},
  language = {fr},
  booktitle = {{Th{\'e}orie analytique de la chaleur}},
  publisher = {{F. Didot p{\`e}re et fils}},
  author = {Fourier, Joseph},
  year = {1822},
  pages = {159-177}
}

@article{marcelja_mathematical_1980,
  title = {Mathematical Description of the Responses of Simple Cortical Cells*},
  volume = {70},
  copyright = {\&\#169; 1980 Optical Society of America},
  doi = {10.1364/JOSA.70.001297},
  abstract = {On the basis of measured receptive field profiles and spatial frequency tuning characteristics of simple cortical cells, it can be concluded that the representation of an image in the visual cortex must involve both spatial and spatial frequency variables. In a scheme due to Gabor, an image is represented in terms of localized symmetrical and antisymmetrical elementary signals. Both measured receptive fields and measured spatial frequency tuning curves conform closely to the functional form of Gabor elementary signals. It is argued that the visual cortex representation corresponds closely to the Gabor scheme owing to its advantages in treating the subsequent problem of pattern recognition.},
  language = {EN},
  number = {11},
  journal = {JOSA},
  author = {Mar{\^c}elja, S.},
  month = nov,
  year = {1980},
  pages = {1297-1300},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/II6HELSX/abstract.html}
}

@article{sobel_isotropic_2014,
  title = {An {{Isotropic}} 3x3 {{Image Gradient Operator}}},
  journal = {Presentation at Stanford A.I. Project 1968},
  author = {Sobel, Irwin},
  month = feb,
  year = {2014},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/ED63F2B6/Sobel - 2014 - An Isotropic 3x3 Image Gradient Operator.pdf}
}

@inproceedings{frangi_multiscale_1998,
  title = {Multiscale Vessel Enhancement Filtering},
  volume = {1496},
  booktitle = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer}},
  author = {Frangi, Alenjandro F. and Niessen, Wiro J. and Vincken, Koen L. and Viergever, Max A.},
  year = {1998},
  pages = {130-137},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/XHHYLMUI/Frangi1998-Vesselness.pdf}
}

@inproceedings{shotton_semantic_2008,
  title = {Semantic Texton Forests for Image Categorization and Segmentation},
  booktitle = {Computer Vision and Pattern Recognition, 2008. {{CVPR}} 2008. {{IEEE Conference}} On},
  publisher = {{IEEE}},
  author = {Shotton, Jamie and Johnson, Matthew and Cipolla, Roberto},
  year = {2008},
  pages = {1--8},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/4H9G2Z6Y/2008-CVPR-semantic-texton-forests.pdf}
}

@inproceedings{shotton_real-time_2011,
  title = {Real-Time Human Pose Recognition in Parts from Single Depth Images},
  booktitle = {Computer {{Vision}} and {{Pattern Recognition}} ({{CVPR}}), 2011 {{IEEE Conference}} On},
  publisher = {{Ieee}},
  author = {Shotton, Jamie and Fitzgibbon, Andrew and Cook, Mat and Sharp, Toby and Finocchio, Mark and Moore, Richard and Kipman, Alex and Blake, Andrew},
  year = {2011},
  pages = {1297--1304},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FSIAZ8B2/BodyPartRecognition.pdf}
}

@inproceedings{grangier_deep_2009,
  title = {Deep Convolutional Networks for Scene Parsing},
  volume = {3},
  booktitle = {{{ICML}} 2009 {{Deep Learning Workshop}}},
  publisher = {{Citeseer}},
  author = {Grangier, David and Bottou, L{\'e}on and Collobert, Ronan},
  year = {2009},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/SFAR2TAA/grangier-deep-cnn.pdf}
}

@phdthesis{farabet_towards_2013,
  type = {{{PhD Thesis}}},
  title = {Towards Real-Time Image Understanding with Convolutional Networks},
  school = {Universit{\'e} Paris-Est},
  author = {Farabet, Cl{\'e}ment},
  year = {2013},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/7QHQQUPK/farabet-pami-13.pdf}
}

@inproceedings{ciresan_deep_2012,
  title = {Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ciresan, Dan and Giusti, Alessandro and Gambardella, Luca M. and Schmidhuber, J{\"u}rgen},
  year = {2012},
  pages = {2843--2851},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/K49NLRMR/nips2012.pdf}
}

@article{ullman_aligning_1989,
  title = {Aligning Pictorial Descriptions: {{An}} Approach to Object Recognition},
  volume = {32},
  issn = {0010-0277},
  shorttitle = {Aligning Pictorial Descriptions},
  doi = {10.1016/0010-0277(89)90036-X},
  abstract = {This paper examines the problem of shape-based object recognition, and proposes a new approach, the alignment of pictorial descriptions. The first part of the paper reviews general approaches to visual object recognition, and divides these approaches into three broad classes: invariant properties methods, object decomposition methods, and alignment methods. The second part presents the alignment method. In this approach the recognition process is divided into two stages. The first determines the transformation in space that is necessary to bring the viewed object into alignment with possible object models. This stage can proceed on the basis of minimal information, such as the object's dominant orientation, or a small number of corresponding feature points in the object and model. The second stage determines the model that best matches the viewed object. At this stage, the search is over all the possible object models, but not over their possible views, since the transformation has already been determined uniquely in the alignment stage. The proposed alignment method also uses abstract description, but unlike structural description methods it uses them pictorially, rather than in symbolic structural descriptions.
R{\'e}sum{\'e}
Cet article {\'e}tudie le probl{\`e}ma de la reconnaissance des objects {\`a} partir de leur forme st propose une nouvelle approche, l'alignement des descriptions graphiques. La premi{\`e}re partie de l'article passe en revue les approches g{\'e}n{\'e}rales de la reconnaissance visuelle des objets et divise ces approches en trois grandes cat{\'e}gories: m{\'e}thodes des propri{\'e}t{\'e}s invariantes, m{\'e}thodes de d{\'e}composition des objets et m{\'e}thodes d'alignment. La seconde partie pr{\'e}sente la m{\'e}thode d'alignment. Dans cette approache le processus de reconnaissance est divis{\'e} en deux {\'e}tapes. La premi{\`e}re d{\'e}termine la transformation dans l'escape n{\'e}cessaire pour amener l'objet vu en alignement avec les mod{\`e}les possibles d'objets. Cette {\'e}tepa peut se d{\'e}rouler sur la base d'une information minimale, telle que l'orientation dominante de l'objet, ou un petit nombre de traits communs entre l'objet et le mod{\`e}le. La seconde {\'e}tape d{\'e}termine le mod{\`e}le qui correspond le mieux avec l'objet vu. Au cours de cette {\'e}tape, la recherche se fait sur tous les mod{\`e}les d'objets possibles, mais pas sur l'ensemble des vues possibles de ces objets, {\'e}tant donn{\'e} que la transformation a d{\'e}ja {\'e}t{\'e} d{\'e}termin{\'e}e d'une mani{\`e}re unique au cours de l'{\'e}tape d'alignment. La m{\'e}thode d'alignment propos{\'e}e utilise {\'e}galement des descriptions abstraites mais {\`a} la diff{\'e}rence des m{\'e}thodes de description structurelles, elle les utilise de mani{\`e}re graphique plut{\^o}t que sous la forme de descriptions structurelles symboliques.},
  number = {3},
  journal = {Cognition},
  author = {Ullman, Shimon},
  month = aug,
  year = {1989},
  pages = {193-254},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/B8DY4IDR/Ullman - 1989 - Aligning pictorial descriptions An approach to ob.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/4TDNWS6B/001002778990036X.html}
}

@book{szeliski_computer_2011,
  address = {London},
  series = {Texts in Computer Science},
  title = {Computer {{Vision}}: {{Algorithms}} and {{Applications}}},
  isbn = {978-1-84882-934-3},
  shorttitle = {Computer {{Vision}}},
  abstract = {Humans perceive the three-dimensional structure of the world with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem and what is the current state of the art? Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging, and for fun, consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos. More than just a source of ``recipes,'' this exceptionally authoritative and comprehensive textbook/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting them to produce descriptions of a scene. These problems are also analyzed using statistical models and solved using rigorous engineering techniques Topics and features: Structured to support active curricula and project-oriented courses, with tips in the Introduction for using the book in a variety of customized courses Presents exercises at the end of each chapter with a heavy emphasis on testing algorithms and containing numerous suggestions for small mid-term projects Provides additional material and more detailed mathematical topics in the Appendices, which cover linear algebra, numerical techniques, and Bayesian estimation theory Suggests additional reading at the end of each chapter, including the latest research in each sub-field, in addition to a full Bibliography at the end of the book Supplies supplementary course material for students at the associated website, http://szeliski.org/Book/ Suitable for an upper-level undergraduate or graduate-level course in computer science or engineering, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries. Its design and exposition also make it eminently suitable as a unique reference to the fundamental techniques and current research literature in computer vision. Dr. Richard Szeliski has more than 25 years' experience in computer vision research, most notably at Digital Equipment Corporation and Microsoft Research. This text draws on that experience, as well as on computer vision courses he has taught at the University of Washington and Stanford.},
  language = {en},
  publisher = {{Springer-Verlag}},
  author = {Szeliski, Richard},
  year = {2011},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HFLTTL8B/9781848829343.html}
}

@inproceedings{schneiderman_probabilistic_1998,
  title = {Probabilistic Modeling of Local Appearance and Spatial Relationships for Object Recognition},
  booktitle = {Computer {{Vision}} and {{Pattern Recognition}}, 1998. {{Proceedings}}. 1998 {{IEEE Computer Society Conference}} On},
  publisher = {{IEEE}},
  author = {Schneiderman, Henry and Kanade, Takeo},
  year = {1998},
  pages = {45--51},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/P7QXKCC5/10.1.1.437.2826.pdf}
}

@inproceedings{vidal-naquet_object_2003,
  title = {Object {{Recognition}} with {{Informative Features}} and {{Linear Classification}}.},
  volume = {3},
  booktitle = {{{ICCV}}},
  author = {{Vidal-Naquet}, Michel and Ullman, Shimon},
  year = {2003},
  pages = {281},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/P4GRHN79/vidal-iccv-03.pdf}
}

@book{crevier_ai_1993,
  address = {New York, NY, USA},
  title = {{{AI}}: {{The Tumultuous History}} of the {{Search}} for {{Artificial Intelligence}}},
  isbn = {978-0-465-02997-6},
  shorttitle = {{{AI}}},
  publisher = {{Basic Books, Inc.}},
  author = {Crevier, Daniel},
  year = {1993}
}

@book{boden_mind_2008,
  address = {Oxford; New York},
  title = {{Mind as Machine: A History of Cognitive Science}},
  isbn = {978-0-19-954316-8},
  shorttitle = {{Mind as Machine}},
  abstract = {The development of cognitive science is one of the most remarkable and fascinating intellectual achievements of the modern era. The quest to understand the mind is as old as recorded human thought; but the progress of modern science has offered new methods and techniques which have revolutionized this enquiry. Oxford University Press now presents a masterful history of cognitive science, told by one of its most eminent practitioners. Cognitive science is the project of understanding the mind by modelling its workings. Psychology is its heart, but it draws together various adjoining fields of research, including artificial intelligence; neuroscientific study of the brain; philosophical investigation of mind, language, logic, and understanding; computational work on logic and reasoning; linguistic research on grammar, semantics, and communication; and anthropological explorations of human similarities and differences. Each discipline, in its own way, asks what the mind is, what it does, how it works, how it developed - how it is even possible. The key distinguishing characteristic of cognitive science, Boden suggests, compared with older ways of thinking about the mind, is the notion of understanding the mind as a kind of machine. She traces the origins of cognitive science back to Descartes's revolutionary ideas, and follows the story through the eighteenth and nineteenth centuries, when the pioneers of psychology and computing appear. Then she guides the reader through the complex interlinked paths along which the study of the mind developed in the twentieth century. Cognitive science, in Boden's broad conception, covers a wide range of aspects of mind: not just 'cognition' in the sense of knowledge or reasoning, but emotion, personality, social communication, and even action. In each area of investigation, Boden introduces the key ideas and the people who developed them. No one else could tell this story as Boden can: she has been an active participant in cognitive science since the 1960s, and has known many of the key figures personally. Her narrative is written in a lively, swift-moving style, enriched by the personal touch of someone who knows the story at first hand. Her history looks forward as well as back: it is her conviction that cognitive science today - and tomorrow - cannot be properly understood without a historical perspective. Mind as Machine will be a rich resource for anyone working on the mind, in any academic discipline, who wants to know how our understanding of our mental activities and capacities has developed.},
  language = {Anglais},
  publisher = {{OUP Oxford}},
  author = {Boden, Margaret},
  month = jun,
  year = {2008}
}

@techreport{papert_summer_1966,
  title = {The {{Summer Vision Project}}},
  abstract = {The summer vision project is an attempt to use our summer workers effectively in the construction of a significant part of a visual system. The particular task was chosen partly because it can be segmented into sub-problems which allow individuals to work independently and yet participate in the construction of a system complex enough to be real landmark in the development of "pattern recognition". The basic structure is fixed for the first phase of work extending to some point in July. Everyone is invited to contribute to the discussion of the second phase. Sussman is coordinator of "Vision Project" meetings and should be consulted by anyone who wishes to participate. The primary goal of the project is to construct a system of programs which will divide a vidisector picture into regions such as likely objects, likely background areas and chaos. We shall call this part of its operation FIGURE-GROUND analysis. It will be impossible to do this without considerable analysis of shape and surface properties, so FIGURE-GROUND analysis is really inseparable in practice from the second goal which is REGION DESCRIPTION. The final goal is OBJECT IDENTIFICATION which will actually name objects by matching them with a vocabulary of known objects.},
  author = {Papert, Seymour},
  year = {1966},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/NANA8QL8/AIM-100.pdf}
}

@article{girshick_region-based_2016,
  title = {Region-{{Based Convolutional Networks}} for {{Accurate Object Detection}} and {{Segmentation}}},
  volume = {38},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2015.2437384},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
  number = {1},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Girshick, R. and Donahue, J. and Darrell, T. and Malik, J.},
  month = jan,
  year = {2016},
  keywords = {object detection,Proposals,transfer learning,Training,deep learning,image segmentation,Detectors,semantic segmentation,Image segmentation,Feature extraction,Object detection,Object recognition,Support vector machines,mAP,mean average precision,source code,convolutional networks,canonical PASCAL VOC Challenge datasets,convolutional codes,detection,high-capacity convolutional networks,image coding,object segmentation,region-based convolutional networks,source coding},
  pages = {142-158},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WXMCGU2W/7112511.html}
}

@inproceedings{liu_ssd_2016,
  series = {Lecture Notes in Computer Science},
  title = {{{SSD}}: {{Single Shot MultiBox Detector}}},
  isbn = {978-3-319-46447-3 978-3-319-46448-0},
  shorttitle = {{{SSD}}},
  doi = {10.1007/978-3-319-46448-0_2},
  abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300\texttimes{}300300\texttimes{}300300 $\backslash$times 300 input, SSD achieves 74.3 \% mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512\texttimes{}512512\texttimes{}512512 $\backslash$times 512 input, SSD achieves 76.9 \% mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.},
  language = {en},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2016},
  publisher = {{Springer, Cham}},
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  month = oct,
  year = {2016},
  pages = {21-37},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/CJGUUA4U/Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/N78A6695/978-3-319-46448-0_2.html}
}

@article{uijlings_selective_2013,
  title = {Selective {{Search}} for {{Object Recognition}}},
  volume = {104},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-013-0620-5},
  abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 \% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/\textasciitilde{}uijlings/SelectiveSearch.html).},
  language = {en},
  number = {2},
  journal = {International Journal of Computer Vision},
  author = {Uijlings, J. R. R. and van de Sande, K. E. A. and Gevers, T. and Smeulders, A. W. M.},
  month = sep,
  year = {2013},
  pages = {154-171},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/4MKZSWNB/Uijlings et al. - 2013 - Selective Search for Object Recognition.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8ATG6QWV/s11263-013-0620-5.html}
}

@inproceedings{gu_recognition_2009,
  title = {Recognition Using Regions},
  doi = {10.1109/CVPR.2009.5206727},
  abstract = {This paper presents a unified framework for object detection, segmentation, and classification using regions. Region features are appealing in this context because: (1) they encode shape and scale information of objects naturally; (2) they are only mildly affected by background clutter. Regions have not been popular as features due to their sensitivity to segmentation errors. In this paper, we start by producing a robust bag of overlaid regions for each image using Arbeldez et al., CVPR 2009. Each region is represented by a rich set of image cues (shape, color and texture). We then learn region weights using a max-margin framework. In detection and segmentation, we apply a generalized Hough voting scheme to generate hypotheses of object locations, scales and support, followed by a verification classifier and a constrained segmenter on each hypothesis. The proposed approach significantly outperforms the state of the art on the ETHZ shape database(87.1\% average detection rate compared to Ferrari et al. 's 67.2\%), and achieves competitive performance on the Caltech 101 database.},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Gu, Chunhui and Lim, J. J. and Arbelaez, P. and Malik, J.},
  month = jun,
  year = {2009},
  keywords = {object detection,image representation,learning (artificial intelligence),Shape,feature extraction,image classification,image segmentation,Robustness,visual databases,Layout,Image databases,Image segmentation,Computer vision,Object detection,Face detection,error statistics,Horses,shape database,Voting},
  pages = {1030-1037},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/XMTUX27C/5206727.html}
}

@article{sermanet_overfeat_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.6229},
  primaryClass = {cs},
  title = {{{OverFeat}}: {{Integrated Recognition}}, {{Localization}} and {{Detection}} Using {{Convolutional Networks}}},
  shorttitle = {{{OverFeat}}},
  abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
  journal = {arXiv:1312.6229 [cs]},
  author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
  month = dec,
  year = {2013},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/22AL6ZS5/Sermanet et al. - 2013 - OverFeat Integrated Recognition, Localization and.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/LYTZSVB6/1312.html}
}

@inproceedings{zou_generic_2014,
  title = {Generic {{Object Detection}} with {{Dense Neural Patterns}} and {{Regionlets}}},
  isbn = {978-1-901725-52-0},
  doi = {10.5244/C.28.72},
  language = {en},
  booktitle = {Proceedings of the {{British Machine Vision Conference}}},
  publisher = {{BMVA Press}},
  author = {Zou, Will and Wang, Xiaoyu and Sun, Miao and Lin, Yuanqing},
  year = {2014},
  pages = {72.1-72.11}
}

@techreport{widrow_adaptive_1960,
  title = {An Adaptive "{{ADALINE}}" Neuron Using Chemical "Memistors"},
  institution = {{Stanford University}},
  author = {Widrow, Bernard},
  month = oct,
  year = {1960},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FL7LJ5M9/t1960anadaptive.pdf}
}

@inproceedings{winter_madaline_1988,
  title = {{{MADALINE RULE II}}: A Training Algorithm for Neural Networks},
  shorttitle = {{{MADALINE RULE II}}},
  doi = {10.1109/ICNN.1988.23872},
  abstract = {A novel algorithm for training multilayer fully connected feedforward networks of ADALINE neurons has been developed. Such networks cannot be trained by the popular backpropagation algorithm, since the ADALINE processing element uses the nondifferentiable signum function for its nonlinearity. The algorithm is called MRII for MADALINE RULE II. Previously, MRII successfully trained the adaptive 'descrambler' portion of a neural network system used for translation invariant pattern recognition. Since then, studies of the algorithm's convergence rates and its ability to produce generalizations have been made. These were conducted by training networks with MRII to emulate fixed networks. The authors present the principles and experimental details of the MRII algorithm. Typical learning curves show the algorithm's efficient use of training data. Architectures that take advantage of MRII's quick learning to produce useful generalizations are presented.$<$$>$},
  booktitle = {{{IEEE}} 1988 {{International Conference}} on {{Neural Networks}}},
  author = {Winter, R. and Widrow, B.},
  month = jul,
  year = {1988},
  keywords = {neural nets,Neural networks,Learning systems,neural networks,artificial intelligence,ADALINE,Artificial intelligence,learning curves,learning systems,MADALINE RULE II,MRII algorithm,multilayer feedforward networks,pattern recognition,training algorithm},
  pages = {401-408 vol.1},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/P6SA2W7X/23872.html}
}

@article{widrow_30_1990,
  title = {30 Years of Adaptive Neural Networks: Perceptron, {{Madaline}}, and Backpropagation},
  volume = {78},
  issn = {0018-9219},
  shorttitle = {30 Years of Adaptive Neural Networks},
  doi = {10.1109/5.58323},
  abstract = {Fundamental developments in feedforward artificial neural networks from the past thirty years are reviewed. The history, origination, operating characteristics, and basic theory of several supervised neural-network training algorithms (including the perceptron rule, the least-mean-square algorithm, three Madaline rules, and the backpropagation technique) are described. The concept underlying these iterative adaptation algorithms is the minimal disturbance principle, which suggests that during training it is advisable to inject new information into a network in a manner that disturbs stored information to the smallest extent possible. The two principal kinds of online rules that have developed for altering the weights of a network are examined for both single-threshold elements and multielement networks. They are error-correction rules, which alter the weights of a network to correct error in the output response to the present input pattern, and gradient rules, which alter the weights of a network during each pattern presentation by gradient descent with the objective of reducing mean-square error (averaged over all training patterns)},
  number = {9},
  journal = {Proceedings of the IEEE},
  author = {Widrow, B. and Lehr, M. A.},
  month = sep,
  year = {1990},
  keywords = {neural nets,Neural networks,backpropagation,Machine learning,Artificial neural networks,iterative methods,Pattern recognition,learning systems,pattern recognition,adaptive neural networks,adaptive systems,Adaptive systems,Backpropagation algorithms,Biological system modeling,error-correction rules,History,iterative adaptation algorithms,Least squares approximation,least-mean-square algorithm,Madaline,multielement networks,pattern presentation,perceptron,single-threshold elements,Subspace constraints,training algorithms},
  pages = {1415-1442},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8WCFWQWA/58323.html}
}

@book{moravec_mind_1988,
  address = {Cambridge},
  title = {{Mind Children \textendash{} The Future of Robot \& Human Intelligence}},
  isbn = {978-0-674-57618-6},
  abstract = {A dizzying display of intellect and wild imaginings by Moravec, a world-class roboticist who has himself developed clever beasts . . . Undeniably, Moravec comes across as a highly knowledgeable and creative talent-which is just what the field needs" - Kirkus Reviews.},
  language = {Anglais},
  publisher = {{Harvard University Press}},
  author = {Moravec, Hans},
  year = {1988}
}

@article{duchi_adaptive_2011,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  volume = {12},
  issn = {ISSN 1533-7928},
  number = {Jul},
  journal = {Journal of Machine Learning Research},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  year = {2011},
  pages = {2121-2159},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/JMPPJ4ET/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/E67JV8MV/duchi11a.html}
}

@inproceedings{nesterov_method_1983,
  title = {A Method of Solving a Convex Programming Problem with Convergence Rate {{O}} (1/K2)},
  volume = {27},
  booktitle = {Soviet {{Mathematics Doklady}}},
  author = {Nesterov, Yurii},
  year = {1983},
  pages = {372--376}
}

@book{qian_momentum_1999,
  title = {On the {{Momentum Term}} in {{Gradient Descent Learning Algorithms}}},
  abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for converge...},
  author = {Qian, Ning},
  year = {1999}
}

@incollection{bottou_stochastic_2012,
  series = {Lecture Notes in Computer Science},
  title = {Stochastic {{Gradient Descent Tricks}}},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.},
  language = {en},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Bottou, L{\'e}on},
  year = {2012},
  pages = {421-436},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/7HV82KFH/Bottou - 2012 - Stochastic Gradient Descent Tricks.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/RJBS6TED/10.html},
  doi = {10.1007/978-3-642-35289-8_25}
}

@article{loshchilov_sgdr_2016,
  title = {{{SGDR}}: {{Stochastic Gradient Descent}} with {{Warm Restarts}}},
  shorttitle = {{{SGDR}}},
  abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate...},
  author = {Loshchilov, Ilya and Hutter, Frank},
  month = nov,
  year = {2016},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/Q4B4CYC2/Loshchilov et Hutter - 2016 - SGDR Stochastic Gradient Descent with Warm Restar.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8RBBD5FA/forum.html}
}

@article{polyak_acceleration_1992,
  title = {Acceleration of {{Stochastic Approximation}} by {{Averaging}}},
  volume = {30},
  issn = {0363-0129},
  doi = {10.1137/0330046},
  number = {4},
  journal = {SIAM J. Control Optim.},
  author = {Polyak, B. T. and Juditsky, A. B.},
  month = jul,
  year = {1992},
  keywords = {optimal algorithms,recursive estimation,stochastic approximation,stochastic optimization},
  pages = {838--855}
}

@misc{tielman_lecture_2012,
  title = {Lecture 6.5---{{RmsProp}}: {{Divide}} the Gradient by a Running Average of Its Recent Magnitude},
  publisher = {{COURSERA: Neural Networks for Machine Learning}},
  author = {Tielman, T. and Hinton, Geoffrey},
  year = {2012}
}

@inproceedings{krogh_simple_1991,
  address = {San Francisco, CA, USA},
  series = {NIPS'91},
  title = {A {{Simple Weight Decay Can Improve Generalization}}},
  isbn = {978-1-55860-222-9},
  abstract = {It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Neural Information Processing Systems}}},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  author = {Krogh, Anders and Hertz, John A.},
  year = {1991},
  pages = {950--957}
}

@inproceedings{wan_regularization_2013,
  title = {Regularization of {{Neural Networks}} Using {{DropConnect}}},
  abstract = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations ar...},
  language = {en},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Cun, Yann Le and Fergus, Rob},
  month = feb,
  year = {2013},
  pages = {1058-1066},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/GYPU76HA/Wan et al. - 2013 - Regularization of Neural Networks using DropConnec.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/QF83X24Z/wan13.html}
}

@article{zeiler_stochastic_2013,
  title = {Stochastic {{Pooling}} for {{Regularization}} of {{Deep Convolutional Neural Networks}}},
  abstract = {We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly...},
  author = {Zeiler, Matthew and Fergus, Rob},
  month = jan,
  year = {2013},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WK47XE36/forum.html}
}

@article{saxe_exact_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.6120},
  primaryClass = {cond-mat, q-bio, stat},
  title = {Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks},
  abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
  journal = {arXiv:1312.6120 [cond-mat, q-bio, stat]},
  author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
  month = dec,
  year = {2013},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Learning,Statistics - Machine Learning,Quantitative Biology - Neurons and Cognition,Computer Science - Computer Vision and Pattern Recognition,Condensed Matter - Disordered Systems and Neural Networks},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/RK8H9LYM/Saxe et al. - 2013 - Exact solutions to the nonlinear dynamics of learn.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/5MBP4HN8/1312.html}
}

@inproceedings{jarrett_what_2009,
  title = {What Is the Best Multi-Stage Architecture for Object Recognition?},
  doi = {10.1109/ICCV.2009.5459469},
  abstract = {In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63\% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6\%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 ($>$ 65\%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53\%).},
  booktitle = {2009 {{IEEE}} 12th {{International Conference}} on {{Computer Vision}}},
  author = {Jarrett, K. and Kavukcuoglu, K. and Ranzato, M. and LeCun, Y.},
  month = sep,
  year = {2009},
  keywords = {Image edge detection,object recognition,Histograms,feature extraction,Learning systems,Feature extraction,unsupervised learning,Object recognition,Error analysis,Brain modeling,Caltech-101,feature pooling layer,feature rectification,filter bank,Filter bank,Gabor filters,local contrast normalization,multistage architecture,nonlinear transformation,NORB dataset,Refining,supervised learning,unprocessed MNIST dataset},
  pages = {2146-2153},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/U94K9V33/5459469.html}
}

@article{pinto_why_2008,
  title = {Why Is {{Real}}-{{World Visual Object Recognition Hard}}?},
  volume = {4},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.0040027},
  abstract = {Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, ``natural'' images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled ``natural'' images in guiding that progress. In particular, we show that a simple V1-like model\textemdash{}a neuroscientist's ``null'' model, which should perform poorly at real-world visual object recognition tasks\textemdash{}outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test. As a counterpoint, we designed a ``simpler'' recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model. Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction. Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition\textemdash{}real-world image variation.},
  language = {en},
  number = {1},
  journal = {PLOS Computational Biology},
  author = {Pinto, Nicolas and Cox, David D. and DiCarlo, James J.},
  month = jan,
  year = {2008},
  keywords = {Object recognition,Support vector machines,Principal component analysis,Grayscale,Human performance,Imaging techniques,Primates,Vision},
  pages = {e27},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/ZEAR43IG/Pinto et al. - 2008 - Why is Real-World Visual Object Recognition Hard.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WIWWZ5G6/article.html}
}

@inproceedings{lee_deeply-supervised_2015,
  title = {Deeply-{{Supervised Nets}}},
  abstract = {We propose deeply-supervised nets (DSN), a method that simultaneously minimizes classification error and improves the directness and transparency of the hidden layer learning process. We focus our ...},
  language = {en},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Lee, Chen-Yu and Xie, Saining and Gallagher, Patrick and Zhang, Zhengyou and Tu, Zhuowen},
  month = feb,
  year = {2015},
  pages = {562-570},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/IPSAFNZ4/Lee et al. - 2015 - Deeply-Supervised Nets.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/C76DVE8G/lee15a.html}
}

@inproceedings{szegedy_rethinking_2016,
  title = {Rethinking the Inception Architecture for Computer Vision},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  year = {2016},
  pages = {2818--2826},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/NZH2H8RP/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf}
}

@inproceedings{ciresan_multi-column_2012,
  address = {Washington, DC, USA},
  series = {CVPR '12},
  title = {Multi-Column {{Deep Neural Networks}} for {{Image Classification}}},
  isbn = {978-1-4673-1226-4},
  abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
  booktitle = {Proceedings of the 2012 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  publisher = {{IEEE Computer Society}},
  author = {Ciresan, Dan and Meier, Ueli and Schmidhuber, J{\"u}rgen},
  year = {2012},
  keywords = {Computer architecture,Training,Benchmark testing,Neurons,Error analysis,Graphics processing unit},
  pages = {3642--3649}
}

@article{cui_scalable_2017,
  title = {Scalable {{Bag}} of {{Subpaths Kernel}} for {{Learning}} on {{Hierarchical Image Representations}} and {{Multi}}-{{Source Remote Sensing Data Classification}}},
  volume = {9},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  doi = {10.3390/rs9030196},
  abstract = {The geographic object-based image analysis (GEOBIA) framework has gained increasing interest for the last decade. One of its key advantages is the hierarchical representation of an image, where object topological features can be extracted and modeled in the form of structured data. We thus propose to use a structured kernel relying on the concept of bag of subpaths to directly cope with such features. The kernel can be approximated using random Fourier features, allowing it to be applied on a large structure size (the number of objects in the structured data) and large volumes of data (the number of pixels or regions for training). With the so-called scalable bag of subpaths kernel (SBoSK), we also introduce a novel multi-source classification approach performing machine learning directly on a hierarchical image representation built from two images at different resolutions under the GEOBIA framework. Experiments run on an urban classification task show that the proposed approach run on a single image improves the classification overall accuracy in comparison with conventional approaches from 2\% to 5\% depending on the training set size and that fusing two images allows a supplementary 4\% accuracy gain. Additional evaluations on public available large-scale datasets illustrate further the potential of SBoSK, with overall accuracy rates improvement ranging from 1\% to 11\% depending on the considered setup.},
  language = {en},
  number = {3},
  journal = {Remote Sensing},
  author = {Cui, Yanwei and Chapel, Laetitia and Lef{\`e}vre, S{\'e}bastien},
  month = feb,
  year = {2017},
  keywords = {GEOBIA,hierarchical representations,kernel approximation,large-scale machine learning,random Fourier features,structured kernel},
  pages = {196},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/KM7BUVGJ/Cui et al. - 2017 - Scalable Bag of Subpaths Kernel for Learning on Hi.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/YMPSRPTJ/Cui et al. - 2017 - Scalable Bag of Subpaths Kernel for Learning on Hi.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/BX5XQIW2/196.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/E3U5W433/196.html}
}

@inproceedings{zhou_stereo_1988,
  title = {Stereo Matching Using a Neural Network},
  doi = {10.1109/ICASSP.1988.196745},
  abstract = {A polynomial is fitted to find a smooth continuous intensity function in a window and the first-order intensity derivatives are estimated. A neural network is then used to implement the matching procedure under the epipolar, photometric and smoothness constraints, using the estimated first-order derivatives. Owing to the dense intensity derivatives, a dense array of disparities is generated with only a few iterations. The method does not require surface interpolation. Computer simulations to demonstrate the efficacy of the method are presented},
  booktitle = {{{ICASSP}}-88., {{International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Zhou, Y. T. and Chellappa, R.},
  month = apr,
  year = {1988},
  keywords = {Image edge detection,neural nets,Neural networks,Detectors,Smoothing methods,Interpolation,Humans,Signal processing,Polynomials,neural network,computer simulations,computerised picture processing,dense array,Distortion measurement,epipolar constraints,estimated first-order derivatives,first-order intensity derivatives,intensity derivatives,matching procedure,Noise level,photometric constraints,smooth continuous intensity function,smoothness constraints,stereo matching},
  pages = {940-943 vol.2},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/KZMCYEJA/cookiedetectresponse.html}
}

@article{ulyanov_deep_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.10925},
  primaryClass = {cs, stat},
  title = {Deep {{Image Prior}}},
  abstract = {Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs. Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity. Code and supplementary material are available at https://dmitryulyanov.github.io/deep\_image\_prior .},
  journal = {arXiv:1711.10925 [cs, stat]},
  author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  month = nov,
  year = {2017},
  keywords = {Statistics - Machine Learning,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/UMRDSXKF/Ulyanov et al. - 2017 - Deep Image Prior.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FSTJFEQI/1711.html}
}

@article{odena_deconvolution_2016,
  title = {Deconvolution and {{Checkerboard Artifacts}}},
  volume = {1},
  issn = {2476-0757},
  doi = {10.23915/distill.00003},
  abstract = {When we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts.},
  language = {en},
  number = {10},
  journal = {Distill},
  author = {Odena, Augustus and Dumoulin, Vincent and Olah, Chris},
  month = oct,
  year = {2016},
  pages = {e3},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/2W87L3KL/deconv-checkerboard.html}
}

@article{shensa_discrete_1992,
  title = {The Discrete Wavelet Transform: Wedding the a Trous and {{Mallat}} Algorithms},
  volume = {40},
  issn = {1053-587X},
  shorttitle = {The Discrete Wavelet Transform},
  doi = {10.1109/78.157290},
  abstract = {Two separately motivated implementations of the wavelet transform are brought together. It is observed that these algorithms are both special cases of a single filter bank structure, the discrete wavelet transform, the behavior of which is governed by the choice of filters. In fact, the a trous algorithm is more properly viewed as a nonorthonormal multiresolution algorithm for which the discrete wavelet transform is exact. Moreover, it is shown that the commonly used Lagrange a trous filters are in one-to-one correspondence with the convolutional squares of the Daubechies filters for orthonormal wavelets of compact support. A systematic framework for the discrete wavelet transform is provided, and conditions are derived under which it computes the continuous wavelet transform exactly. Suitable filter constraints for finite energy and boundedness of the discrete transform are also derived. Relevant signal processing parameters are examined, and it is observed that orthonormality is balanced by restrictions on resolution},
  number = {10},
  journal = {IEEE Transactions on Signal Processing},
  author = {Shensa, M. J.},
  month = oct,
  year = {1992},
  keywords = {Discrete transforms,Convolution,Signal processing,signal processing,Filter bank,a trous algorithm,continuous wavelet transform,Continuous wavelet transforms,convolutional squares,Daubechies filters,discrete wavelet transform,Discrete wavelet transforms,Energy resolution,filtering and prediction theory,finite energy,Lagrange a trous filters,Lagrangian functions,Mallat algorithms,nonorthonormal multiresolution algorithm,Signal processing algorithms,signal processing parameters,Signal resolution,single filter bank structure,wavelet transforms},
  pages = {2464-2482},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WM9KUGKU/cookiedetectresponse.html}
}

@inproceedings{papageorgiou_general_1998,
  address = {Washington, DC, USA},
  series = {ICCV '98},
  title = {A {{General Framework}} for {{Object Detection}}},
  isbn = {978-81-7319-221-0},
  abstract = {This paper presents a general trainable framework for object detection in static images of cluttered scenes. The detection technique we develop is based on a wavelet representation of an object class derived from a statistical analysis of the class instances. By learning an object class in terms of a subset of an overcomplete dictionary of wavelet basis functions, we derive a compact representation of an object class which is used as an input to a support vector machine classifier. This representation overcomes both the problem of in-class variability and provides a low false detection rate in unconstrained environments.We demonstr ate the capabilities of the technique in two domains whose inherent information content differs significantly. The first system is face detection and the second is the domain of people which, in contrast to faces, vary greatly in color, texture, and patterns. Unlike previous approaches, this system learns from examples and does not rely on any a priori (hand-crafted) models or motion-based segmentation. The paper also presents a motion-based extension to enhance the performance of the detection algorithm over video sequences. The results presented here suggest that this architecture may well be quite general.},
  booktitle = {Proceedings of the {{Sixth International Conference}} on {{Computer Vision}}},
  publisher = {{IEEE Computer Society}},
  author = {Papageorgiou, Constantine P. and Oren, Michael and Poggio, Tomaso},
  year = {1998},
  pages = {555--}
}

@inproceedings{viola_robust_2001,
  title = {Robust {{Real}}-Time {{Object Detection}}},
  abstract = {This paper describes a visual object detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the ``Integral Image '' which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features and yields extremely efficient classifiers [6]. The third contribution is a method for combining classifiers in a ``cascade '' which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. A set of experiments in the domain of face detection are presented. The system yields face detection performace comparable to the best previous systems [18, 13, 16, 12, 1]. Implemented on a conventional desktop, face detection proceeds at 15 frames per second. 1.},
  booktitle = {International {{Journal}} of {{Computer Vision}}},
  author = {Viola, Paul and Jones, Michael},
  year = {2001},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/YTIP7A8E/Viola et Jones - 2001 - Robust Real-time Object Detection.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8H2CV6JS/summary.html}
}

@inproceedings{szegedy_inception-v4_2017,
  title = {Inception-v4, Inception-Resnet and the Impact of Residual Connections on Learning.},
  volume = {4},
  booktitle = {{{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
  year = {2017},
  pages = {12},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/4WMTVYN2/14806-66795-1-PB.pdf}
}

@article{ciresan_deep_2010,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1003.0358},
  title = {Deep {{Big Simple Neural Nets Excel}} on {{Handwritten Digit Recognition}}},
  volume = {22},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00052},
  abstract = {Good old on-line back-propagation for plain multi-layer perceptrons yields a very low 0.35\% error rate on the famous MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images, and graphics cards to greatly speed up learning.},
  number = {12},
  journal = {Neural Computation},
  author = {Ciresan, Dan Claudiu and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, Juergen},
  month = dec,
  year = {2010},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Artificial Intelligence},
  pages = {3207-3220},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/R4DGA6CA/Ciresan et al. - 2010 - Deep Big Simple Neural Nets Excel on Handwritten D.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/R4XPAVGE/1003.html}
}

@inproceedings{raina_large-scale_2009,
  address = {New York, NY, USA},
  series = {ICML '09},
  title = {Large-Scale {{Deep Unsupervised Learning Using Graphics Processors}}},
  isbn = {978-1-60558-516-1},
  doi = {10.1145/1553374.1553486},
  abstract = {The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton \& Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples. In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods. We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods.},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}}},
  publisher = {{ACM}},
  author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
  year = {2009},
  pages = {873--880},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/QVE7RZ2F/Raina et al. - 2009 - Large-scale Deep Unsupervised Learning Using Graph.pdf}
}

@inproceedings{peng_large_2017,
  title = {Large {{Kernel Matters}} -- {{Improve Semantic Segmentation}} by {{Global Convolutional Network}}},
  author = {Peng, Chao and Zhang, Xiangyu and Yu, Gang and Luo, Guiming and Sun, Jian},
  year = {2017},
  pages = {4353-4361},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/F475HUNL/Peng_Large_Kernel_Matters_CVPR_2017_paper.html}
}

@inproceedings{fourure_residual_2017,
  address = {Londre, France},
  title = {Residual {{Conv}}-{{Deconv Grid Network}} for {{Semantic Segmentation}}},
  abstract = {This paper presents GridNet, a new Convolutional Neural Network (CNN) architecture for semantic image segmentation (full scene labelling). Classical neural networks are implemented as one stream from the input to the output with subsampling operators applied in the stream in order to reduce the feature maps size and to increase the receptive field for the final prediction. However, for semantic image segmentation, where the task consists in providing a semantic class to each pixel of an image, feature maps reduction is harmful because it leads to a resolution loss in the output prediction. To tackle this problem, our GridNet follows a grid pattern allowing multiple interconnected streams to work at different resolutions. We show that our network generalizes many well known networks such as conv-deconv, residual or U-Net networks. GridNet is trained from scratch and achieves competitive results on the Cityscapes dataset.},
  booktitle = {{{BMVC}} 2017},
  author = {Fourure, Damien and Emonet, R{\'e}mi and Fromont, Elisa and Muselet, Damien and Tremeau, Alain and Wolf, Christian},
  month = sep,
  year = {2017},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/LQFMU565/Fourure et al. - 2017 - Residual Conv-Deconv Grid Network for Semantic Seg.pdf}
}

@inproceedings{oyallon_building_2017,
  title = {Building a {{Regular Decision Boundary}} with {{Deep Networks}}},
  doi = {10.1109/CVPR.2017.204},
  abstract = {In this work, we build a generic architecture of Convolutional Neural Networks to discover empirical properties of neural networks. Our first contribution is to introduce a state-of-the-art framework that depends upon few hyper parameters and to study the network when we vary them. It has no max pooling, no biases, only 13 layers, is purely convolutional and yields up to 95.4\% and 79.6\% accuracy respectively on CIFAR10 and CIFAR100. We show that the nonlinearity of a deep network does not need to be continuous, non expansive or point-wise, to achieve good performance. We show that increasing the width of our network permits being competitive with very deep networks. Our second contribution is an analysis of the contraction and separation properties of this network. Indeed, a 1-nearest neighbor classifier applied on deep features progressively improves with depth, which indicates that the representation is progressively more regular. Besides, we defined and analyzed local support vectors that separate classes locally. All our experiments are reproducible and code will be available online, based on TensorFlow.},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Oyallon, E.},
  month = jul,
  year = {2017},
  keywords = {neural nets,deep features,deep neural networks,Buildings,convolutional neural networks,Computer architecture,Training,deep network,Standards,Convolution,pattern classification,1-nearest neighbor classifier,CIFAR10,CIFAR100,network contraction properties,network separation properties,regular decision boundary,Wavelet transforms},
  pages = {1886-1894},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/6YQ9CCD8/8099687.html}
}

@inproceedings{chollet_xception_2017,
  title = {Xception: {{Deep Learning}} with {{Depthwise Separable Convolutions}}},
  shorttitle = {Xception},
  doi = {10.1109/CVPR.2017.195},
  abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chollet, F.},
  month = jul,
  year = {2017},
  keywords = {learning (artificial intelligence),Correlation,Computer architecture,deep learning,image classification,ImageNet dataset,feedforward neural nets,Biological neural networks,Convolutional codes,deep convolutional neural network architecture,depthwise separable convolution operation,Google,image classification dataset,Inception module,Inception V3,neural net architecture,pointwise convolution,regular convolution,Xception},
  pages = {1800-1807},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/JIW6E36G/8099678.html}
}

@article{frieden_new_1976,
  title = {A New Restoring Algorithm for the Preferential Enhancement of Edge Gradients},
  volume = {66},
  copyright = {\&\#169; 1976 Optical Society of America},
  doi = {10.1364/JOSA.66.000280},
  abstract = {We present here a new restoring algorithm that is based on use of a ``median-window filter.'' This is intrinsically a nonlinear operation that, when cyclically used with any linear restoring algorithm, can typically enhance edge gradients by a factor of 5:1 with nearly a complete absence of Gibbs oscillation.},
  language = {EN},
  number = {3},
  journal = {JOSA},
  author = {Frieden, B. R.},
  month = mar,
  year = {1976},
  pages = {280-283},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/MI4AT492/abstract.html}
}

@inproceedings{papadomanolaki_patch-based_2017,
  title = {Patch-Based Deep Learning Architectures for Sparse Annotated Very High Resolution Datasets},
  doi = {10.1109/JURSE.2017.7924538},
  abstract = {In this paper, we compare the performance of different deep-learning architectures under a patch-based framework for the semantic labeling of sparse annotated urban scenes from very high resolution images. In particular, the simple convolutional network ConvNet, the AlexNet and the VGG models have been trained and tested on the publicly available, multispectral, very high resolution Summer Zurich v1.0 dataset. Experiments with patches of different dimensions have been performed and compared, indicating the optimal size for the semantic segmentation of very high resolution satellite data. The overall validation and assessment indicated the robustness of the high level features that are computed with the employed deep architectures for the semantic labeling of urban scenes.},
  booktitle = {2017 {{Joint Urban Remote Sensing Event}} ({{JURSE}})},
  author = {Papadomanolaki, M. and Vakalopoulou, M. and Karantzalos, K.},
  month = mar,
  year = {2017},
  keywords = {Labeling,geophysical techniques,semantic labeling,Computer architecture,Computational modeling,Machine learning,Semantics,Training,semantic segmentation,Remote sensing,very high resolution images,multispectral dataset,patch-based deep learning architectures,patch-based framework,simple convolutional network,sparse annotated urban scenes,very high resolution dataset sparse annotation,very high resolution satellite data,very high resolution Summer Zurich dataset},
  pages = {1-4},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/2J3BIY69/7924538.html}
}

@inproceedings{vakalopoulou_building_2015,
  title = {Building Detection in Very High Resolution Multispectral Data with Deep Learning Features},
  doi = {10.1109/IGARSS.2015.7326158},
  abstract = {The automated man-made object detection and building extraction from single satellite images is, still, one of the most challenging tasks for various urban planning and monitoring engineering applications. To this end, in this paper we propose an automated building detection framework from very high resolution remote sensing data based on deep convolutional neural networks. The core of the developed method is based on a supervised classification procedure employing a very large training dataset. An MRF model is then responsible for obtaining the optimal labels regarding the detection of scene buildings. The experimental results and the performed quantitative validation indicate the quite promising potentials of the developed approach.},
  booktitle = {2015 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}} ({{IGARSS}})},
  author = {Vakalopoulou, M. and Karantzalos, K. and Komodakis, N. and Paragios, N.},
  month = jul,
  year = {2015},
  keywords = {neural nets,object detection,deep convolutional networks,Buildings,satellite images,Satellites,Image resolution,Machine learning,Training,image classification,remote sensing,town and country planning,Feature extraction,Remote sensing,deep convolutional neural networks,Support vector machines,image resolution,automated building detection framework,automated man-made object detection,building extraction,buildings (structures),deep learning features,extraction,ImageNet,man made objects,MRF model,optimal labels,quantitative validation,scene building detection,supervised classification procedure,training dataset,urban monitoring engineering applications,urban planning applications,very high resolution multispectral data,very high resolution remote sensing data},
  pages = {1873-1876},
  file = {/home/naudeber/Bibliographie//IEEE/2015/Vakalopoulou et al 2015 - Building detection in very high resolution multispectral data with deep.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/KG6L5R94/7326158.html}
}

@inproceedings{zhou_scene_2017,
  title = {Scene {{Parsing}} through {{ADE20K Dataset}}},
  doi = {10.1109/CVPR.2017.544},
  abstract = {Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the communitys efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the ADE20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A scene parsing benchmark is built upon the ADE20K with 150 object and stuff classes included. Several segmentation baseline models are evaluated on the benchmark. A novel network design called Cascade Segmentation Module is proposed to parse a scene into stuff, objects, and object parts in a cascade and improve over the baselines. We further show that the trained scene parsing networks can lead to applications such as image content removal and scene synthesis1.},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhou, B. and Zhao, H. and Puig, X. and Fidler, S. and Barriuso, A. and Torralba, A.},
  month = jul,
  year = {2017},
  keywords = {Labeling,image representation,learning (artificial intelligence),Visualization,object recognition,Neural networks,computer vision,Semantics,image segmentation,Image segmentation,Computer vision,object segmentation,ADE20K dataset,dense annotations,detailed annotations,image datasets,object parts,scene parsing benchmark,Sun,trained scene parsing networks},
  pages = {5122-5130},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/LIFS3QTA/8100027.html}
}

@incollection{huang_deep_2016,
  series = {Lecture Notes in Computer Science},
  title = {Deep {{Networks}} with {{Stochastic Depth}}},
  isbn = {978-3-319-46492-3 978-3-319-46493-0},
  abstract = {Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91 \% on CIFAR-10).},
  language = {en},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2016},
  publisher = {{Springer, Cham}},
  author = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q.},
  month = oct,
  year = {2016},
  pages = {646-661},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WDB9V6K4/Huang et al. - 2016 - Deep Networks with Stochastic Depth.pdf},
  doi = {10.1007/978-3-319-46493-0_39}
}

@inproceedings{veit_residual_2016,
  address = {USA},
  series = {NIPS'16},
  title = {Residual {{Networks Behave Like Ensembles}} of {{Relatively Shallow Networks}}},
  isbn = {978-1-5108-3881-9},
  abstract = {In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}}},
  publisher = {{Curran Associates Inc.}},
  author = {Veit, Andreas and Wilber, Michael and Belongie, Serge},
  year = {2016},
  pages = {550--558},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/M62EK43K/Veit et al. - 2016 - Residual Networks Behave Like Ensembles of Relativ.pdf}
}

@phdthesis{maggiori_learning_2017,
  title = {Learning Approaches for Large-Scale Remote Sensing Image Classification},
  abstract = {The analysis of airborne and satellite images is one of the core subjects in remote sensing. In recent years, technological developments have facilitated the availability of large-scale sources of data, which cover significant extents of the earth's surface, often at impressive spatial resolutions. In addition to the evident computational complexity issues that arise, one of the current challenges is to handle the variability in the appearance of the objects across different geographic regions. For this, it is necessary to design classification methods that go beyond the analysis of individual pixel spectra, introducing higher-level contextual information in the process. In this thesis, we first propose a method to perform classification with shape priors, based on the optimization of a hierarchical subdivision data structure. We then delve into the use of the increasingly popular convolutional neural networks (CNNs) to learn deep hierarchical contextual features. We investigate CNNs from multiple angles, in order to address the different points required to adapt them to our problem. Among other subjects, we propose different solutions to output high-resolution classification maps and we study the acquisition of training data. We also created a dataset of aerial images over dissimilar locations, and assess the generalization capabilities of CNNs. Finally, we propose a technique to polygonize the output classification maps, so as to integrate them into operational geographic information systems, thus completing the typical processing pipeline observed in a wide number of applications. Throughout this thesis, we experiment on hyperspectral, atellite and aerial images, with scalability, generalization and applicability goals in mind.},
  language = {en},
  school = {Universit{\'e} C{\^o}te d'Azur},
  author = {Maggiori, Emmanuel},
  month = jun,
  year = {2017},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/6INDP93T/Maggiori - 2017 - Learning approaches for large-scale remote sensing.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/6BJQWUL2/tel-01589661.html}
}

@article{hollstein_ready--use_2016,
  title = {Ready-to-{{Use Methods}} for the {{Detection}} of {{Clouds}}, {{Cirrus}}, {{Snow}}, {{Shadow}}, {{Water}} and {{Clear Sky Pixels}} in {{Sentinel}}-2 {{MSI Images}}},
  volume = {8},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  doi = {10.3390/rs8080666},
  abstract = {Classification of clouds, cirrus, snow, shadows and clear sky areas is a crucial step in the pre-processing of optical remote sensing images and is a valuable input for their atmospheric correction. The Multi-Spectral Imager on board the Sentinel-2's of the Copernicus program offers optimized bands for this task and delivers unprecedented amounts of data regarding spatial sampling, global coverage, spectral coverage, and repetition rate. Efficient algorithms are needed to process, or possibly reprocess, those big amounts of data. Techniques based on top-of-atmosphere reflectance spectra for single-pixels without exploitation of external data or spatial context offer the largest potential for parallel data processing and highly optimized processing throughput. Such algorithms can be seen as a baseline for possible trade-offs in processing performance when the application of more sophisticated methods is discussed. We present several ready-to-use classification algorithms which are all based on a publicly available database of manually classified Sentinel-2A images. These algorithms are based on commonly used and newly developed machine learning techniques which drastically reduce the amount of time needed to update the algorithms when new images are added to the database. Several ready-to-use decision trees are presented which allow to correctly label about     91 \%     of the spectra within a validation dataset. While decision trees are simple to implement and easy to understand, they offer only limited classification skill. It improves to     98 \%     when the presented algorithm based on the classical Bayesian method is applied. This method has only recently been used for this task and shows excellent performance concerning classification skill and processing performance. A comparison of the presented algorithms with other commonly used techniques such as random forests, stochastic gradient descent, or support vector machines is also given. Especially random forests and support vector machines show similar classification skill as the classical Bayesian method.},
  language = {en},
  number = {8},
  journal = {Remote Sensing},
  author = {Hollstein, Andr{\'e} and Segl, Karl and Guanter, Luis and Brell, Maximilian and Enesco, Marta},
  month = aug,
  year = {2016},
  keywords = {decision trees,machine learning,Bayesian classification,cirrus detection,cloud detection,Sentinel-2 MSI,shadow detection,snow detection},
  pages = {666},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/XNRBV953/Hollstein et al. - 2016 - Ready-to-Use Methods for the Detection of Clouds, .pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/3ZLFVU68/666.html}
}

@phdthesis{pelletier_cartographie_2017,
  title = {{Cartographie de l'occupation des sols {\`a} partir de s{\'e}ries temporelles d'images satellitaires {\`a} hautes r{\'e}solutions Identification et traitement des donn{\'e}es mal {\'e}tiquet{\'e}es}},
  abstract = {L'{\'e}tude des surfaces continentales est devenue ces derni{\`e}res ann{\'e}es un enjeu majeur {\`a} l'{\'e}chelle mondiale pour la gestion et le suivi des territoires, notamment en mati{\`e}re de consommation des terres agricoles et d'{\'e}talement urbain. Dans ce contexte, les cartes d'occupation du sol caract{\'e}risant la couverture biophysique des terres {\'e}merg{\'e}es jouent un r{\^o}le essentiel pour la cartographie des surfaces continentales. La production de ces cartes sur de grandes {\'e}tendues s'appuie sur des donn{\'e}es satellitaires qui permettent de photographier les surfaces continentales fr{\'e}quemment et {\`a} faible co{\^u}t. Le lancement de nouvelles constellations satellitaires -- Landsat-8 et Sentinel-2 -- permet depuis quelques ann{\'e}es l'acquisition de s{\'e}ries temporelles {\`a} hautes r{\'e}solutions. Ces derni{\`e}res sont utilis{\'e}es dans des processus de classification supervis{\'e}e afin de produire les cartes d'occupation du sol. L'arriv{\'e}e de ces nouvelles donn{\'e}es ouvre de nouvelles perspectives, mais questionne sur le choix des algorithmes de classification et des donn{\'e}es {\`a} fournir en entr{\'e}e du syst{\`e}me de classification. Outre les donn{\'e}es satellitaires, les algorithmes de classification supervis{\'e}e utilisent des {\'e}chantillons d'apprentissage pour d{\'e}finir leur r{\`e}gle de d{\'e}cision. Dans notre cas, ces {\'e}chantillons sont {\'e}tiquet{\'e}s, $\backslash$ie\{\} la classe associ{\'e}e {\`a} une occupation des sols est connue. Ainsi, la qualit{\'e} de la carte d'occupation des sols est directement li{\'e}e {\`a} la qualit{\'e} des {\'e}tiquettes des {\'e}chantillons d'apprentissage. Or, la classification sur de grandes {\'e}tendues n{\'e}cessite un grand nombre d'{\'e}chantillons, qui caract{\'e}rise la diversit{\'e} des paysages. Cependant, la collecte de donn{\'e}es de r{\'e}f{\'e}rence est une t{\^a}che longue et fastidieuse. Ainsi, les {\'e}chantillons d'apprentissage sont bien souvent extraits d'anciennes bases de donn{\'e}es pour obtenir un nombre cons{\'e}quent d'{\'e}chantillons sur l'ensemble de la surface {\`a} cartographier. Cependant, l'utilisation de ces anciennes donn{\'e}es pour classer des images satellitaires plus r{\'e}centes conduit {\`a} la pr{\'e}sence de nombreuses donn{\'e}es mal {\'e}tiquet{\'e}es parmi les {\'e}chantillons d'apprentissage. Malheureusement, l'utilisation de ces {\'e}chantillons mal {\'e}tiquet{\'e}s dans le processus de classification peut engendrer des erreurs de classification, et donc une d{\'e}t{\'e}rioration de la qualit{\'e} de la carte produite. L'objectif g{\'e}n{\'e}ral de la th{\`e}se vise {\`a} am{\'e}liorer la classification des nouvelles s{\'e}ries temporelles d'images satellitaires {\`a} hautes r{\'e}solutions. Le premier objectif consiste {\`a} d{\'e}terminer la stabilit{\'e} et la robustesse des m{\'e}thodes de classification sur de grandes {\'e}tendues. Plus particuli{\`e}rement, les travaux portent sur l'analyse d'algorithmes de classification et la sensibilit{\'e} de ces algorithmes vis-{\`a}-vis de leurs param{\`e}tres et des donn{\'e}es en entr{\'e}e du syst{\`e}me de classification. De plus, la robustesse de ces algorithmes {\`a} la pr{\'e}sence des donn{\'e}es imparfaites est {\'e}tudi{\'e}e. Le second objectif s'int{\'e}resse aux erreurs pr{\'e}sentes dans les donn{\'e}es d'apprentissage, connues sous le nom de donn{\'e}es mal {\'e}tiquet{\'e}es. Dans un premier temps, des m{\'e}thodes de d{\'e}tection de donn{\'e}es mal {\'e}tiquet{\'e}es sont propos{\'e}es et {\'e}tudi{\'e}es. Dans un second temps, un cadre m{\'e}thodologique est propos{\'e} afin de prendre en compte les donn{\'e}es mal {\'e}tiquet{\'e}es dans le processus de classification. L'objectif est de r{\'e}duire l'influence des donn{\'e}es mal {\'e}tiquet{\'e}es sur les performances de l'algorithme de classification, et donc d'am{\'e}liorer la carte d'occupation des sols produite.},
  language = {fr},
  school = {Universit{\'e} Toulouse 3 Paul Sabatier (UT3 Paul Sabatier)},
  author = {Pelletier, Charlotte},
  month = dec,
  year = {2017},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/7XC32FSU/Pelletier - 2017 - Cartographie de l’occupation des sols à partir de .pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/99X4Y5A7/tel-01665121v2.html}
}

@book{thomas_mapping_2017,
  title = {Mapping {{Learning}}},
  abstract = {Mapping Learning se veut un outil pour faciliter l'utilisation d'algorithmes de machine learning, en vue de produire des cartes, mais pas seulement. De nombreux algorithmes sont disponibles, et peuvent {\^e}tre appliqu{\'e}s {\`a} des donn{\'e}es de diff{\'e}rentes natures (tableau, image, donn{\'e}es vectorielles). Application open-source, Mapping Learning vise {\`a} devenir un projet {\'e}ducatif sur le machine learning mais aussi le d{\'e}veloppement {\`a} l'aide de Python ( scikit-learn, mlpy, Gdal/Ogr/Osgeo).},
  publisher = {{CNRS, r{\'e}seau DEVLOG}},
  author = {Thomas, Alban and Corpetti, Thomas and Corgne, Samuel S. and Garnier, Laurent and Tavenard, Romain and Oszwald, Johan},
  month = jul,
  year = {2017},
  keywords = {Python,apprentissage statistique,Cartographie,télédétection},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FMXPNGKE/Thomas et al. - 2017 - Mapping Learning.pdf},
  note = {Published: JDEVs, les Journ{\'e}es du DEVeloppement logiciel}
}

@article{pham_feature_2018,
  title = {Feature {{Profiles}} from {{Attribute Filtering}} for {{Classification}} of {{Remote Sensing Images}}},
  volume = {11},
  issn = {1939-1404},
  doi = {10.1109/JSTARS.2017.2773367},
  abstract = {This paper proposes a novel extension of morphological attribute profiles (APs) for classification of remote sensing data. In standard AP-based approaches, an input image is characterized by a set of filtered images achieved from the sequential application of attribute filters based on the image tree representation. Hence, only pixel values (i.e. gray levels) are employed to form the output profiles. In this paper, during the attribute filtering process, instead of outputting the gray levels, we propose to extract both statistical and geometrical features from the connected components (w.r.t. tree nodes) to build the so-called feature profiles (FPs). These features are expected to better characterize the object or region encoded by each connected component. They are then exploited to classify remote sensing images. To evaluate the effectiveness of the proposed approach, supervised classification using the random forest classifier is conducted on the panchromatic Reykjavik image as well as the hyperspectral Pavia University data. Experimental results show the FPs provide a competitive performance compared against standard APs and thus constitute a promising alternative.},
  number = {1},
  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  author = {Pham, M. T. and Aptoula, E. and Lef{\`e}vre, S.},
  month = jan,
  year = {2018},
  keywords = {trees (mathematics),geophysical image processing,Vegetation,remote sensing imagery,Shape,feature extraction,image classification,remote sensing,supervised classification,Standards,Feature extraction,Remote sensing,remote sensing images,random forest,Principal component analysis,morphological attribute profiles,attribute filtering process,attribute filters,Attribute profiles (APs),feature profiles,feature profiles (FPs),filtered images,geometrical features,gray levels,image tree representation,input image,output profiles,panchromatic Reykjavik image,remote sensing data,standard AP-based approaches,statistical features,tree nodes},
  pages = {249-256},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/BHXB6WRD/Pham et al. - 2018 - Feature Profiles from Attribute Filtering for Clas.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/CW4UGV8I/8118175.html}
}

@inproceedings{cui_combining_2016,
  title = {Combining Multiscale Features for Classification of Hyperspectral Images: {{A}} Sequence-Based Kernel Approach},
  shorttitle = {Combining Multiscale Features for Classification of Hyperspectral Images},
  doi = {10.1109/WHISPERS.2016.8071671},
  abstract = {Nowadays, hyperspectral image classification widely copes with spatial information to improve accuracy. One of the most popular way to integrate such information is to extract hierarchical features from a multiscale segmentation. In the classification context, the extracted features are commonly concatenated into a long vector (also called stacked vector), on which is applied a conventional vector-based machine learning technique (e.g. SVM with Gaussian kernel). In this paper, we rather propose to use a sequence structured kernel: the spectrum kernel. We show that the conventional stacked vector-based kernel is actually a special case of this kernel. Experiments conducted on various publicly available hyperspectral datasets illustrate the improvement of the proposed kernel w.r.t. conventional ones using the same hierarchical spatial features.},
  booktitle = {2016 8th {{Workshop}} on {{Hyperspectral Image}} and {{Signal Processing}}: {{Evolution}} in {{Remote Sensing}} ({{WHISPERS}})},
  author = {Cui, Y. and Chapel, L. and Lef{\`e}vre, S.},
  month = aug,
  year = {2016},
  keywords = {support vector machines,learning (artificial intelligence),geophysical image processing,hyperspectral imaging,Kernel,Training,feature extraction,image classification,image segmentation,spatial information,hyperspectral image classification,SVM,Standards,Feature extraction,Hyperspectral imaging,Support vector machines,multiscale segmentation,conventional ones,conventional stacked vector,conventional vector-based machine learning technique,Gaussian kernel,hierarchical features,hierarchical spatial features,Image representation,multiscale features,multiscale image representation,publicly available hyperspectral datasets,sequence structured kernel,sequence-based kernel approach,spectrum kernel,Spectrum kernel,stacked vector},
  pages = {1-5},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HCKJV66M/Cui et al. - 2016 - Combining multiscale features for classification o.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/ZW9TA3A9/8071671.html}
}

@article{broxton_global_2014,
  title = {A {{Global Land Cover Climatology Using MODIS Data}}},
  volume = {53},
  issn = {1558-8424},
  doi = {10.1175/JAMC-D-13-0270.1},
  abstract = {Global land cover data are widely used in weather, climate, and hydrometeorological models. The Collection 5.1 Moderate Resolution Imaging Spectroradiometer (MODIS) Land Cover Type (MCD12Q1) product is found to have a substantial amount of interannual variability, with 40\% of land pixels showing land cover change one or more times during 2001\textendash{}10. This affects the global distribution of vegetation if any one year or many years of data are used, for example, to parameterize land processes in regional and global models. In this paper, a value-added global 0.5-km land cover climatology (a single representative map for 2001\textendash{}10) is developed by weighting each land cover type by its corresponding confidence score for each year and using the highest-weighted land cover type in each pixel in the 2001\textendash{}10 MODIS data. The climatology is validated by comparing it with the System for Terrestrial Ecosystem Parameterization database as well as additional pixels that are identified from the Google Earth proprietary software database. When compared with the data of any individual year, this climatology does not substantially alter the overall global frequencies of most land cover classes but does affect the global distribution of many land cover classes. In addition, it is validated as well as or better than the MODIS data for individual years. Also, it is based on higher-quality data and is validated better than the Global Land Cover Characteristics database, which is based on 1 year of Advanced Very High Resolution Radiometer data and represents a widely used first-generation global product.},
  number = {6},
  journal = {Journal of Applied Meteorology and Climatology},
  author = {Broxton, Patrick D. and Zeng, Xubin and {Sulla-Menashe}, Damien and Troch, Peter A.},
  month = apr,
  year = {2014},
  pages = {1593-1605},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/J6ZX2Q7B/Broxton et al. - 2014 - A Global Land Cover Climatology Using MODIS Data.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/6IEH4LYI/JAMC-D-13-0270.html}
}

@phdthesis{randrianarivo_learning_2016,
  type = {Theses},
  title = {Learning of Semantic Classes for Aerial Image Analysis},
  school = {Conservatoire national des arts et metiers - CNAM},
  author = {Randrianarivo, Hicham},
  month = dec,
  year = {2016},
  keywords = {Machine learning,Object detection,Apprentissage statistique,Contextual Model,Détection d'objets,Modèle de contexte,Segmentation sémantique,Semantic segmentation},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/JJNQ4ZMV/Randrianarivo - 2016 - Learning of semantic classes for aerial image anal.pdf}
}

@phdthesis{bosilj_indexation_2016,
  title = {{Indexation et recherche d'images par arbres des coupes}},
  abstract = {Cette th{\`e}se explore l'utilisation de repr{\'e}sentations hi{\'e}rarchiques des images issues de la morphologie math{\'e}matique, les arbres des coupes, pour la recherche et la classification d'images. Diff{\'e}rents types de structures arborescentes sont analys{\'e}s et une nouvelle classification en deux superclasses est propos{\'e}e, ainsi qu'une contribution {\`a} l'indexation et {\`a} la repr{\'e}sentation de ces structures par des dendogrammes. Deux contributions {\`a} la recherche d'images sont propos{\'e}es, l'une sur la d{\'e}tection de r{\'e}gions d'int{\'e}r{\^e}t et l'autre sur la description de ces r{\'e}gions. Les r{\'e}gions MSER peuvent {\^e}tre d{\'e}tect{\'e}es par un algorithme s'appuyant sur une repr{\'e}sentation des images par arbres min et max. L'utilisation d'autres structures arborescentes sous-jacentes permet de d{\'e}tecter des r{\'e}gions pr{\'e}sentant des propri{\'e}t{\'e}s de stabilit{\'e} diff{\'e}rentes. Un nouveau d{\'e}tecteur, bas{\'e} sur les arbres des formes, est propos{\'e} et {\'e}valu{\'e} en recherche d'images. Pour la description des r{\'e}gions, le concept de spectres de formes 2D permettant de d{\'e}crire globalement une image est {\'e}tendu afin de proposer un descripteur local, au pouvoir discriminant plus puissant. Ce nouveau descripteur pr{\'e}sente de bonnes propri{\'e}t{\'e}s {\`a} la fois de compacit{\'e} et d'invariance {\`a} la rotation et {\`a} la translation. Une attention particuli{\`e}re a {\'e}t{\'e} port{\'e}e {\`a} la pr{\'e}servation de l'invariance {\`a} l'{\'e}chelle. Le descripteur est {\'e}valu{\'e} {\`a} la fois en classification d'images et en recherche d'images satellitaires. Enfin, une technique de simplification des arbres de coupes est pr{\'e}sent{\'e}e, qui permet {\`a} l'utilisateur de r{\'e}{\'e}valuer les mesures du niveau d'agr{\'e}gation des r{\'e}gions impos{\'e} par les arbres des coupes.},
  language = {fr},
  school = {Universit{\'e} de Bretagne Sud},
  author = {Bosilj, Petra},
  month = jan,
  year = {2016},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/MNQIQTAF/Bosilj - 2016 - Indexation et recherche d’images par arbres des co.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/3VABQXTD/tel-01362165.html}
}

@article{blaschke_object_2010,
  title = {Object Based Image Analysis for Remote Sensing},
  volume = {65},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2009.06.004},
  abstract = {Remote sensing imagery needs to be converted into tangible information which can be utilised in conjunction with other data sets, often within widely used Geographic Information Systems (GIS). As long as pixel sizes remained typically coarser than, or at the best, similar in size to the objects of interest, emphasis was placed on per-pixel analysis, or even sub-pixel analysis for this conversion, but with increasing spatial resolutions alternative paths have been followed, aimed at deriving objects that are made up of several pixels. This paper gives an overview of the development of object based methods, which aim to delineate readily usable objects from imagery while at the same time combining image processing and GIS functionalities in order to utilize spectral and contextual information in an integrative way. The most common approach used for building objects is image segmentation, which dates back to the 1970s. Around the year 2000 GIS and image processing started to grow together rapidly through object based image analysis (OBIA - or GEOBIA for geospatial object based image analysis). In contrast to typical Landsat resolutions, high resolution images support several scales within their images. Through a comprehensive literature review several thousand abstracts have been screened, and more than 820 OBIA-related articles comprising 145 journal papers, 84 book chapters and nearly 600 conference papers, are analysed in detail. It becomes evident that the first years of the OBIA/GEOBIA developments were characterised by the dominance of `grey' literature, but that the number of peer-reviewed journal articles has increased sharply over the last four to five years. The pixel paradigm is beginning to show cracks and the OBIA methods are making considerable progress towards a spatially explicit information extraction workflow, such as is required for spatial planning as well as for many monitoring programmes.},
  number = {1},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  author = {Blaschke, T.},
  month = jan,
  year = {2010},
  keywords = {GEOBIA,GIScience,Multiscale image analysis,OBIA,Object based image analysis},
  pages = {2-16},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/QU6JYTGK/Blaschke - 2010 - Object based image analysis for remote sensing.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/AW6HCNFZ/S0924271609000884.html}
}

@article{luus_multiview_2015,
  title = {Multiview {{Deep Learning}} for {{Land}}-{{Use Classification}}},
  volume = {12},
  issn = {1545-598X},
  doi = {10.1109/LGRS.2015.2483680},
  abstract = {A multiscale input strategy for multiview deep learning is proposed for supervised multispectral land-use classification, and it is validated on a well-known data set. The hypothesis that simultaneous multiscale views can improve composition-based inference of classes containing size-varying objects compared to single-scale multiview is investigated. The end-to-end learning system learns a hierarchical feature representation with the aid of convolutional layers to shift the burden of feature determination from hand-engineering to a deep convolutional neural network (DCNN). This allows the classifier to obtain problem-specific features that are optimal for minimizing the multinomial logistic regression objective, as opposed to user-defined features which trade optimality for generality. A heuristic approach to the optimization of the DCNN hyperparameters is used, based on empirical performance evidence. It is shown that a single DCNN can be trained simultaneously with multiscale views to improve prediction accuracy over multiple single-scale views. Competitive performance is achieved for the UC Merced data set, where the 93.48\% accuracy of multiview deep learning outperforms the 85.37\% accuracy of SIFT-based methods and the 90.26\% accuracy of unsupervised feature learning.},
  number = {12},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  author = {Luus, F. P. S. and Salmon, B. P. and van den Bergh, F. and Maharaj, B. T. J.},
  month = dec,
  year = {2015},
  keywords = {Accuracy,neural nets,learning (artificial intelligence),geophysical techniques,Neural networks,Machine learning,Training,remote sensing,Feature extraction,Remote sensing,urban areas,Neurons,unsupervised feature learning,pattern classification,composition-based in- ference,convolutional layers,DCNN hyperparameters optimization,deep convolutional neural network,end-to-end learning sys- tem,feature determination,geophysics computing,hand-engineering,heuristic approach,hierarchical feature representation,land use,multinomial logistic re- gression objective,multiple single-scale views,multiscale input strategy,Multiview Deep Learning,neural network applications,neural network architecture,problem-specific fea- tures,SIFT-based methods,size-varying objects,Storage tanks,supervised multispectral land-use classification,UC Merced data set,user-defined features,well-known data set},
  pages = {2448-2452},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/TQHPLERB/Luus et al. - 2015 - Multiview Deep Learning for Land-Use Classificatio.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/LRKZ8SPM/7307121.html}
}

@article{walter_object-based_2004,
  series = {Integration of Geodata and Imagery for Automated Refinement and Update of Spatial Databases},
  title = {Object-Based Classification of Remote Sensing Data for Change Detection},
  volume = {58},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2003.09.007},
  abstract = {In this paper, a change detection approach based on an object-based classification of remote sensing data is introduced. The approach classifies not single pixels but groups of pixels that represent already existing objects in a GIS database. The approach is based on a supervised maximum likelihood classification. The multispectral bands grouped by objects and very different measures that can be derived from multispectral bands represent the n-dimensional feature space for the classification. The training areas are derived automatically from the geographical information system (GIS) database. After an introduction into the general approach, different input channels for the classification are defined and discussed. The results of a test on two test areas are presented. Afterwards, further measures, which can improve the result of the classification and enable the distinction between more land-use classes than with the introduced approach, are presented.},
  number = {3},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  author = {Walter, Volker},
  month = jan,
  year = {2004},
  keywords = {classification,data fusion,change detection,object-oriented image analysis},
  pages = {225-238},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/2M4GXTNS/Walter - 2004 - Object-based classification of remote sensing data.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/K9BCSIS7/S0924271603000595.html}
}

@article{sevo_convolutional_2016,
  title = {Convolutional {{Neural Network Based Automatic Object Detection}} on {{Aerial Images}}},
  volume = {13},
  issn = {1545-598X},
  doi = {10.1109/LGRS.2016.2542358},
  abstract = {We are witnessing daily acquisition of large amounts of aerial and satellite imagery. Analysis of such large quantities of data can be helpful for many practical applications. In this letter, we present an automatic content-based analysis of aerial imagery in order to detect and mark arbitrary objects or regions in high-resolution images. For that purpose, we proposed a method for automatic object detection based on a convolutional neural network. A novel two-stage approach for network training is implemented and verified in the tasks of aerial image classification and object detection. First, we tested the proposed training approach using UCMerced data set of aerial images and achieved accuracy of approximately 98.6\%. Second, the method for automatic object detection was implemented and verified. For implementation on GPGPU, a required processing time for one aerial image of size 5000 \texttimes{} 5000 pixels was around 30 s.},
  number = {5},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  author = {{\v S}evo, I. and Avramovi{\'c}, A.},
  month = may,
  year = {2016},
  keywords = {neural nets,object detection,convolutional neural network,Image color analysis,classification,Image resolution,Training,image classification,remote sensing,Urban areas,graphics processing units,Feature extraction,Remote sensing,Object detection,satellite imagery,aerial image classification,Aerial images,automatic content-based aerial imagery analysis,automatic object detection,convolutional neural networks (cNNs),general-purpose GPU,GPGPU,graphics processing unit,high-resolution image,network training,UCMerced data set},
  pages = {740-744},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/DUA9R64T/Ševo et Avramović - 2016 - Convolutional Neural Network Based Automatic Objec.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/3MSKFWHH/7447728.html}
}

@article{michel_stable_2015,
  title = {Stable {{Mean}}-{{Shift Algorithm}} and {{Its Application}} to the {{Segmentation}} of {{Arbitrarily Large Remote Sensing Images}}},
  volume = {53},
  issn = {0196-2892},
  doi = {10.1109/TGRS.2014.2330857},
  abstract = {Segmentation of real-world remote sensing images is challenging because of the large size of those data, particularly for very high resolution imagery. However, a lot of high-level remote sensing methods rely on segmentation at some point and are therefore difficult to assess at full image scale, for real remote sensing applications. In this paper, we define a new property called stability of segmentation algorithms and demonstrate that piece- or tile-wise computation of a stable segmentation algorithm can be achieved with identical results with respect to processing the whole image at once. We also derive a technique to empirically estimate the stability of a given segmentation algorithm and apply it to four different algorithms. Among those algorithms, the mean-shift algorithm is found to be quite unstable. We propose a modified version of this algorithm enforcing its stability and thus allowing for tile-wise computation with identical results. Finally, we present results of this method and discuss the various trends and applications.},
  number = {2},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  author = {Michel, J. and Youssefi, D. and Grizonnet, M.},
  month = feb,
  year = {2015},
  keywords = {geophysical image processing,image segmentation,remote sensing,Image segmentation,Remote sensing,image processing,Image processing,image resolution,Measurement,stability,arbitrarily large remote sensing image segmentation,high-level remote sensing method,mean-shift,object-based image analysis,piecewise computation,Software algorithms,Stability criteria,stability of segmentation,stable mean-shift algorithm,tile-wise computation},
  pages = {952-964},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/BFRNGIR3/Michel et al. - 2015 - Stable Mean-Shift Algorithm and Its Application to.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/IVGGF4YA/6849524.html}
}

@inproceedings{mattyus_enhancing_2015,
  title = {Enhancing {{Road Maps}} by {{Parsing Aerial Images Around}} the {{World}}},
  doi = {10.1109/ICCV.2015.197},
  abstract = {In recent years, contextual models that exploit maps have been shown to be very effective for many recognition and localization tasks. In this paper we propose to exploit aerial images in order to enhance freely available world maps. Towards this goal, we make use of OpenStreetMap and formulate the problem as the one of inference in a Markov random field parameterized in terms of the location of the road-segment centerlines as well as their width. This parameterization enables very efficient inference and returns only topologically correct roads. In particular, we can segment all OSM roads in the whole world in a single day using a small cluster of 10 computers. Importantly, our approach generalizes very well, it can be trained using only 1.5 km2 aerial imagery and produce very accurate results in any location across the globe. We demonstrate the effectiveness of our approach outperforming the state-of-the-art in two new benchmarks that we collect. We then show how our enhanced maps are beneficial for semantic segmentation of ground images.},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {M{\'a}ttyus, G. and Wang, S. and Fidler, S. and Urtasun, R.},
  month = dec,
  year = {2015},
  keywords = {Markov processes,image segmentation,aerial imagery,Robustness,semantic segmentation,Roads,Image segmentation,Computer vision,traffic engineering computing,enhancing road maps,ground images,Manuals,Markov random field,OpenStreetMap,parsing aerial images,road segment centerlines,road traffic,Topology},
  pages = {1689-1697},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/PNWTKS3I/Máttyus et al. - 2015 - Enhancing Road Maps by Parsing Aerial Images Aroun.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/I3WWLCVL/7410554.html}
}

@article{lassalle_scalable_2015,
  title = {A {{Scalable Tile}}-{{Based Framework}} for {{Region}}-{{Merging Segmentation}}},
  volume = {53},
  issn = {0196-2892},
  doi = {10.1109/TGRS.2015.2422848},
  abstract = {Processing large very high-resolution remote sensing images on resource-constrained devices is a challenging task because of the large size of these data sets. For applications such as environmental monitoring or natural resources management, complex algorithms have to be used to extract information from the images. The memory required to store the images and the data structures of such algorithms may be very high (hundreds of gigabytes) and therefore leads to unfeasibility on commonly available computers. Segmentation algorithms constitute an essential step for the extraction of objects of interest in a scene and will be the topic of the investigation in this paper. The objective of the present work is to adapt image segmentation algorithms for large amounts of data. To overcome the memory issue, large images are usually divided into smaller image tiles, which are processed independently. Region-merging algorithms do not cope well with image tiling since artifacts are present on the tile edges in the final result due to the incoherencies of the regions across the tiles. In this paper, we propose a scalable tile-based framework for region-merging algorithms to segment large images, while ensuring identical results, with respect to processing the whole image at once. We introduce the original concept of the stability margin for a tile. It allows ensuring identical results to those obtained if the whole image had been segmented without tiling. Finally, we discuss the benefits of this framework and demonstrate the scalability of this approach by applying it to real large images.},
  number = {10},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  author = {Lassalle, P. and Inglada, J. and Michel, J. and Grizonnet, M. and Malik, J.},
  month = oct,
  year = {2015},
  keywords = {Image edge detection,image segmentation,Image segmentation,Image processing,Measurement,Stability criteria,image tiles,image tiling,large images,memory issue,Merging,Partitioning algorithms,region merging,region-merging segmentation,scalability,scalable tile-based framework,stability margin},
  pages = {5473-5485},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/4HFDLKRU/Lassalle et al. - 2015 - A Scalable Tile-Based Framework for Region-Merging.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/ZQACFA6E/7101250.html}
}

@article{santurkar_how_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.11604},
  primaryClass = {cs, stat},
  title = {How {{Does Batch Normalization Help Optimization}}? ({{No}}, {{It Is Not About Internal Covariate Shift}})},
  shorttitle = {How {{Does Batch Normalization Help Optimization}}?},
  abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training. These findings bring us closer to a true understanding of our DNN training toolkit.},
  journal = {arXiv:1805.11604 [cs, stat]},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  month = may,
  year = {2018},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Learning,Statistics - Machine Learning},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/F5PMMX8S/Santurkar et al. - 2018 - How Does Batch Normalization Help Optimization (N.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9NEZU9HF/1805.html}
}

@article{zhu_deep_2017,
  title = {Deep {{Learning}} in {{Remote Sensing}}: {{A Comprehensive Review}} and {{List}} of {{Resources}}},
  volume = {5},
  issn = {2473-2397},
  shorttitle = {Deep {{Learning}} in {{Remote Sensing}}},
  doi = {10.1109/MGRS.2017.2762307},
  abstract = {Central to the looming paradigm shift toward data-intensive science, machine-learning techniques are becoming increasingly important. In particular, deep learning has proven to be both a major breakthrough and an extremely powerful tool in many fields. Shall we embrace deep learning as the key to everything? Or should we resist a black-box solution? These are controversial issues within the remote-sensing community. In this article, we analyze the challenges of using deep learning for remote-sensing data analysis, review recent advances, and provide resources we hope will make deep learning in remote sensing seem ridiculously simple. More importantly, we encourage remote-sensing scientists to bring their expertise into deep learning and use it as an implicit general model to tackle unprecedented, large-scale, influential challenges, such as climate change and urbanization.},
  number = {4},
  journal = {IEEE Geoscience and Remote Sensing Magazine},
  author = {Zhu, X. X. and Tuia, D. and Mou, L. and Xia, G. S. and Zhang, L. and Xu, F. and Fraundorfer, F.},
  month = dec,
  year = {2017},
  keywords = {Tutorials,Machine learning,remote sensing,Feature extraction,Computer vision,Remote sensing,Hyperspectral imaging,climate change,data-intensive science,looming paradigm shift,machine-learning techniques,remote-sensing data analysis},
  pages = {8-36},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8JH34YDD/Zhu et al. - 2017 - Deep Learning in Remote Sensing A Comprehensive R.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/96FIYLU6/8113128.html}
}

@article{lary_machine_2016,
  series = {Special Issue: Progress of Machine Learning in Geosciences},
  title = {Machine Learning in Geosciences and Remote Sensing},
  volume = {7},
  issn = {1674-9871},
  doi = {10.1016/j.gsf.2015.07.003},
  abstract = {Learning incorporates a broad range of complex procedures. Machine learning (ML) is a subdivision of artificial intelligence based on the biological learning process. The ML approach deals with the design of algorithms to learn from machine readable data. ML covers main domains such as data mining, difficult-to-program applications, and software applications. It is a collection of a variety of algorithms (e.g. neural networks, support vector machines, self-organizing map, decision trees, random forests, case-based reasoning, genetic programming, etc.) that can provide multivariate, nonlinear, nonparametric regression or classification. The modeling capabilities of the ML-based methods have resulted in their extensive applications in science and engineering. Herein, the role of ML as an effective approach for solving problems in geosciences and remote sensing will be highlighted. The unique features of some of the ML techniques will be outlined with a specific attention to genetic programming paradigm. Furthermore, nonparametric regression and classification illustrative examples are presented to demonstrate the efficiency of ML for tackling the geosciences and remote sensing problems.},
  number = {1},
  journal = {Geoscience Frontiers},
  author = {Lary, David J. and Alavi, Amir H. and Gandomi, Amir H. and Walker, Annette L.},
  month = jan,
  year = {2016},
  keywords = {Machine learning,Remote sensing,Classification,Geosciences,Regression},
  pages = {3-10},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HB4PR67U/Lary et al. - 2016 - Machine learning in geosciences and remote sensing.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/RJ8Z97HH/S1674987115000821.html}
}

@article{benediktsson_neural_1990,
  title = {Neural {{Network Approaches Versus Statistical Methods In Classification Of Multisource Remote Sensing Data}}},
  volume = {28},
  issn = {0196-2892},
  doi = {10.1109/TGRS.1990.572944},
  abstract = {Not Available},
  number = {4},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  author = {Benediktsson, J. A. and Swain, P. H. and Ersoy, O. K.},
  month = jul,
  year = {1990},
  keywords = {Neural networks,Satellites,Earth,Remote sensing,Pattern recognition,Bayesian methods,Data mining,Intelligent networks,Space technology,Statistical analysis},
  pages = {540-552},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/XRQF8D8P/Benediktsson et al. - 1990 - Neural Network Approaches Versus Statistical Metho.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/3L6H4BFE/572944.html}
}

@article{pal_support_2005,
  title = {Support Vector Machines for Classification in Remote Sensing},
  volume = {26},
  issn = {0143-1161},
  doi = {10.1080/01431160512331314083},
  abstract = {Support vector machines (SVM) represent a promising development in machine learning research that is not widely used within the remote sensing community. This paper reports the results of two experiments in which multi-class SVMs are compared with maximum likelihood (ML) and artificial neural network (ANN) methods in terms of classification accuracy. The two land cover classification experiments use multispectral (Landsat-7 ETM+) and hyperspectral (DAIS) data, respectively, for test areas in eastern England and central Spain. Our results show that the SVM achieves a higher level of classification accuracy than either the ML or the ANN classifier, and that the SVM can be used with small training datasets and high-dimensional data.},
  number = {5},
  journal = {International Journal of Remote Sensing},
  author = {Pal, M. and Mather, P. M.},
  month = mar,
  year = {2005},
  pages = {1007-1011},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/EKSM5SAU/Pal et Mather - 2005 - Support vector machines for classification in remo.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/YTYKI5AY/01431160512331314083.html}
}

@article{pal_random_2005,
  title = {Random Forest Classifier for Remote Sensing Classification},
  volume = {26},
  issn = {0143-1161},
  doi = {10.1080/01431160412331269698},
  abstract = {Growing an ensemble of decision trees and allowing them to vote for the most popular class produced a significant increase in classification accuracy for land cover classification. The objective of this study is to present results obtained with the random forest classifier and to compare its performance with the support vector machines (SVMs) in terms of classification accuracy, training time and user defined parameters. Landsat Enhanced Thematic Mapper Plus (ETM+) data of an area in the UK with seven different land covers were used. Results from this study suggest that the random forest classifier performs equally well to SVMs in terms of classification accuracy and training time. This study also concludes that the number of user-defined parameters required by random forest classifiers is less than the number required for SVMs and easier to define.},
  number = {1},
  journal = {International Journal of Remote Sensing},
  author = {Pal, M.},
  month = jan,
  year = {2005},
  pages = {217-222},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/DD8BNBRE/Pal - 2005 - Random forest classifier for remote sensing classi.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/37YQ5BP4/01431160412331269698.html}
}

@article{lawrence_classification_2004,
  title = {Classification of Remotely Sensed Imagery Using Stochastic Gradient Boosting as a Refinement of Classification Tree Analysis},
  volume = {90},
  issn = {0034-4257},
  doi = {10.1016/j.rse.2004.01.007},
  abstract = {Classification tree analysis (CTA) provides an effective suite of algorithms for classifying remotely sensed data, but it has the limitations of (1) not searching for optimal tree structures and (2) being adversely affected by outliers, inaccurate training data, and unbalanced data sets. Stochastic gradient boosting (SGB) is a refinement of standard CTA that attempts to minimize these limitations by (1) using classification errors to iteratively refine the trees using a random sample of the training data and (2) combining the multiple trees iteratively developed to classify the data. We compared traditional CTA results to SGB for three remote sensing based data sets, an IKONOS image from the Sierra Nevada Mountains of California, a Probe-1 hyperspectral image from the Virginia City mining district of Montana, and a series of Landsat ETM+ images from the Greater Yellowstone Ecosystem (GYE). SGB improved the overall accuracy of the IKONOS classification from 84\% to 95\% and the Probe-1 classification from 83\% to 93\%. The worst performing classes using CTA exhibited the largest increases in class accuracy using SGB. A slight decrease in overall classification accuracy resulted from the SGB analysis of the Landsat data.},
  number = {3},
  journal = {Remote Sensing of Environment},
  author = {Lawrence, Rick and Bunn, Andrew and Powell, Scott and Zambon, Michael},
  month = apr,
  year = {2004},
  keywords = {Accuracy,Classification tree analysis,Stochastic gradient boosting},
  pages = {331-336},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9G8XSFKS/Lawrence et al. - 2004 - Classification of remotely sensed imagery using st.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/5PPDKFVI/S0034425704000148.html}
}

@article{melgani_classification_2004,
  title = {Classification of Hyperspectral Remote Sensing Images with Support Vector Machines},
  volume = {42},
  issn = {0196-2892},
  doi = {10.1109/TGRS.2004.831865},
  abstract = {This paper addresses the problem of the classification of hyperspectral remote sensing images by support vector machines (SVMs). First, we propose a theoretical discussion and experimental analysis aimed at understanding and assessing the potentialities of SVM classifiers in hyperdimensional feature spaces. Then, we assess the effectiveness of SVMs with respect to conventional feature-reduction-based approaches and their performances in hypersubspaces of various dimensionalities. To sustain such an analysis, the performances of SVMs are compared with those of two other nonparametric classifiers (i.e., radial basis function neural networks and the K-nearest neighbor classifier). Finally, we study the potentially critical issue of applying binary SVMs to multiclass problems in hyperspectral data. In particular, four different multiclass strategies are analyzed and compared: the one-against-all, the one-against-one, and two hierarchical tree-based strategies. Different performance indicators have been used to support our experimental studies in a detailed and accurate way, i.e., the classification accuracy, the computational time, the stability to parameter setting, and the complexity of the multiclass architecture. The results obtained on a real Airborne Visible/Infrared Imaging Spectroradiometer hyperspectral dataset allow to conclude that, whatever the multiclass strategy adopted, SVMs are a valid and effective alternative to conventional pattern recognition approaches (feature-reduction procedures combined with a classification method) for the classification of hyperspectral remote sensing data.},
  number = {8},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  author = {Melgani, F. and Bruzzone, L.},
  month = aug,
  year = {2004},
  keywords = {support vector machines,Computer architecture,feature extraction,image classification,remote sensing,Hyperspectral sensors,Support vector machine classification,hyperspectral images,geophysical signal processing,Infrared imaging,Remote sensing,Classification,Hyperspectral imaging,Support vector machines,SVMs,data acquisition,multidimensional signal processing,pattern recognition,Airborne Visible/Infrared Imaging Spectroradiometer hyperspectral dataset,binary SVM,feature reduction,hierarchical tree-based strategies,Hughes phenomenon,hyperdimensional feature spaces,hyperspectral remote sensing images,hypersubspaces,K-nearest neighbor classifier,multiclass architecture complexity,multiclass problems,multiclass strategies,nonparametric classifiers,Performance analysis,Radial basis function networks,radial basis function neural networks,spectral analysis,Stability,SVM classifiers},
  pages = {1778-1790},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/LRGI6F2N/Melgani et Bruzzone - 2004 - Classification of hyperspectral remote sensing ima.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FTDGII6S/1323134.html}
}

@inproceedings{liu_dense_2017,
  title = {Dense {{Semantic Labeling}} of {{Very}}-{{High}}-{{Resolution Aerial Imagery}} and {{LiDAR}} with {{Fully}}-{{Convolutional Neural Networks}} and {{Higher}}-{{Order CRFs}}},
  doi = {10.1109/CVPRW.2017.200},
  abstract = {The increasing availability of very-high-resolution (VHR) aerial optical images as well as coregistered Li-DAR data opens great opportunities for improving object-level dense semantic labeling of airborne remote sensing imagery. As a result, efficient and effective multisensor fusion techniques are needed to fully exploit these complementary data modalities. Recent researches demonstrated how to process remote sensing images using pre-trained deep convolutional neural networks (DCNNs) at the feature level. In this paper, we propose a decision-level fusion approach using a probabilistic graphical model for the task of dense semantic labeling. Our proposed method first obtains two initial probabilistic labeling predictions from a fully-convolutional neural network and a linear classifier, e.g. logistic regression, respectively. These two predictions are then combined within a higher-order conditional random field (CRF). We utilize graph cut inference to estimate the final dense semantic labeling results. Higher-order CRF modeling helps to resolve fusion ambiguities by explicitly using the spatial contextual information, which can be learned from the training data. Experiments on the ISPRS 2D semantic labeling Potsdam dataset show that our proposed approach compares favorably to the state-of-the-art baseline methods.},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Liu, Y. and Piramanayagam, S. and Monteiro, S. T. and Saber, E.},
  month = jul,
  year = {2017},
  keywords = {Labeling,learning (artificial intelligence),geophysical image processing,probability,Laser radar,Neural networks,convolution,Semantics,Training,image classification,conditional random field,Optical imaging,Remote sensing,deep convolutional neural networks,image resolution,sensor fusion,optical radar,graph theory,airborne remote sensing imagery,coregistered Li-DAR data,DCNN training,dense semantic labeling,fully-convolutional neural networks,graph cut inference,higher-order CRFs,linear classifier,multisensor fusion,probabilistic graphical model,radar imaging,remote sensing by radar,very-high-resolution aerial imagery,VHR aerial optical images},
  pages = {1561-1570},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/2SZZN4SD/Liu et al. - 2017 - Dense Semantic Labeling of Very-High-Resolution Ae.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VBV9PSML/8014934.html}
}

@article{liu_path_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.01534},
  primaryClass = {cs},
  title = {Path {{Aggregation Network}} for {{Instance Segmentation}}},
  abstract = {The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with subtle extra computational overhead. Our PANet reaches the 1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. It is also state-of-the-art on MVD and Cityscapes.},
  journal = {arXiv:1803.01534 [cs]},
  author = {Liu, Shu and Qi, Lu and Qin, Haifang and Shi, Jianping and Jia, Jiaya},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/YCBWMEXU/Liu et al. - 2018 - Path Aggregation Network for Instance Segmentation.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/UVZ6SY5I/1803.html}
}

@article{g._barber_sar_1991,
  title = {{{SAR Sea Ice Discrimination Using Texture Statistics}}: {{A Multivariate Approach}}},
  volume = {57},
  shorttitle = {{{SAR Sea Ice Discrimination Using Texture Statistics}}},
  abstract = {Discrimination of sea ice classes using texture statistics derived from the conditional joint probability density functions of the grey level Co-occurrence matrix (GLCM) is reported on. Univariate and multivariate analyses are used to describe the separability of synthetic aperture radar (SAR) sea ice feature space. Impact of the conditional parameters (interpixel sampling distance) and a (orientation), and the effect of adaptive filtering, are measured for distributions arising from five GLCM texture statistics. Results show that the effects of a, and adaptive filtering are captured in each texture statistic's distribution. Multivariate classification accuracies are dependent on the number of texture sta- tistics used in computation of the discriminant functions, and whether training or cross validation sets are used. Maximum discrimination is obtained when three texture statistics are used with parallel to the look direction and at an interpixel sampling distance of one.},
  journal = {Photogrammetric Engineering and Remote Sensing},
  author = {G. Barber, David and LeDrew, Ellsworth},
  month = apr,
  year = {1991}
}

@article{fu_classification_2017,
  title = {Classification for {{High Resolution Remote Sensing Imagery Using}} a {{Fully Convolutional Network}}},
  volume = {9},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  doi = {10.3390/rs9050498},
  abstract = {As a variant of Convolutional Neural Networks (CNNs) in Deep Learning, the Fully Convolutional Network (FCN) model achieved state-of-the-art performance for natural image semantic segmentation. In this paper, an accurate classification approach for high resolution remote sensing imagery based on the improved FCN model is proposed. Firstly, we improve the density of output class maps by introducing Atrous convolution, and secondly, we design a multi-scale network architecture by adding a skip-layer structure to make it capable for multi-resolution image classification. Finally, we further refine the output class map using Conditional Random Fields (CRFs) post-processing. Our classification model is trained on 70 GF-2 true color images, and tested on the other 4 GF-2 images and 3 IKONOS true color images. We also employ object-oriented classification, patch-based CNN classification, and the FCN-8s approach on the same images for comparison. The experiments show that compared with the existing approaches, our approach has an obvious improvement in accuracy. The average precision, recall, and Kappa coefficient of our approach are 0.81, 0.78, and 0.83, respectively. The experiments also prove that our approach has strong applicability for multi-resolution image classification.},
  language = {en},
  number = {5},
  journal = {Remote Sensing},
  author = {Fu, Gang and Liu, Changjun and Zhou, Rong and Sun, Tao and Zhang, Qijian},
  month = may,
  year = {2017},
  keywords = {classification,deep learning,remote sensing,convolutional neural network (CNN),high resolution,fully convolutional network (FCN)},
  pages = {498},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/NZUVK9AW/Fu et al. - 2017 - Classification for High Resolution Remote Sensing .pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/JNXXNS6R/498.html}
}

@article{liu_comparing_2018,
  title = {Comparing Fully Convolutional Networks, Random Forest, Support Vector Machine, and Patch-Based Deep Convolutional Neural Networks for Object-Based Wetland Mapping Using Images from Small Unmanned Aircraft System},
  volume = {55},
  issn = {1548-1603},
  doi = {10.1080/15481603.2018.1426091},
  abstract = {Deep learning networks have shown great success in several computer vision applications, but its implementation in natural land cover mapping in the context of object-based image analysis (OBIA) is rarely explored area especially in terms of the impact of training sample size on the performance comparison. In this study, two representatives of deep learning networks including fully convolutional networks (FCN) and patch-based deep convolutional neural networks (DCNN), and two conventional classifiers including random forest and support vector machine were implemented within the framework of OBIA to classify seven natural land cover types. We assessed the deep learning classifiers using different training sample sizes and compared their performance with traditional classifiers. FCN was implemented using two types of training samples to investigate its ability to utilize object surrounding information.Our results indicate that DCNN may produce inferior performance compared to conventional classifiers when the training sample size is small, but it tends to show substantially higher accuracy than the conventional classifiers when the training sample size becomes large. The results also imply that FCN is more efficient in utilizing the information in the training sample than DCNN and conventional classifiers, with higher if not similar achieved accuracy regardless of sample size. DCNN and FCN tend to show similar performance for the large sample size when the training samples used for training the FCN do not contain object surrounding label information. However, with the ability of utilizing surrounding label information, FCN always achieved much higher accuracy than all the other classification methods regardless of the number of training samples.},
  number = {2},
  journal = {GIScience \& Remote Sensing},
  author = {Liu, Tao and {Abd-Elrahman}, Amr and Morton, Jon and Wilhelm, Victor L.},
  month = mar,
  year = {2018},
  keywords = {convolutional neural network,deep learning,OBIA,FCN,UAS},
  pages = {243-264},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/5FC8KUCB/Liu et al. - 2018 - Comparing fully convolutional networks, random for.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/QBAYQHIH/15481603.2018.html}
}

@article{li_lidar_2017,
  title = {Lidar {{Aboveground Vegetation Biomass Estimates}} in {{Shrublands}}: {{Prediction}}, {{Uncertainties}} and {{Application}} to {{Coarser Scales}}},
  volume = {9},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  shorttitle = {Lidar {{Aboveground Vegetation Biomass Estimates}} in {{Shrublands}}},
  doi = {10.3390/rs9090903},
  abstract = {Our study objectives were to model the aboveground biomass in a xeric shrub-steppe landscape with airborne light detection and ranging (Lidar) and explore the uncertainty associated with the models we created. We incorporated vegetation vertical structure information obtained from Lidar with ground-measured biomass data, allowing us to scale shrub biomass from small field sites (1 m subplots and 1 ha plots) to a larger landscape. A series of airborne Lidar-derived vegetation metrics were trained and linked with the field-measured biomass in Random Forests (RF) regression models. A Stepwise Multiple Regression (SMR) model was also explored as a comparison. Our results demonstrated that the important predictors from Lidar-derived metrics had a strong correlation with field-measured biomass in the RF regression models with a pseudo R2 of 0.76 and RMSE of 125 g/m2 for shrub biomass and a pseudo R2 of 0.74 and RMSE of 141 g/m2 for total biomass, and a weak correlation with field-measured herbaceous biomass. The SMR results were similar but slightly better than RF, explaining 77\textendash{}79\% of the variance, with RMSE ranging from 120 to 129 g/m2 for shrub and total biomass, respectively. We further explored the computational efficiency and relative accuracies of using point cloud and raster Lidar metrics at different resolutions (1 m to 1 ha). Metrics derived from the Lidar point cloud processing led to improved biomass estimates at nearly all resolutions in comparison to raster-derived Lidar metrics. Only at 1 m were the results from the point cloud and raster products nearly equivalent. The best Lidar prediction models of biomass at the plot-level (1 ha) were achieved when Lidar metrics were derived from an average of fine resolution (1 m) metrics to minimize boundary effects and to smooth variability. Overall, both RF and SMR methods explained more than 74\% of the variance in biomass, with the most important Lidar variables being associated with vegetation structure and statistical measures of this structure (e.g., standard deviation of height was a strong predictor of biomass). Using our model results, we developed spatially-explicit Lidar estimates of total and shrub biomass across our study site in the Great Basin, U.S.A., for monitoring and planning in this imperiled ecosystem.},
  language = {en},
  number = {9},
  journal = {Remote Sensing},
  author = {Li, Aihua and Dhakal, Shital and Glenn, Nancy F. and Spaete, Lucas P. and Shinneman, Douglas J. and Pilliod, David S. and Arkle, Robert S. and McIlroy, Susan K.},
  month = aug,
  year = {2017},
  keywords = {rangelands,machine learning,Lidar,above ground biomass,above ground carbon,drylands,semi-arid},
  pages = {903},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/3JLYF8BD/Li et al. - 2017 - Lidar Aboveground Vegetation Biomass Estimates in .pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/63GXTS55/htm.html}
}

@article{oliver_realistic_2018,
  title = {Realistic {{Evaluation}} of {{Semi}}-{{Supervised Learning Algorithms}}},
  abstract = {Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. Approaches based on deep neural networks have recently...},
  author = {Oliver, Avital and Odena, Augustus and Raffel, Colin and Cubuk, Ekin D. and Goodfellow, Ian J.},
  month = feb,
  year = {2018},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/P5S7PDFE/Oliver et al. - 2018 - Realistic Evaluation of Semi-Supervised Learning A.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/V3D7TT9N/forum.html}
}

@inproceedings{huang_large-scale_2018,
  title = {Large-Scale Semantic Classification: Outcome of the First Year of {{Inria}} Aerial Image Labeling Benchmark},
  shorttitle = {Large-Scale Semantic Classification},
  abstract = {Over the recent years, there has been an increasing interest in large-scale classification of remote sensing images. In this context, the Inria Aerial Image Labeling Benchmark has been released online in December 2016. In this paper, we discuss the outcomes of the first year of the benchmark contest, which consisted in dense labeling of aerial images into building / not building classes, covering areas of five cities not present in the training set. We present four methods with the highest numerical accuracies, all four being convolutional neural network approaches. It is remarkable that three of these methods use the U-net architecture, which has thus proven to become a new standard in image dense labeling.},
  language = {en},
  author = {Huang, Bohao and Lu, Kangkang and Audebert, Nicolas and Khalel, Andrew and Tarabalka, Yuliya and Malof, Jordan and Boulch, Alexandre and Saux, Bertrand Le and Collins, Leslie and Bradbury, Kyle and Lef{\`e}vre, S{\'e}bastien and {El-Saban}, Motaz},
  month = jul,
  year = {2018},
  keywords = {deep learning,feature extraction,image classification,image segmentation,Machine learning,remote sensing,Segmentation algorithms,Semantics,Shape,superpixels,Training},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HNQ6FHU2/Huang et al. - 2018 - Large-scale semantic classification outcome of th.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/B49LBBR9/hal-01767807.html}
}

@inproceedings{guennec_classication_2018,
  address = {Marne-la-Vall{\'e}e, France},
  title = {{Classification de donn{\'e}es LiDAR bi-spectral topo-bathym{\'e}triques par une approche multi-{\'e}chelle : Application en milieu fluvial}},
  abstract = {The monitoring of a natural/semi-natural space often requires the automatic identification of the various objects present such as vegetation, soil, water, buildings, etc. In the fluvial context, the detection of bathymetric classes (surface and mainly water bottom) is essential for many applications, such as the management of rivers or streams. In order to obtain them, the topo-bathymetric LiDAR is an interesting tool because it makes it possible to construct two 3D point clouds at a very high spatial resolution of the scenes scanned from two specific wavelengths: 1064 nm and 532 nm. The topographic aspect, by the wavelength 1064 nm, allows to recover the spatial structure of the different objects. The bathymetric aspect comes from the wavelength 532 nm entering the water and sometimes allowing access to the bottom of the river. Thus, classifying the different topo-bathymetric objects from these two points clouds is an essential issue. In this article, we extend the multi-scale approach to characterize spatial structures, which has already demonstrated its performance for point cloud analysis, with new descriptors taking into consideration the bi-spectral aspect. The classification is carried out using a technique of Random Forest which offers the possibility to analyze the most contributive descriptors in the classification and thus to better understand the impact of the bispectral data.},
  language = {fr},
  booktitle = {{CFPT Conf{\'e}rence Annuelle Fran{\c c}aise de Photogramm{\'e}trie et T{\'e}l{\'e}d{\'e}tection}},
  author = {Guennec, Arthur Le and Lef{\`e}vre, S{\'e}bastien and Lague, Dimitri and Corpetti, Thomas},
  month = jun,
  year = {2018},
  pages = {8},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VTRNFUUN/Guennec et al. - Classiﬁcation de données LiDAR bi-spectral topo-ba.pdf}
}

@article{benediktsson_classification_2003,
  title = {Classification and Feature Extraction for Remote Sensing Images from Urban Areas Based on Morphological Transformations},
  volume = {41},
  issn = {0196-2892},
  doi = {10.1109/TGRS.2003.814625},
  abstract = {Classification of panchromatic high-resolution data from urban areas using morphological and neural approaches is investigated. The proposed approach is based on three steps. First, the composition of geodesic opening and closing operations of different sizes is used in order to build a differential morphological profile that records image structural information. Although, the original panchromatic image only has one data channel, the use of the composition operations will give many additional channels, which may contain redundancies. Therefore, feature extraction or feature selection is applied in the second step. Both discriminant analysis feature extraction and decision boundary feature extraction are investigated in the second step along with a simple feature selection based on picking the largest indexes of the differential morphological profiles. Third, a neural network is used to classify the features from the second step. The proposed approach is applied in experiments on high-resolution Indian Remote Sensing 1C (IRS-1C) and IKONOS remote sensing data from urban areas. In experiments, the proposed method performs well in terms of classification accuracies. It is seen that relatively few features are needed to achieve the same classification accuracies as in the original feature space.},
  number = {9},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  author = {Benediktsson, J. A. and Pesaresi, M. and Amason, K.},
  month = sep,
  year = {2003},
  keywords = {Councils,differential morphological profile,discriminant analysis,feature extraction,Feature extraction,feature selection,high-resolution imagery,IKONOS,image classification,Image edge detection,Image processing,Image segmentation,Indian Remote Sensing 1C data,mathematical morphology,morphological transformations,Morphology,neural network,Neural networks,panchromatic high-resolution data,remote sensing,Remote sensing,remote sensing images,Shape,structural information,terrain mapping,urban areas,Urban areas},
  pages = {1940-1949},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/U5DAJKCD/Benediktsson et al. - 2003 - Classification and feature extraction for remote s.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/P6M2VVJ6/1232208.html}
}

@inproceedings{dong_accelerating_2016,
  series = {Lecture Notes in Computer Science},
  title = {Accelerating the {{Super}}-{{Resolution Convolutional Neural Network}}},
  isbn = {978-3-319-46474-9 978-3-319-46475-6},
  doi = {10.1007/978-3-319-46475-6_25},
  abstract = {As a successful deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) [1, 2] has demonstrated superior performance to the previous hand-crafted models either in speed and restoration quality. However, the high computational cost still hinders it from practical usage that demands real-time performance (24 fps). In this paper, we aim at accelerating the current SRCNN, and propose a compact hourglass-shape CNN structure for faster and better SR. We re-design the SRCNN structure mainly in three aspects. First, we introduce a deconvolution layer at the end of the network, then the mapping is learned directly from the original low-resolution image (without interpolation) to the high-resolution one. Second, we reformulate the mapping layer by shrinking the input feature dimension before mapping and expanding back afterwards. Third, we adopt smaller filter sizes but more mapping layers. The proposed model achieves a speed up of more than 40 times with even superior restoration quality. Further, we present the parameter settings that can achieve real-time performance on a generic CPU while still maintaining good performance. A corresponding transfer strategy is also proposed for fast training and testing across different upscaling factors.},
  language = {en},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2016},
  publisher = {{Springer, Cham}},
  author = {Dong, Chao and Loy, Chen Change and Tang, Xiaoou},
  month = oct,
  year = {2016},
  pages = {391-407},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/MX4B37MI/Dong et al. - 2016 - Accelerating the Super-Resolution Convolutional Ne.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/5WGXWZRY/978-3-319-46475-6_25.html}
}

@article{santurkar_how_2018-1,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.11604},
  primaryClass = {cs, stat},
  title = {How {{Does Batch Normalization Help Optimization}}? ({{No}}, {{It Is Not About Internal Covariate Shift}})},
  shorttitle = {How {{Does Batch Normalization Help Optimization}}?},
  abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training. These findings bring us closer to a true understanding of our DNN training toolkit.},
  journal = {arXiv:1805.11604 [cs, stat]},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  month = may,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/Z9D2CJ2R/Santurkar et al. - 2018 - How Does Batch Normalization Help Optimization (N.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/YRN2JELM/1805.html}
}

@article{howard_mobilenets_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.04861},
  primaryClass = {cs},
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  journal = {arXiv:1704.04861 [cs]},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  month = apr,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/UFVCKGFH/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/YEABFMGJ/1704.html}
}


