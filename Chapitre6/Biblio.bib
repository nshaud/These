
@article{simonyan_very_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.1556},
  primaryClass = {cs},
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  journal = {arXiv:1409.1556 [cs]},
  author = {Simonyan, Karen and Zisserman, Andrew},
  month = sep,
  year = {2014},
  keywords = {Ã€ lire,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//arXiv1409.1556 [cs]/2014/Simonyan Zisserman 2014 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8VM3BPQT/1409.html}
}

@article{russakovsky_imagenet_2015,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  volume = {115},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-015-0816-y},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
  language = {en},
  number = {3},
  journal = {International Journal of Computer Vision},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and {Fei-Fei}, Li},
  month = apr,
  year = {2015},
  pages = {211-252},
  file = {/home/naudeber/Bibliographie//International Journal of Computer Vision/2015/Russakovsky et al 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/3VH7ZBJ3/s11263-015-0816-y.html}
}

@article{lecun_gradient-based_1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  volume = {86},
  issn = {0018-9219},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day},
  number = {11},
  journal = {Proceedings of the IEEE},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  month = nov,
  year = {1998},
  keywords = {Pattern Recognition,principal component analysis,Neural networks,convolution,backpropagation,multilayer perceptrons,optical character recognition,2D shape variability,GTN,back-propagation,cheque reading,complex decision surface synthesis,convolutional neural network character recognizers,document recognition,document recognition systems,field extraction,gradient based learning technique,gradient-based learning,graph transformer networks,handwritten character recognition,handwritten digit recognition task,high-dimensional patterns,language modeling,multilayer neural networks,multimodule systems,performance measure minimization,segmentation recognition,Character recognition,Hidden Markov models,Multi-layer neural network,Optical character recognition software,Optical computing,Machine learning,feature extraction,Feature extraction,Pattern recognition,Principal component analysis},
  pages = {2278-2324},
  file = {/home/naudeber/Bibliographie//Proceedings of the IEEE/1998/Lecun et al 1998 - Gradient-based learning applied to document recognition.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/ANY9HKIA/726791.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HUI6PF8F/abs_all.html}
}

@article{chen_vehicle_2014,
  title = {Vehicle {{Detection}} in {{Satellite Images}} by {{Hybrid Deep Convolutional Neural Networks}}},
  volume = {11},
  issn = {1545-598X},
  doi = {10.1109/LGRS.2014.2309695},
  abstract = {Detecting small objects such as vehicles in satellite images is a difficult problem. Many features (such as histogram of oriented gradient, local binary pattern, scale-invariant feature transform, etc.) have been used to improve the performance of object detection, but mostly in simple environments such as those on roads. Kembhavi et al. proposed that no satisfactory accuracy has been achieved in complex environments such as the City of San Francisco. Deep convolutional neural networks (DNNs) can learn rich features from the training data automatically and has achieved state-of-the-art performance in many image classification databases. Though the DNN has shown robustness to distortion, it only extracts features of the same scale, and hence is insufficient to tolerate large-scale variance of object. In this letter, we present a hybrid DNN (HDNN), by dividing the maps of the last convolutional layer and the max-pooling layer of DNN into multiple blocks of variable receptive field sizes or max-pooling field sizes, to enable the HDNN to extract variable-scale features. Comparative experimental results indicate that our proposed HDNN significantly outperforms the traditional DNN on vehicle detection.},
  number = {10},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  author = {Chen, X. and Xiang, S. and Liu, C. L. and Pan, C. H.},
  month = oct,
  year = {2014},
  keywords = {neural nets,object detection,vehicles,hybrid Deep convolutional neural networks,max-pooling field sizes,max-pooling layer,satellite images,variable receptive field sizes,variable-scale feature extraction,vehicle detection,Satellites,Deep convolutional neural networks (DNNs),hybrid DNNs (HDNNs),Training,feature extraction,remote sensing},
  pages = {1797-1801},
  file = {/home/naudeber/Bibliographie//IEEE Geoscience and Remote Sensing Letters/2014/Chen et al 2014 - Vehicle Detection in Satellite Images by Hybrid Deep Convolutional Neural_3.pdf;/home/naudeber/Bibliographie//IEEE Geoscience and Remote Sensing Letters/2014/Chen et al 2014 - Vehicle Detection in Satellite Images by Hybrid Deep Convolutional Neural.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9KUQEURF/abs_all.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/B35IGN5I/abs_all.html}
}

@inproceedings{randrianarivo_contextual_2016,
  address = {Spain},
  title = {Contextual {{Discriminatively Trained Model Mixture}} for {{Object Detection}} in {{Aerial Images}}},
  booktitle = {International {{Conference}} on {{Big Data}} from {{Space}} ({{BiDS}}'16)},
  author = {Randrianarivo, H. and Saux, B. Le and Ferecatu, M. and Crucianu, M.},
  month = mar,
  year = {2016}
}

@article{srivastava_dropout_2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  volume = {15},
  shorttitle = {Dropout},
  journal = {Journal of Machine Learning Research},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  pages = {1929-1958},
  file = {/home/naudeber/Bibliographie//Journal of Machine Learning Research/2014/Srivastava et al 2014 - Dropout.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/CDBFKSEG/srivastava14a.html}
}

@article{janney_pose-invariant_2015,
  title = {Pose-Invariant Vehicle Identification in Aerial Electro-Optical Imagery},
  volume = {26},
  issn = {0932-8092, 1432-1769},
  doi = {10.1007/s00138-015-0687-9},
  abstract = {In digital airborne electro-optical imagery, the identification of objects, particularly vehicles, has an important role in wide-area search and surveillance applications. We propose an identification and pose estimation approach based on maximising the correlation of features in an image with projections of 3D models. It has been applied to imagery collected in a controlled laboratory environment as well as imagery collected during airborne field trials. The results show good discrimination between different vehicle classes, although performance is degraded by vehicle camouflage and low-resolution imagery. Our approach is scalable, in terms of database size and feature sets, and computationally efficient.},
  language = {en},
  number = {5},
  journal = {Machine Vision and Applications},
  author = {Janney, Pranam and Booth, David},
  month = jul,
  year = {2015},
  pages = {575-591},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/5R6MTC3U/s00138-015-0687-9.html}
}

@inproceedings{kamenetsky_aerial_2015,
  title = {Aerial {{Car Detection}} and {{Urban Understanding}}},
  doi = {10.1109/DICTA.2015.7371225},
  abstract = {In this work we investigate car detection from aerial imagery and explore how it can be applied to urban understanding. To perform car detection we use the rotationally-invariant Fourier HOG detector. By adding incremental changes we are able to improve its detection probability by 10\% for a range of false alarm rates. Further improvements can be made if we filter out cars that are not near known streets or inside car parks. We use the detected cars for automatic urban understanding: street estimation, car park detection and monitoring. In our experiments we were able to detect about half of all car parks in two major cities. Our method for car park monitoring allows us to find simple trends in car park usage, as well as changes in car park structure. We expect this information to be highly useful for future city planning.},
  booktitle = {2015 {{International Conference}} on {{Digital Image Computing}}: {{Techniques}} and {{Applications}} ({{DICTA}})},
  author = {Kamenetsky, D. and Sherrah, J.},
  month = nov,
  year = {2015},
  keywords = {object detection,geophysical image processing,image colour analysis,probability,Satellites,Training,feature extraction,Cities and towns,Detectors,Fourier analysis,Monitoring,aerial car detection,aerial imagery,automatic urban understanding,automobiles,car park detection,car park monitoring,car park structure,car park usage,city planning,detection probability,false alarm rates,histogram of oriented gradients,rotationally-invariant Fourier HOG detector,street estimation,town and country planning},
  pages = {1-8},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/B65FJ73W/7371225.html}
}

@article{nogueira_towards_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.01517},
  primaryClass = {cs},
  title = {Towards {{Better Exploiting Convolutional Neural Networks}} for {{Remote Sensing Scene Classification}}},
  abstract = {We present an analysis of three possible strategies for exploiting the power of existing convolutional neural networks (ConvNets) in different scenarios from the ones they were trained: full training, fine tuning, and using ConvNets as feature extractors. In many applications, especially including remote sensing, it is not feasible to fully design and train a new ConvNet, as this usually requires a considerable amount of labeled data and demands high computational costs. Therefore, it is important to understand how to obtain the best profit from existing ConvNets. We perform experiments with six popular ConvNets using three remote sensing datasets. We also compare ConvNets in each strategy with existing descriptors and with state-of-the-art baselines. Results point that fine tuning tends to be the best performing strategy. In fact, using the features from the fine-tuned ConvNet with linear SVM obtains the best results. We also achieved state-of-the-art results for the three datasets used.},
  journal = {arXiv:1602.01517 [cs]},
  author = {Nogueira, Keiller and Penatti, Ot{\'a}vio A. B. Penatti Ot{\'a}vio A. B. and Dos Santos, Jefersson A.},
  month = feb,
  year = {2016},
  keywords = {convolutional neural networks,Ã€ lire,Computer Science - Computer Vision and Pattern Recognition,deep learning,feature extraction,remote sensing,hyperspectral images,Fine-tune,Aerial scenes},
  file = {/home/naudeber/Bibliographie//arXiv1602.01517 [cs]/2016/Nogueira et al 2016 - Towards Better Exploiting Convolutional Neural Networks for Remote Sensing.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/4ZZ76SHZ/S0031320316301509.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WHHXGPXD/1602.html}
}

@article{everingham_pascal_2014,
  title = {The {{Pascal Visual Object Classes Challenge}}: {{A Retrospective}}},
  volume = {111},
  issn = {0920-5691, 1573-1405},
  shorttitle = {The {{Pascal Visual Object Classes Challenge}}},
  doi = {10.1007/s11263-014-0733-5},
  abstract = {The Pascal Visual Object Classes (VOC) challenge consists of two components: (i) a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii) an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008\textendash{}2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community's progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges.},
  language = {en},
  number = {1},
  journal = {International Journal of Computer Vision},
  author = {Everingham, Mark and Eslami, S. M. Ali and Gool, Luc Van and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
  month = jun,
  year = {2014},
  keywords = {Image Processing and Computer Vision,Computer Imaging; Vision; Pattern Recognition and Graphics,Artificial Intelligence (incl. Robotics),Pattern Recognition,segmentation,object detection,object recognition,Database,Benchmark},
  pages = {98-136},
  file = {/home/naudeber/Bibliographie//International Journal of Computer Vision/2014/Everingham et al 2014 - The Pascal Visual Object Classes Challenge.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/4CFZXJMT/10.html}
}

@article{razakarivony_vehicle_2016,
  title = {Vehicle {{Detection}} in {{Aerial Imagery}}: {{A}} Small Target Detection Benchmark},
  volume = {34},
  shorttitle = {Vehicle {{Detection}} in {{Aerial Imagery}}},
  journal = {Journal of Visual Communication and Image Representation},
  author = {Razakarivony, S{\'e}bastien and Jurie, Fr{\'e}d{\'e}ric},
  year = {2016},
  pages = {187--203},
  file = {/home/naudeber/Bibliographie//Journal of Visual Communication and Image Representation/2016/Razakarivony Jurie 2016 - Vehicle Detection in Aerial Imagery.pdf}
}

@article{dai_instance-aware_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.04412},
  primaryClass = {cs},
  title = {Instance-Aware {{Semantic Segmentation}} via {{Multi}}-Task {{Network Cascades}}},
  abstract = {Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multi-task Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems. The method described in this paper is the foundation of our submissions to the MS COCO 2015 segmentation competition, where we won the 1st place.},
  journal = {arXiv:1512.04412 [cs]},
  author = {Dai, Jifeng and He, Kaiming and Sun, Jian},
  month = dec,
  year = {2015},
  keywords = {Ã€ lire,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//arXiv1512.04412 [cs]/2015/Dai et al 2015 - Instance-aware Semantic Segmentation via Multi-task Network Cascades.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/SJTEGBFT/1512.html}
}

@article{marmanis_classification_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.01337},
  title = {Classification {{With}} an {{Edge}}: {{Improving Semantic Image Segmentation}} with {{Boundary Detection}}},
  shorttitle = {Classification {{With}} an {{Edge}}},
  abstract = {We present an end-to-end trainable deep convolutional neural network (DCNN) for semantic segmentation with built-in awareness of semantically meaningful boundaries. Semantic segmentation is a fundamental remote sensing task, and most state-of-the-art methods rely on DCNNs as their workhorse. A major reason for their success is that deep networks learn to accumulate contextual information over very large windows (receptive fields). However, this success comes at a cost, since the associated loss of effecive spatial resolution washes out high-frequency details and leads to blurry object boundaries. Here, we propose to counter this effect by combining semantic segmentation with semantically informed edge detection, thus making class-boundaries explicit in the model, First, we construct a comparatively simple, memory-efficient model by adding boundary detection to the Segnet encoder-decoder architecture. Second, we also include boundary detection in FCN-type models and set up a high-end classifier ensemble. We show that boundary detection significantly improves semantic segmentation with CNNs. Our high-end ensemble achieves $>$ 90\% overall accuracy on the ISPRS Vaihingen benchmark.},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  author = {Marmanis, Dimitrios and Schindler, Konrad and Wegner, Jan Dirk and Galliani, Silvano and Datcu, Mihai and Stilla, Uwe},
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//arXiv1612.01337 [cs]/2016/Marmanis et al 2016 - Classification With an Edge.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/XBNT2Q32/Marmanis et al_2016_Classification With an Edge.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VEPZ68S6/1612.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/Z3Z4WVNI/1612.html}
}

@inproceedings{randrianarivo_urban_2013,
  title = {Urban Structure Detection with Deformable Part-Based Models},
  doi = {10.1109/IGARSS.2013.6721126},
  abstract = {In this paper we apply the deformable part model by Felzenszwalb et al., which is at this moment the state of the art in many computer vision related tasks, to detect different types of man made structures in very high resolution aerial images - a reputedly difficult problem in our field. We test the framework on a database of crops of aerial images at a definition of 10 cm/pixel and investigate how the model performs on several classes of objects. The results show that the model can achieve reasonable performance in this context. However, depending on the type of object, there are specific issues which will have to be taken into account to build an effective semi-supervised annotation tool based on this model.},
  booktitle = {2013 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}} - {{IGARSS}}},
  author = {Randrianarivo, H. and Saux, B. Le and Ferecatu, M.},
  month = jul,
  year = {2013},
  keywords = {object detection,support vector machines,geophysical image processing,object recognition,Buildings,computer vision,Image resolution,computer vision related tasks,deformable part-based models,urban structure detection,very high resolution aerial images,Computational modeling,Deformable models,image analysis,very high resolution,feature extraction,remote sensing},
  pages = {200-203},
  file = {/home/naudeber/Bibliographie//undefined/2013/Randrianarivo et al 2013 - Urban structure detection with deformable part-based models.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/QP3KRFWC/6721126.html}
}

@inproceedings{zhou_deep_2016,
  title = {Deep Feature Representations for High-Resolution Remote Sensing Scene Classification},
  doi = {10.1109/EORSA.2016.7552825},
  abstract = {Learning powerful feature representations is of great importance in order to improve the accuracy of high-resolution remote sensing imagery (HRRSI) scene classification. Generally, traditional methods mainly focus on low-level features. Though these feature representations can achieve satisfactory performance to some extent, the performance improvement is small. More importantly, these low-level feature representations are hand-crafted features, and it is difficult to design such a holistic feature representation. Recently, convolutional neural networks (CNN) has achieved remarkable performance on several natural benchmarks. In this paper, we investigate the extraction of the deep feature representations based on the pre-trained CNN architectures for scene classification tasks. More specially, we first evaluate the pre-trained CNN architectures on a public scene dataset and then fine-tune the CNN that performs best on the dataset using the target HRRSI dataset to learn dataset-specific features. Extensive experiments on two publicly available scene datasets indicate that the deep feature representations can achieve state-of-the-art performance.},
  booktitle = {2016 4th {{International Workshop}} on {{Earth Observation}} and {{Remote Sensing Applications}} ({{EORSA}})},
  author = {Zhou, W. and Shao, Z. and Cheng, Q.},
  month = jul,
  year = {2016},
  keywords = {neural nets,image representation,convolutional neural networks,Earth,Spatial resolution,Image resolution,CNN architectures,HRRSI scene classification,deep feature representations extraction,high-resolution remote sensing imagery scene classification,Conferences,Kernel,deep feature representation,fine-tuning,scene classification,transfer learning,Training,feature extraction,image classification,remote sensing},
  pages = {338-342},
  file = {/home/naudeber/Bibliographie//undefined/2016/Zhou et al 2016 - Deep feature representations for high-resolution remote sensing scene.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/4N6FR4HS/7552825.html}
}

@article{sherrah_fully_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.02585},
  primaryClass = {cs},
  title = {Fully {{Convolutional Networks}} for {{Dense Semantic Labelling}} of {{High}}-{{Resolution Aerial Imagery}}},
  abstract = {The trend towards higher resolution remote sensing imagery facilitates a transition from land-use classification to object-level scene understanding. Rather than relying purely on spectral content, appearance-based image features come into play. In this work, deep convolutional neural networks (CNNs) are applied to semantic labelling of high-resolution remote sensing data. Recent advances in fully convolutional networks (FCNs) are adapted to overhead data and shown to be as effective as in other domains. A full-resolution labelling is inferred using a deep FCN with no downsampling, obviating the need for deconvolution or interpolation. To make better use of image features, a pre-trained CNN is fine-tuned on remote sensing data in a hybrid network context, resulting in superior results compared to a network trained from scratch. The proposed approach is applied to the problem of labelling high-resolution aerial imagery, where fine boundary detail is important. The dense labelling yields state-of-the-art accuracy for the ISPRS Vaihingen and Potsdam benchmark data sets.},
  journal = {arXiv:1606.02585 [cs]},
  author = {Sherrah, Jamie},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//arXiv1606.02585 [cs]/2016/Sherrah 2016 - Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/R8U6HVPF/1606.html}
}

@article{courty_optimal_2016,
  title = {Optimal {{Transport}} for {{Domain Adaptation}}},
  abstract = {Domain adaptation is one of the most chal- lenging tasks of modern data analytics. If the adapta- tion is done correctly, models built on a specific data representation become more robust when confronted to data depicting the same classes, but described by another observation system. Among the many strategies proposed, finding domain-invariant representations has shown excel- lent properties, in particular since it allows to train a unique classifier effective in all domains. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labeled samples of the same class in the source domain to remain close during transport. This way, we exploit at the same time the labeled samples in the source and the distributions observed in both domains. Experiments on toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches. In addition, numerical experiments show that our approach leads to better performances on domain invariant deep learning features and can be easily adapted to the semi- supervised case where few labeled samples are available in the target domain.},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Courty, Nicolas and Flamary, R{\'e}mi and Tuia, Devis and Rakotomamonjy, Alain},
  year = {2016},
  file = {/home/naudeber/Bibliographie//IEEE Transactions on Pattern Analysis and Machine Intelligence/2016/Courty et al 2016 - Optimal Transport for Domain Adaptation.pdf}
}

@article{tuia_domain_2016,
  title = {Domain {{Adaptation}} for the {{Classification}} of {{Remote Sensing Data}}: {{An Overview}} of {{Recent Advances}}},
  volume = {4},
  issn = {2168-6831},
  shorttitle = {Domain {{Adaptation}} for the {{Classification}} of {{Remote Sensing Data}}},
  doi = {10.1109/MGRS.2016.2548504},
  number = {2},
  journal = {IEEE Geoscience and Remote Sensing Magazine},
  author = {Tuia, Devis and Persello, Claudio and Bruzzone, Lorenzo},
  month = jun,
  year = {2016},
  pages = {41-57}
}

@inproceedings{gleason_vehicle_2011,
  title = {Vehicle Detection from Aerial Imagery},
  booktitle = {Robotics and {{Automation}} ({{ICRA}}), 2011 {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  author = {Gleason, Joshua and Nefian, Ara V. and Bouyssounousse, Xavier and Fong, Terry and Bebis, George},
  year = {2011},
  pages = {2065--2070},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/DVBV75I7/90.pdf}
}

@article{eikvil_classification-based_2009,
  title = {Classification-Based Vehicle Detection in High-Resolution Satellite Images},
  volume = {64},
  issn = {09242716},
  doi = {10.1016/j.isprsjprs.2008.09.005},
  language = {en},
  number = {1},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  author = {Eikvil, Line and Aurdal, Lars and Koren, Hans},
  month = jan,
  year = {2009},
  pages = {65-72},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/MNW2ZBNP/Eikvil_-_Classification-based_vehicle_detection_in_high-res.pdf}
}

@inproceedings{michel_local_2011,
  title = {Local Feature Based Supervised Object Detection: {{Sampling}}, Learning and Detection Strategies},
  isbn = {978-1-4577-1003-2},
  shorttitle = {Local Feature Based Supervised Object Detection},
  doi = {10.1109/IGARSS.2011.6049689},
  booktitle = {2011 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  publisher = {{IEEE}},
  author = {Michel, J. and Grizonnet, M. and Inglada, J. and Malik, J. and Bricier, A. and Lahlou, O.},
  month = jul,
  year = {2011},
  pages = {2381-2384}
}

@article{holt_object-based_2009,
  title = {Object-Based Detection and Classification of Vehicles from High-Resolution Aerial Photography},
  volume = {75},
  number = {7},
  journal = {Photogrammetric Engineering \& Remote Sensing},
  author = {Holt, Ashley C. and Seto, Edmund YW and Rivard, Tom and Gong, Peng},
  year = {2009},
  pages = {871--880},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/A86J7264/10.1.1.364.980.pdf}
}

@inproceedings{he_deep_2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\texttimes{} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month = jun,
  year = {2016},
  keywords = {neural nets,object detection,learning (artificial intelligence),Image recognition,Visualization,Neural networks,Training,image classification,image segmentation,CIFAR-10,COCO object detection dataset,COCO segmentation,ILSVRC \& COCO 2015 competitions,ILSVRC 2015 classification task,ImageNet dataset,ImageNet localization,ImageNet test set,VGG nets,deep residual learning,deep residual nets,deeper neural network training,residual function learning,residual nets,visual recognition tasks,Complexity theory,Degradation},
  pages = {770-778},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/KUCF9Q86/7780459.html}
}

@inproceedings{penatti_deep_2015,
  title = {Do Deep Features Generalize from Everyday Objects to Remote Sensing and Aerial Scenes Domains?},
  doi = {10.1109/CVPRW.2015.7301382},
  abstract = {In this paper, we evaluate the generalization power of deep features (ConvNets) in two new scenarios: aerial and remote sensing image classification. We evaluate experimentally ConvNets trained for recognizing everyday objects for the classification of aerial and remote sensing images. ConvNets obtained the best results for aerial images, while for remote sensing, they performed well but were outperformed by low-level color descriptors, such as BIC. We also present a correlation analysis, showing the potential for combining/fusing different ConvNets with other descriptors or even for combining multiple ConvNets. A preliminary set of experiments fusing ConvNets obtains state-of-the-art results for the well-known UCMerced dataset.},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Penatti, Ot{\'a}vio A. B. Penatti Ot{\'a}vio A. B. and Nogueira, Keiller and Santos, Jefersson A.},
  month = jun,
  year = {2015},
  keywords = {Accuracy,Image color analysis,Visualization,correlation methods,geophysical image processing,image colour analysis,object recognition,BIC,UCMerced dataset,aerial images,aerial scenes domains,correlation analysis,deep features,generalization power,low-level color descriptors,multiple ConvNets,remote sensing image classification,Correlation,Histograms,feature extraction,image classification,remote sensing},
  pages = {44-51},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/AHDUQXD8/7301382.html}
}

@inproceedings{he_mask_2017,
  title = {Mask {{R}}-{{CNN}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Computer Vision}}},
  author = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/5PC3KSS7/He et al_2017_Mask R-CNN.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/GZZ2UHUE/1703.html}
}

@inproceedings{krizhevsky_imagenet_2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Proceedings of the {{Neural Information Processing Systems}} ({{NIPS}})},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  pages = {1097--1105},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/CPT7II8U/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.html}
}

@article{yang_building_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.08946},
  primaryClass = {cs},
  title = {Building {{Extraction}} at {{Scale}} Using {{Convolutional Neural Network}}: {{Mapping}} of the {{United States}}},
  shorttitle = {Building {{Extraction}} at {{Scale}} Using {{Convolutional Neural Network}}},
  abstract = {Establishing up-to-date large scale building maps is essential to understand urban dynamics, such as estimating population, urban planning and many other applications. Although many computer vision tasks has been successfully carried out with deep convolutional neural networks, there is a growing need to understand their large scale impact on building mapping with remote sensing imagery. Taking advantage of the scalability of CNNs and using only few areas with the abundance of building footprints, for the first time we conduct a comparative analysis of four state-of-the-art CNNs for extracting building footprints across the entire continental United States. The four CNN architectures namely: branch-out CNN, fully convolutional neural network (FCN), conditional random field as recurrent neural network (CRFasRNN), and SegNet, support semantic pixel-wise labeling and focus on capturing textural information at multi-scale. We use 1-meter resolution aerial images from National Agriculture Imagery Program (NAIP) as the test-bed, and compare the extraction results across the four methods. In addition, we propose to combine signed-distance labels with SegNet, the preferred CNN architecture identified by our extensive evaluations, to advance building extraction results to instance level. We further demonstrate the usefulness of fusing additional near IR information into the building extraction framework. Large scale experimental evaluations are conducted and reported using metrics that include: precision, recall rate, intersection over union, and the number of buildings extracted. With the improved CNN model and no requirement of further post-processing, we have generated building maps for the United States. The quality of extracted buildings and processing time demonstrated the proposed CNN-based framework fits the need of building extraction at scale.},
  journal = {arXiv:1805.08946 [cs]},
  author = {Yang, Hsiuhan Lexie and Yuan, Jiangye and Lunga, Dalton and Laverdiere, Melanie and Rose, Amy and Bhaduri, Budhendra},
  month = may,
  year = {2018},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/EBCVF954/Yang et al. - 2018 - Building Extraction at Scale using Convolutional N.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/RDCQLIPA/1805.html}
}


