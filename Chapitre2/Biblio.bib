
@inproceedings{zhao_stacked_2015,
  title = {Stacked {{What}}-{{Where Auto}}-Encoders},
  abstract = {We present a novel architecture, the "stacked what-where auto-encoders" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the "what" which are fed to the next layer, and its complementary variable "where" that are fed to the corresponding layer in the generative decoder.},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}}},
  author = {Zhao, Junbo and Mathieu, Michael and Goroshin, Ross and LeCun, Yann},
  month = jun,
  year = {2015},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Learning,Statistics - Machine Learning},
  file = {/home/naudeber/Bibliographie//undefined/2015/Zhao et al 2015 - Stacked What-Where Auto-encoders.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/297AMMBV/1506.html}
}

@inproceedings{li_superpixel_2015,
  title = {Superpixel {{Segmentation Using Linear Spectral Clustering}}},
  author = {Li, Zhengqin and Chen, Jiansheng},
  year = {2015},
  pages = {1356-1363},
  file = {/home/naudeber/Bibliographie//undefined/2015/Li Chen 2015 - Superpixel Segmentation Using Linear Spectral Clustering.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8NRWMJAB/Li_Superpixel_Segmentation_Using_2015_CVPR_paper.html}
}

@inproceedings{audebert_semantic_2016,
  title = {Semantic {{Segmentation}} of {{Earth Observation Data Using Multimodal}} and {{Multi}}-Scale {{Deep Networks}}},
  doi = {10.1007/978-3-319-54181-5_12},
  abstract = {This work investigates the use of deep fully convolutional neural networks (DFCNN) for pixel-wise scene labeling of Earth Observation images. Especially, we train a variant of the SegNet architecture on remote sensing data over an urban area and study different strategies for performing accurate semantic segmentation. Our contributions are the following: (1) we transfer efficiently a DFCNN from generic everyday images to remote sensing images; (2) we introduce a multi-kernel convolutional layer for fast aggregation of predictions at multiple scales; (3) we perform data fusion from heterogeneous sensors (optical and laser) using residual correction. Our framework improves state-of-the-art accuracy on the ISPRS Vaihingen 2D Semantic Labeling dataset.},
  booktitle = {Computer {{Vision}} \textendash{} {{ACCV}} 2016},
  publisher = {{Springer, Cham}},
  author = {Audebert, Nicolas and Le Saux, Bertrand and Lef{\`e}vre, S{\'e}bastien},
  month = nov,
  year = {2016},
  pages = {180-196},
  file = {/home/naudeber/Bibliographie//Springer, Cham/2016/Audebert et al 2016 - Semantic Segmentation of Earth Observation Data Using Multimodal and.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/394SFXNP/Audebert et al_2016_Semantic Segmentation of Earth Observation Data Using Multimodal and.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/PC7RWZAX/978-3-319-54181-5_12.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/TJHU6H38/978-3-319-54181-5_12.html}
}

@incollection{vedaldi_quick_2008,
  series = {Lecture Notes in Computer Science},
  title = {Quick {{Shift}} and {{Kernel Methods}} for {{Mode Seeking}}},
  copyright = {\textcopyright{}2008 Springer Berlin Heidelberg},
  isbn = {978-3-540-88692-1 978-3-540-88693-8},
  abstract = {We show that the complexity of the recently introduced medoid-shift algorithm in clustering N points is O(N 2), with a small constant, if the underlying distance is Euclidean. This makes medoid shift considerably faster than mean shift, contrarily to what previously believed. We then exploit kernel methods to extend both mean shift and the improved medoid shift to a large family of distances, with complexity bounded by the effective rank of the resulting kernel matrix, and with explicit regularization constraints. Finally, we show that, under certain conditions, medoid shift fails to cluster data points belonging to the same mode, resulting in over-fragmentation. We propose remedies for this problem, by introducing a novel, simple and extremely efficient clustering algorithm, called quick shift, that explicitly trades off under- and over-fragmentation. Like medoid shift, quick shift operates in non-Euclidean spaces in a straightforward manner. We also show that the accelerated medoid shift can be used to initialize mean shift for increased efficiency. We illustrate our algorithms to clustering data on manifolds, image segmentation, and the automatic discovery of visual categories.},
  language = {en},
  number = {5305},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2008},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Vedaldi, Andrea and Soatto, Stefano},
  editor = {Forsyth, David and Torr, Philip and Zisserman, Andrew},
  month = oct,
  year = {2008},
  keywords = {Image Processing and Computer Vision,Computer Imaging; Vision; Pattern Recognition and Graphics,Computer Graphics,Pattern Recognition,Data Mining and Knowledge Discovery,Computer Appl. in Arts and Humanities},
  pages = {705-718},
  file = {/home/naudeber/Bibliographie//Springer Berlin Heidelberg/2008/Vedaldi Soatto 2008 - Quick Shift and Kernel Methods for Mode Seeking.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VKURP9DA/10.html},
  doi = {10.1007/978-3-540-88693-8_52}
}

@inproceedings{razavian_cnn_2014,
  title = {{{CNN Features Off}}-the-{{Shelf}}: {{An Astounding Baseline}} for {{Recognition}}},
  shorttitle = {{{CNN Features Off}}-the-{{Shelf}}},
  doi = {10.1109/CVPRW.2014.131},
  abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the OverFeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the OverFeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
  month = jun,
  year = {2014},
  keywords = {attribute detection,augmentation techniques,Birds,CNN features,convolutional nets,convolutional neural network,deep learning,feature extraction,feature representation,fine grained recognition,generic image representation,ILSVRC13,image classification,Image recognition,image representation,image retrieval,learning (artificial intelligence),linear SVM classifier,low memory footprint methods,neural nets,object image classification,OverFeat network training,scene recognition,support vector machines,Training,Vectors,visual classification tasks,Visualization},
  pages = {512-519},
  file = {/home/naudeber/Bibliographie//undefined/2014/Razavian et al 2014 - CNN Features Off-the-Shelf.pdf;/home/naudeber/Bibliographie//undefined/2014/Sharif Razavian et al 2014 - CNN features off-the-shelf.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/U3T9SK7M/articleDetails.html}
}

@techreport{achanta_slic_2010,
  title = {{{SLIC}} Superpixels},
  author = {Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurelien and Fua, Pascal and S{\"u}sstrunk, Sabine},
  year = {2010},
  file = {/home/naudeber/Bibliographie//undefined/2010/Achanta et al 2010 - Slic superpixels.pdf}
}

@article{felzenszwalb_efficient_2004,
  title = {Efficient {{Graph}}-{{Based Image Segmentation}}},
  volume = {59},
  issn = {0920-5691, 1573-1405},
  doi = {10.1023/B:VISI.0000022288.19776.77},
  abstract = {This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.},
  language = {en},
  number = {2},
  journal = {International Journal of Computer Vision},
  author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
  month = sep,
  year = {2004},
  keywords = {Artificial Intelligence (incl. Robotics),Computer Imaging; Graphics and Computer Vision,Image Processing,Automation and Robotics,clustering,perceptual organization,graph algorithm,image segmentation},
  pages = {167-181},
  file = {/home/naudeber/Bibliographie//International Journal of Computer Vision/2004/Felzenszwalb Huttenlocher 2004 - Efficient Graph-Based Image Segmentation.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/56QJGA7K/10.1023BVISI.0000022288.19776.html}
}

@inproceedings{duan_image_2015,
  title = {Image Partitioning into Convex Polygons},
  booktitle = {{{IEEE}} Conference on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Duan, Liuyun and Lafarge, Florent},
  year = {2015},
  file = {/home/naudeber/Bibliographie//undefined/2015/Duan Lafarge 2015 - Image partitioning into convex polygons.pdf}
}

@article{simonyan_very_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.1556},
  primaryClass = {cs},
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  journal = {arXiv:1409.1556 [cs]},
  author = {Simonyan, Karen and Zisserman, Andrew},
  month = sep,
  year = {2014},
  keywords = {À lire,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//arXiv1409.1556 [cs]/2014/Simonyan Zisserman 2014 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8VM3BPQT/1409.html}
}

@article{marmanis_semantic_2016,
  title = {Semantic {{Segmentation}} of {{Aerial Images}} with an {{Ensemble}} of {{CNNs}}},
  volume = {3},
  doi = {10.5194/isprs-annals-III-3-473-2016},
  abstract = {This paper describes a deep learning approach to semantic segmentation of very high resolution (aerial) images. Deep neural architectures hold the promise of end-to-end learning from raw images, making heuristic feature design obsolete. Over the last decade this idea has seen a revival, and in recent years deep convolutional neural networks (CNNs) have emerged as the method of choice for a range of image interpretation tasks like visual recognition and object detection. Still, standard CNNs do not lend themselves to per-pixel semantic segmentation, mainly because one of their fundamental principles is to gradually aggregate information over larger and larger image regions, making it hard to disentangle contributions from different pixels. Very recently two extensions of the CNN framework have made it possible to trace the semantic information back to a precise pixel position: deconvolutional network layers undo the spatial downsampling, and Fully Convolution Networks (FCNs) modify the fully connected classification layers of the network in such a way that the location of individual activations remains explicit. We design a FCN which takes as input intensity and range data and, with the help of aggressive deconvolution and recycling of early network layers, converts them into a pixelwise classification at full resolution. We discuss design choices and intricacies of such a network, and demonstrate that an ensemble of several networks achieves excellent results on challenging data such as the ISPRS semantic labeling benchmark, using only the raw data as input.},
  journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
  author = {Marmanis, Dimitrios and Wegner, Jan Dirk and Galliani, Silvano and Schindler, Konrad and Datcu, Mihai and Stilla, Uwe},
  month = jun,
  year = {2016},
  pages = {473-480},
  file = {/home/naudeber/Bibliographie//ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences/2016/Marmanis et al 2016 - Semantic Segmentation of Aerial Images with an Ensemble of CNNs.pdf}
}

@incollection{van_den_bergh_seeds_2012,
  series = {Lecture Notes in Computer Science},
  title = {{{SEEDS}}: {{Superpixels Extracted}} via {{Energy}}-{{Driven Sampling}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-33785-7 978-3-642-33786-4},
  shorttitle = {{{SEEDS}}},
  abstract = {Superpixel algorithms aim to over-segment the image by grouping pixels that belong to the same object. Many state-of-the-art superpixel algorithms rely on minimizing objective functions to enforce color homogeneity. The optimization is accomplished by sophisticated methods that progressively build the superpixels, typically by adding cuts or growing superpixels. As a result, they are computationally too expensive for real-time applications. We introduce a new approach based on a simple hill-climbing optimization. Starting from an initial superpixel partitioning, it continuously refines the superpixels by modifying the boundaries. We define a robust and fast to evaluate energy function, based on enforcing color similarity between the boundaries and the superpixel color histogram. In a series of experiments, we show that we achieve an excellent compromise between accuracy and efficiency. We are able to achieve a performance comparable to the state-of-the-art, but in real-time on a single Intel i7 CPU at 2.8GHz.},
  language = {en},
  number = {7578},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2012},
  publisher = {{Springer Berlin Heidelberg}},
  author = {{Van den Bergh}, Michael and Boix, Xavier and Roig, Gemma and de Capitani, Benjamin and Gool, Luc Van},
  editor = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
  month = oct,
  year = {2012},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Computer Graphics,Computer Imaging; Vision; Pattern Recognition and Graphics,Image Processing and Computer Vision,Pattern Recognition,segmentation,superpixels},
  pages = {13-26},
  file = {/home/naudeber/Bibliographie//Springer Berlin Heidelberg/2012/Bergh et al 2012 - SEEDS.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/K6W7IK43/10.html},
  doi = {10.1007/978-3-642-33786-4_2}
}

@inproceedings{yosinski_how_2014,
  title = {How Transferable Are Features in Deep Neural Networks?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  year = {2014},
  pages = {3320--3328},
  file = {/home/naudeber/Bibliographie//undefined/2014/Yosinski et al 2014 - How transferable are features in deep neural networks.pdf}
}

@article{baatz_multiresolution_2000,
  title = {Multiresolution {{Segmentation}}: An Optimization Approach for High Quality Multi-Scale Image Segmentation},
  journal = {Angewandte Geographische Informationsverarbeitung XII: Beitr{\"a}ge zum AGIT-Symposium Salzburg},
  author = {Baatz, Martin and Sch{\"a}pe, Arno},
  year = {2000},
  pages = {12--23},
  file = {/home/naudeber/Bibliographie//Angewandte Geographische Informationsverarbeitung XII Beiträge zum AGIT-Symposium Salzburg/2000/Baatz Schäpe 2000 - Multiresolution Segmentation.pdf}
}

@article{tilton_best_2012,
  title = {Best {{Merge Region}}-{{Growing Segmentation With Integrated Nonadjacent Region Object Aggregation}}},
  volume = {50},
  issn = {0196-2892, 1558-0644},
  doi = {10.1109/TGRS.2012.2190079},
  number = {11},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  author = {Tilton, James C. and Tarabalka, Yuliya and Montesano, Paul M. and Gofman, Emanuel},
  month = nov,
  year = {2012},
  pages = {4454-4467}
}

@article{rottensteiner_isprs_2012,
  title = {The {{ISPRS}} Benchmark on Urban Object Classification and {{3D}} Building Reconstruction},
  volume = {1},
  journal = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  author = {Rottensteiner, Franz and Sohn, Gunho and Jung, Jaewook and Gerke, Markus and Baillard, Caroline and Benitez, Sebastien and Breitkopf, Uwe},
  year = {2012},
  pages = {3},
  file = {/home/naudeber/Bibliographie//ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci/2012/Rottensteiner et al 2012 - The ISPRS benchmark on urban object classification and 3D building.pdf}
}

@inproceedings{lin_efficient_2015,
  title = {Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation},
  abstract = {Recent advances in semantic image segmentation have mostly been achieved by training deep convolutional neural networks (CNNs) for the task. We show how to improve semantic segmentation through the use of contextual information. Specifically, we explore ‘patch-patch' context and ‘patch-background' context with deep CNNs. For learning the patch-patch context between image regions, we formulate Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied to avoid repeated expensive CRF inference for back propagation. In order to capture the patch-background context, we show that a network design with traditional multi-scale image input and sliding pyramid pooling is effective for improving performance. Our experiment results set new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. Particularly, we achieve an intersection-over-union score of \$77.8\$ on the challenging PASCAL VOC 2012 dataset.},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Lin, Guosheng and Shen, Chunhua and Van Den Hengel, Anton and Reid, Ian},
  month = apr,
  year = {2015},
  keywords = {À lire,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//undefined/2015/Lin et al 2015 - Efficient piecewise training of deep structured models for semantic segmentation.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9VAWMIBH/1504.html}
}

@inproceedings{yu_multi-scale_2015,
  title = {Multi-{{Scale Context Aggregation}} by {{Dilated Convolutions}}},
  abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}}},
  author = {Yu, Fisher and Koltun, Vladlen},
  month = nov,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//undefined/2015/Yu Koltun 2015 - Multi-Scale Context Aggregation by Dilated Convolutions_2.pdf;/home/naudeber/Bibliographie//undefined/2015/Yu Koltun 2015 - Multi-Scale Context Aggregation by Dilated Convolutions.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/M4N6U752/1511.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WT6KRJ7K/1511.html}
}

@techreport{gerke_use_2015,
  title = {Use of the {{Stair Vision Library}} within the {{ISPRS 2D Semantic Labeling Benchmark}} ({{Vaihingen}})},
  institution = {{International Institute for Geo-Information Science and Earth Observation}},
  author = {Gerke, Markus},
  year = {2015},
  file = {/home/naudeber/Bibliographie//International Institute for Geo-Information Science and Earth Observation/2015/Gerke 2015 - Use of the Stair Vision Library within the ISPRS 2D Semantic Labeling Benchmark.pdf}
}

@article{nogueira_towards_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.01517},
  primaryClass = {cs},
  title = {Towards {{Better Exploiting Convolutional Neural Networks}} for {{Remote Sensing Scene Classification}}},
  abstract = {We present an analysis of three possible strategies for exploiting the power of existing convolutional neural networks (ConvNets) in different scenarios from the ones they were trained: full training, fine tuning, and using ConvNets as feature extractors. In many applications, especially including remote sensing, it is not feasible to fully design and train a new ConvNet, as this usually requires a considerable amount of labeled data and demands high computational costs. Therefore, it is important to understand how to obtain the best profit from existing ConvNets. We perform experiments with six popular ConvNets using three remote sensing datasets. We also compare ConvNets in each strategy with existing descriptors and with state-of-the-art baselines. Results point that fine tuning tends to be the best performing strategy. In fact, using the features from the fine-tuned ConvNet with linear SVM obtains the best results. We also achieved state-of-the-art results for the three datasets used.},
  journal = {arXiv:1602.01517 [cs]},
  author = {Nogueira, Keiller and Penatti, Ot{\'a}vio and {dos Santos}, Jefersson A.},
  month = feb,
  year = {2016},
  keywords = {À lire,Aerial scenes,Computer Science - Computer Vision and Pattern Recognition,convolutional neural networks,deep learning,feature extraction,Fine-tune,hyperspectral images,remote sensing},
  file = {/home/naudeber/Bibliographie//arXiv1602.01517 [cs]/2016/Nogueira et al 2016 - Towards Better Exploiting Convolutional Neural Networks for Remote Sensing.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/4ZZ76SHZ/S0031320316301509.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WHHXGPXD/1602.html}
}

@article{zhao_learning_2016,
  title = {Learning Multiscale and Deep Representations for Classifying Remotely Sensed Imagery},
  volume = {113},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2016.01.004},
  abstract = {It is widely agreed that spatial features can be combined with spectral properties for improving interpretation performances on very-high-resolution (VHR) images in urban areas. However, many existing methods for extracting spatial features can only generate low-level features and consider limited scales, leading to unpleasant classification results. In this study, multiscale convolutional neural network (MCNN) algorithm was presented to learn spatial-related deep features for hyperspectral remote imagery classification. Unlike traditional methods for extracting spatial features, the MCNN first transforms the original data sets into a pyramid structure containing spatial information at multiple scales, and then automatically extracts high-level spatial features using multiscale training data sets. Specifically, the MCNN has two merits: (1) high-level spatial features can be effectively learned by using the hierarchical learning structure and (2) multiscale learning scheme can capture contextual information at different scales. To evaluate the effectiveness of the proposed approach, the MCNN was applied to classify the well-known hyperspectral data sets and compared with traditional methods. The experimental results shown a significant increase in classification accuracies especially for urban areas.},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  author = {Zhao, Wenzhi and Du, Shihong},
  month = mar,
  year = {2016},
  keywords = {remote sensing image classification,Multiscale convolutional neural network (MCNN),deep learning,feature extraction},
  pages = {155-165},
  file = {/home/naudeber/Bibliographie//ISPRS Journal of Photogrammetry and Remote Sensing/2016/Zhao Du 2016 - Learning multiscale and deep representations for classifying remotely sensed.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/QU49WF63/S0924271616000137.html}
}

@article{marmanis_classification_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.01337},
  title = {Classification {{With}} an {{Edge}}: {{Improving Semantic Image Segmentation}} with {{Boundary Detection}}},
  shorttitle = {Classification {{With}} an {{Edge}}},
  abstract = {We present an end-to-end trainable deep convolutional neural network (DCNN) for semantic segmentation with built-in awareness of semantically meaningful boundaries. Semantic segmentation is a fundamental remote sensing task, and most state-of-the-art methods rely on DCNNs as their workhorse. A major reason for their success is that deep networks learn to accumulate contextual information over very large windows (receptive fields). However, this success comes at a cost, since the associated loss of effecive spatial resolution washes out high-frequency details and leads to blurry object boundaries. Here, we propose to counter this effect by combining semantic segmentation with semantically informed edge detection, thus making class-boundaries explicit in the model, First, we construct a comparatively simple, memory-efficient model by adding boundary detection to the Segnet encoder-decoder architecture. Second, we also include boundary detection in FCN-type models and set up a high-end classifier ensemble. We show that boundary detection significantly improves semantic segmentation with CNNs. Our high-end ensemble achieves $>$ 90\% overall accuracy on the ISPRS Vaihingen benchmark.},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  author = {Marmanis, Dimitrios and Schindler, Konrad and Wegner, Jan Dirk and Galliani, Silvano and Datcu, Mihai and Stilla, Uwe},
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//arXiv1612.01337 [cs]/2016/Marmanis et al 2016 - Classification With an Edge.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/XBNT2Q32/Marmanis et al_2016_Classification With an Edge.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VEPZ68S6/1612.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/Z3Z4WVNI/1612.html}
}

@inproceedings{quang_efficient_2015,
  title = {An {{Efficient Framework}} for {{Pixel}}-Wise {{Building Segmentation}} from {{Aerial Images}}},
  booktitle = {Proceedings of the {{Sixth International Symposium}} on {{Information}} and {{Communication Technology}}},
  publisher = {{ACM}},
  author = {Quang, Nguyen Tien and Thuy, Nguyen Thi and Sang, Dinh Viet and Binh, Huynh Thi Thanh},
  year = {2015},
  pages = {43},
  file = {/home/naudeber/Bibliographie//ACM/2015/Quang et al 2015 - An Efficient Framework for Pixel-wise Building Segmentation from Aerial Images.pdf}
}

@article{marmanis_deep_2016,
  title = {Deep {{Learning Earth Observation Classification Using ImageNet Pretrained Networks}}},
  volume = {13},
  issn = {1545-598X},
  doi = {10.1109/LGRS.2015.2499239},
  abstract = {Deep learning methods such as convolutional neural networks (CNNs) can deliver highly accurate classification results when provided with large enough data sets and respective labels. However, using CNNs along with limited labeled data can be problematic, as this leads to extensive overfitting. In this letter, we propose a novel method by considering a pretrained CNN designed for tackling an entirely different classification problem, namely, the ImageNet challenge, and exploit it to extract an initial set of representations. The derived representations are then transferred into a supervised CNN classifier, along with their class labels, effectively training the system. Through this two-stage framework, we successfully deal with the limited-data problem in an end-to-end processing scheme. Comparative results over the UC Merced Land Use benchmark prove that our method significantly outperforms the previously best stated results, improving the overall accuracy from 83.1\% up to 92.4\%. Apart from statistical improvements, our method introduces a novel feature fusion algorithm that effectively tackles the large data dimensionality by using a simple and computationally efficient approach.},
  number = {1},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  author = {Marmanis, D. and Datcu, M. and Esch, T. and Stilla, U.},
  month = jan,
  year = {2016},
  keywords = {Adaptation models,Arrays,convolutional neural networks,Convolutional neural networks (CNNs),Data models,deep learning (DL),deep learning earth observation classification,deep learning method,end-to-end processing scheme,feature extraction,Feature extraction,fusion algorithm,geophysical techniques,ImageNet pretrained networks,land-use classification,limited labeled data,limited-data problem,neural nets,Neural networks,pretrained network,remote sensing,Remote sensing,remote sensing (RS),supervised CNN classifier,Training,UC Merced Land Use benchmark},
  pages = {105-109},
  file = {/home/naudeber/Bibliographie//IEEE Geoscience and Remote Sensing Letters/2016/Marmanis et al 2016 - Deep Learning Earth Observation Classification Using ImageNet Pretrained.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/6NHD6Y5U/Marmanis et al. - 2016 - Deep Learning Earth Observation Classification Usi.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/CDKGES4I/7342907.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/H2VNFIIW/login.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/T3TS89A6/7342907.html}
}

@techreport{boulch_dag_2015,
  title = {{{DAG}} of Convolutional Networks for Semantic Labeling},
  institution = {{Office national d'{\'e}tudes et de recherches a{\'e}rospatiales}},
  author = {Boulch, Alexandre},
  year = {2015},
  file = {/home/naudeber/Bibliographie//Office national d'études et de recherches aérospatiales/2015/Boulch 2015 - DAG of convolutional networks for semantic labeling.pdf}
}

@article{liao_competitive_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.05635},
  primaryClass = {cs},
  title = {Competitive {{Multi}}-Scale {{Convolution}}},
  abstract = {In this paper, we introduce a new deep convolutional neural network (ConvNet) module that promotes competition among a set of multi-scale convolutional filters. This new module is inspired by the inception module, where we replace the original collaborative pooling stage (consisting of a concatenation of the multi-scale filter outputs) by a competitive pooling represented by a maxout activation unit. This extension has the following two objectives: 1) the selection of the maximum response among the multi-scale filters prevents filter co-adaptation and allows the formation of multiple sub-networks within the same model, which has been shown to facilitate the training of complex learning problems; and 2) the maxout unit reduces the dimensionality of the outputs from the multi-scale filters. We show that the use of our proposed module in typical deep ConvNets produces classification results that are either better than or comparable to the state of the art on the following benchmark datasets: MNIST, CIFAR-10, CIFAR-100 and SVHN.},
  journal = {arXiv:1511.05635 [cs]},
  author = {Liao, Zhibin and Carneiro, Gustavo},
  month = nov,
  year = {2015},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Learning,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//arXiv1511.05635 [cs]/2015/Liao Carneiro 2015 - Competitive Multi-scale Convolution_2.pdf;/home/naudeber/Bibliographie//arXiv1511.05635 [cs]/2015/Liao Carneiro 2015 - Competitive Multi-scale Convolution.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/SGVESFPM/1511.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/XE24K4U4/1511.html}
}

@inproceedings{szegedy_going_2015,
  title = {Going Deeper with Convolutions},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2015},
  pages = {1--9},
  file = {/home/naudeber/Bibliographie//undefined/2015/Szegedy et al 2015 - Going deeper with convolutions_2.pdf;/home/naudeber/Bibliographie//undefined/2015/Szegedy et al 2015 - Going deeper with convolutions.pdf}
}

@inproceedings{ioffe_batch_2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  pages = {448-456},
  file = {/home/naudeber/Bibliographie//undefined/2015/Ioffe Szegedy 2015 - Batch Normalization.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/SZXJDI3V/ioffe15.html}
}

@inproceedings{chatfield_return_2014,
  title = {Return of the {{Devil}} in the {{Details}}: {{Delving Deep}} into {{Convolutional Nets}}},
  isbn = {978-1-901725-52-0},
  shorttitle = {Return of the {{Devil}} in the {{Details}}},
  doi = {10.5244/C.28.6},
  language = {en},
  booktitle = {Proceedings of the {{British Machine Vision Conference}}},
  publisher = {{British Machine Vision Association}},
  author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2014},
  pages = {6.1-6.12}
}

@article{campos-taberner_processing_2016,
  title = {Processing of {{Extremely High}}-{{Resolution LiDAR}} and {{RGB Data}}: {{Outcome}} of the 2015 {{IEEE GRSS Data Fusion Contest Part A}}: 2-{{D Contest}}},
  volume = {9},
  issn = {1939-1404},
  shorttitle = {Processing of {{Extremely High}}-{{Resolution LiDAR}} and {{RGB Data}}},
  doi = {10.1109/JSTARS.2016.2569162},
  abstract = {In this paper, we discuss the scientific outcomes of the 2015 data fusion contest organized by the Image Analysis and Data Fusion Technical Committee (IADF TC) of the IEEE Geoscience and Remote Sensing Society (IEEE GRSS). As for previous years, the IADF TC organized a data fusion contest aiming at fostering new ideas and solutions for multisource studies. The 2015 edition of the contest proposed a multiresolution and multisensorial challenge involving extremely high-resolution RGB images and a three-dimensional (3-D) LiDAR point cloud. The competition was framed in two parallel tracks, considering 2-D and 3-D products, respectively. In this paper, we discuss the scientific results obtained by the winners of the 2-D contest, which studied either the complementarity of RGB and LiDAR with deep neural networks (winning team) or provided a comprehensive benchmarking evaluation of new classification strategies for extremely high-resolution multimodal data (runner-up team). The data and the previously undisclosed ground truth will remain available for the community and can be obtained at http://www.grss-ieee.org/community/technical-committees/data-fusion/2015-ieee-grss-data-fusion-contest/. The 3-D part of the contest is discussed in the Part-B paper [1].},
  number = {12},
  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  author = {{Campos-Taberner}, Manuel and {Romero-Soriano}, Adriana and Gatta, Carlo and {Camps-Valls}, Gustau and Lagrange, Adrien and Le Saux, Bertrand and Beaup{\`e}re, Anne and Boulch, Alexandre and {Chan-Hon-Tong}, Adrien and Herbin, St{\'e}phane and Randrianarivo, Hicham and Ferecatu, Marin and Shimoni, Michal and Moser, Gabriele and Tuia, Devis},
  month = dec,
  year = {2016},
  keywords = {Data integration,deep neural networks,Earth,extremely high spatial resolution,image analysis and data fusion (IADF),landcover classification,Laser radar,LiDAR,multimodal-data fusion,multiresolution-,multisource-,remote sensing,Spatial resolution,Three-dimensional displays},
  pages = {5547-5559},
  file = {/home/naudeber/Bibliographie//IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing/2016/Campos-Taberner et al 2016 - Processing of Extremely High-Resolution LiDAR and RGB Data.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FC7U9666/articleDetails.html}
}

@article{sherrah_fully_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.02585},
  primaryClass = {cs},
  title = {Fully {{Convolutional Networks}} for {{Dense Semantic Labelling}} of {{High}}-{{Resolution Aerial Imagery}}},
  abstract = {The trend towards higher resolution remote sensing imagery facilitates a transition from land-use classification to object-level scene understanding. Rather than relying purely on spectral content, appearance-based image features come into play. In this work, deep convolutional neural networks (CNNs) are applied to semantic labelling of high-resolution remote sensing data. Recent advances in fully convolutional networks (FCNs) are adapted to overhead data and shown to be as effective as in other domains. A full-resolution labelling is inferred using a deep FCN with no downsampling, obviating the need for deconvolution or interpolation. To make better use of image features, a pre-trained CNN is fine-tuned on remote sensing data in a hybrid network context, resulting in superior results compared to a network trained from scratch. The proposed approach is applied to the problem of labelling high-resolution aerial imagery, where fine boundary detail is important. The dense labelling yields state-of-the-art accuracy for the ISPRS Vaihingen and Potsdam benchmark data sets.},
  journal = {arXiv:1606.02585 [cs]},
  author = {Sherrah, Jamie},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Bibliographie//arXiv1606.02585 [cs]/2016/Sherrah 2016 - Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/R8U6HVPF/1606.html}
}

@inproceedings{nogueira_improving_2015,
  title = {Improving {{Spatial Feature Representation}} from {{Aerial Scenes}} by {{Using Convolutional Networks}}},
  isbn = {978-1-4673-7962-5},
  doi = {10.1109/SIBGRAPI.2015.39},
  publisher = {{IEEE}},
  author = {Nogueira, Keiller and Miranda, Waner O. and {dos Santos}, Jefferson A.},
  month = aug,
  year = {2015},
  pages = {289-296},
  file = {/home/naudeber/Bibliographie//IEEE/2015/Nogueira et al 2015 - Improving Spatial Feature Representation from Aerial Scenes by Using.pdf}
}

@inproceedings{neubert_compact_2014,
  title = {Compact {{Watershed}} and {{Preemptive SLIC}}: {{On Improving Trade}}-Offs of {{Superpixel Segmentation Algorithms}}.},
  shorttitle = {Compact {{Watershed}} and {{Preemptive SLIC}}},
  booktitle = {{{ICPR}}},
  author = {Neubert, Peer and Protzel, Peter},
  year = {2014},
  pages = {996--1001},
  file = {/home/naudeber/Bibliographie//undefined/2014/Neubert Protzel 2014 - Compact Watershed and Preemptive SLIC.pdf}
}

@inproceedings{neubert_superpixel_2012,
  title = {Superpixel Benchmark and Comparison},
  booktitle = {Proc. {{Forum Bildverarbeitung}}},
  author = {Neubert, Peer and Protzel, Peter},
  year = {2012},
  pages = {1--12},
  file = {/home/naudeber/Bibliographie//undefined/2012/Neubert Protzel 2012 - Superpixel benchmark and comparison_2.pdf}
}

@article{lin_refinenet_2016,
  title = {{{RefineNet}}: {{Multi}}-{{Path Refinement Networks}} with {{Identity Mappings}} for {{High}}-{{Resolution Semantic Segmentation}}},
  shorttitle = {{{RefineNet}}},
  journal = {arXiv preprint arXiv:1611.06612},
  author = {Lin, Guosheng and Milan, Anton and Shen, Chunhua and Reid, Ian},
  year = {2016},
  file = {/home/naudeber/Bibliographie//arXiv preprint arXiv1611.06612/2016/Lin et al 2016 - RefineNet.pdf}
}

@article{volpi_dense_2017,
  title = {Dense {{Semantic Labeling}} of {{Subdecimeter Resolution Images With Convolutional Neural Networks}}},
  volume = {55},
  issn = {0196-2892},
  doi = {10.1109/TGRS.2016.2616585},
  abstract = {Semantic labeling (or pixel-level land-cover classification) in ultrahigh-resolution imagery ($<$;10 cm) requires statistical models able to learn high-level concepts from spatial data, with large appearance variations. Convolutional neural networks (CNNs) achieve this goal by learning discriminatively a hierarchy of representations of increasing abstraction. In this paper, we present a CNN-based system relying on a downsample-then-upsample architecture. Specifically, it first learns a rough spatial map of high-level representations by means of convolutions and then learns to upsample them back to the original resolution by deconvolutions. By doing so, the CNN learns to densely label every pixel at the original resolution of the image. This results in many advantages, including: 1) the state-of-the-art numerical accuracy; 2) the improved geometric accuracy of predictions; and 3) high efficiency at inference time. We test the proposed system on the Vaihingen and Potsdam subdecimeter resolution data sets, involving the semantic labeling of aerial images of 9- and 5-cm resolution, respectively. These data sets are composed by many large and fully annotated tiles, allowing an unbiased evaluation of models making use of spatial information. We do so by comparing two standard CNN architectures with the proposed one: standard patch classification, prediction of local label patches by employing only convolutions, and full patch labeling by employing deconvolutions. All the systems compare favorably or outperform a state-of-the-art baseline relying on superpixels and powerful appearance descriptors. The proposed full patch labeling CNN outperforms these models by a large margin, also showing a very appealing inference time.},
  number = {2},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  author = {Volpi, Michele and Tuia, Devis},
  month = feb,
  year = {2017},
  keywords = {aerial images,classification,CNN-based system,convolutional neural network,Convolutional neural networks (CNNs),Data models,deconvolution networks,deep learning,feature extraction,geophysical image processing,image classification,Image resolution,Labeling,land cover,Machine learning,neural nets,patch labeling,pixel-level land cover classification,Potsdam subdecimeter resolution dataset,remote sensing,semantic labeling,semantic Web,Semantics,standard patch classification,statistical model,subdecimeter resolution,subdecimeter resolution images,ultrahigh-resolution imagery,Vaihingen subdecimeter resolution dataset},
  pages = {881-893},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/UVV6QJZ8/7725499.html}
}

@inproceedings{he_deep_2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\texttimes{} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month = jun,
  year = {2016},
  keywords = {neural nets,object detection,learning (artificial intelligence),Image recognition,Visualization,Neural networks,Training,image classification,image segmentation,CIFAR-10,COCO object detection dataset,COCO segmentation,ILSVRC \& COCO 2015 competitions,ILSVRC 2015 classification task,ImageNet dataset,ImageNet localization,ImageNet test set,VGG nets,deep residual learning,deep residual nets,deeper neural network training,residual function learning,residual nets,visual recognition tasks,Complexity theory,Degradation},
  pages = {770-778},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/KUCF9Q86/7780459.html}
}

@inproceedings{long_fully_2015,
  title = {Fully Convolutional Networks for Semantic Segmentation},
  doi = {10.1109/CVPR.2015.7298965},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build ``fully convolutional'' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  month = jun,
  year = {2015},
  keywords = {Adaptation models,Computer architecture,contemporary classification networks,convolution,Deconvolution,fully convolutional networks,image classification,image segmentation,inference,inference mechanisms,learning,learning (artificial intelligence),NYUDv2,Pascal VOC,pixels-to-pixels,semantic segmentation,Semantics,SIFT flow,Training,visual models},
  pages = {3431-3440},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/BGDX4IA8/7298965.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VAQSQ2JQ/7298965.html}
}

@inproceedings{paisitkriangkrai_effective_2015,
  title = {Effective Semantic Pixel Labelling with Convolutional Networks and {{Conditional Random Fields}}},
  doi = {10.1109/CVPRW.2015.7301381},
  abstract = {Large amounts of available training data and increasing computing power have led to the recent success of deep convolutional neural networks (CNN) on a large number of applications. In this paper, we propose an effective semantic pixel labelling using CNN features, hand-crafted features and Conditional Random Fields (CRFs). Both CNN and hand-crafted features are applied to dense image patches to produce per-pixel class probabilities. The CRF infers a labelling that smooths regions while respecting the edges present in the imagery. The method is applied to the ISPRS 2D semantic labelling challenge dataset with competitive classification accuracy.},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Paisitkriangkrai, Sakrapee and Sherrah, Jamie and Janney, Pranam and Van Den Hengel, Anton},
  month = jun,
  year = {2015},
  keywords = {Accuracy,classification accuracy,CNN features,computing power,conditional random fields,Convolutional networks,CRF,deep-convolutional neural networks,dense-image patches,edge detection,feature extraction,hand-crafted features,image classification,Image edge detection,image region smoothing,ISPRS 2D semantic labelling challenge dataset,Labeling,neural nets,per-pixel class probabilities,probability,semantic pixel labelling,Semantics,smoothing methods,Training,training data,Visualization},
  pages = {36-43},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/5WKQRPFF/7301381.html}
}

@inproceedings{penatti_deep_2015,
  title = {Do Deep Features Generalize from Everyday Objects to Remote Sensing and Aerial Scenes Domains?},
  doi = {10.1109/CVPRW.2015.7301382},
  abstract = {In this paper, we evaluate the generalization power of deep features (ConvNets) in two new scenarios: aerial and remote sensing image classification. We evaluate experimentally ConvNets trained for recognizing everyday objects for the classification of aerial and remote sensing images. ConvNets obtained the best results for aerial images, while for remote sensing, they performed well but were outperformed by low-level color descriptors, such as BIC. We also present a correlation analysis, showing the potential for combining/fusing different ConvNets with other descriptors or even for combining multiple ConvNets. A preliminary set of experiments fusing ConvNets obtains state-of-the-art results for the well-known UCMerced dataset.},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Penatti, Ot{\'a}vio and Nogueira, Keiller and {dos Santos}, Jefersson A.},
  month = jun,
  year = {2015},
  keywords = {Accuracy,aerial images,aerial scenes domains,BIC,Correlation,correlation analysis,correlation methods,deep features,feature extraction,generalization power,geophysical image processing,Histograms,image classification,Image color analysis,image colour analysis,low-level color descriptors,multiple ConvNets,object recognition,remote sensing,remote sensing image classification,UCMerced dataset,Visualization},
  pages = {44-51},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/AHDUQXD8/7301382.html}
}

@inproceedings{lagrange_benchmarking_2015,
  title = {Benchmarking Classification of Earth-Observation Data: {{From}} Learning Explicit Features to Convolutional Networks},
  shorttitle = {Benchmarking Classification of Earth-Observation Data},
  doi = {10.1109/IGARSS.2015.7326745},
  abstract = {In this paper, we address the task of semantic labeling of multisource earth-observation (EO) data. Precisely, we benchmark several concurrent methods of the last 15 years, from expert classifiers, spectral support-vector classification and high-level features to deep neural networks. We establish that (1) combining multisensor features is essential for retrieving some specific classes, (2) in the image domain, deep convolutional networks obtain significantly better overall performances and (3) transfer of learning from large generic-purpose image sets is highly effective to build EO data classifiers.},
  booktitle = {2015 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}} ({{IGARSS}})},
  author = {Lagrange, Adrien and Le Saux, Bertrand and Beaup{\`e}re, Anne and Boulch, Alexandre and {Chan-Hon-Tong}, Adrien and Herbin, St{\'e}phane and Randrianarivo, Hicham and Ferecatu, Marin},
  month = jul,
  year = {2015},
  keywords = {Buildings,deep convolutional networks,deep neural networks,expert classifiers,feature extraction,generic-purpose image sets,geophysical image processing,high-level features,image classification,image domain,Laser radar,learning explicit features,multisensor features,multisource earth-observation data benchmarking classification,neural nets,Neural networks,Pattern analysis,remote sensing,semantic labeling,Semantics,spectral support-vector classification,support vector machines,terrain mapping},
  pages = {4173-4176},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WXE5EVSM/7326745.html}
}

@inproceedings{he_delving_2015,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human}}-{{Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  doi = {10.1109/ICCV.2015.123},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1\%, [26]) on this dataset.},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month = dec,
  year = {2015},
  keywords = {Adaptation models,Biological neural networks,Computational modeling,Gaussian distribution,human-level performance,ILSVRC 2014 winner,image classification,ImageNet 2012 classification dataset,ImageNet classification,model fitting,network architectures,neural nets,overfitting risk,parametric rectified linear unit,PReLU,rectified activation units,rectifier neural networks,rectifier nonlinearities,robust initialization method,state-of-the-art neural networks,Testing,Training},
  pages = {1026-1034},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/NXDQWID9/7410480.html}
}

@article{badrinarayanan_segnet_2017,
  title = {{{SegNet}}: {{A Deep Convolutional Encoder}}-{{Decoder Architecture}} for {{Scene Segmentation}}},
  volume = {39},
  issn = {0162-8828},
  shorttitle = {{{SegNet}}},
  doi = {10.1109/TPAMI.2016.2644615},
  abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.a- .uk/projects/segnet/.},
  number = {12},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  month = dec,
  year = {2017},
  keywords = {Computer architecture,Decoder,Decoding,Deep Convolutional Neural Networks,Encoder,image segmentation,Indoor Scenes,Neural networks,Pooling,Road Scenes,Roads,Semantic Pixel-Wise Segmentation,Semantics,Training,Upsampling},
  pages = {2481-2495},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9SZV73FK/7803544.html}
}

@article{fauvel_advances_2013,
  title = {Advances in {{Spectral}}-{{Spatial Classification}} of {{Hyperspectral Images}}},
  volume = {101},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2012.2197589},
  abstract = {Recent advances in spectral-spatial classification of hyperspectral images are presented in this paper. Several techniques are investigated for combining both spatial and spectral information. Spatial information is extracted at the object (set of pixels) level rather than at the conventional pixel level. Mathematical morphology is first used to derive the morphological profile of the image, which includes characteristics about the size, orientation, and contrast of the spatial structures present in the image. Then, the morphological neighborhood is defined and used to derive additional features for classification. Classification is performed with support vector machines (SVMs) using the available spectral information and the extracted spatial information. Spatial postprocessing is next investigated to build more homogeneous and spatially consistent thematic maps. To that end, three presegmentation techniques are applied to define regions that are used to regularize the preliminary pixel-wise thematic map. Finally, a multiple-classifier (MC) system is defined to produce relevant markers that are exploited to segment the hyperspectral image with the minimum spanning forest algorithm. Experimental results conducted on three real hyperspectral images with different spatial and spectral resolutions and corresponding to various contexts are presented. They highlight the importance of spectral-spatial strategies for the accurate classification of hyperspectral images and validate the proposed methods.},
  number = {3},
  journal = {Proceedings of the IEEE},
  author = {Fauvel, Mathieu and Tarabalka, Yuliya and Benediktsson, J{\'o}n Atli and Chanussot, Jocelyn and Tilton, James C.},
  month = mar,
  year = {2013},
  keywords = {classification,Classification algorithms,conventional pixel level,feature extraction,geophysical image processing,homogeneous thematic maps,hyperspectral image,hyperspectral image segment,hyperspectral imaging,image classification,image morphological profile,Image resolution,image segmentation,Kernel,kernel methods,Mathematical morphology,morphological neighborhood,multiple-classifier system,Nearest neighbor searches,object level,pixel-wise thematic map,presegmentation techniques,remote sensing,segmentation,spanning forest algorithm,spatial information,spatial postprocessing,Spatial resolution,spatial structure contrast,spatial structure orientation,spatial structure size,spatially consistent thematic maps,Spectral analysis,spectral information,spectral-spatial classification,spectral–spatial classifier,support vector machines,vegetation mapping},
  pages = {652-675},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8XPE5X8G/Fauvel et al_2013_Advances in Spectral-Spatial Classification of Hyperspectral Images.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/D7SGGI9Z/6297992.html}
}

@inproceedings{maggiori_can_2017,
  title = {Can {{Semantic Labeling Methods Generalize}} to {{Any City}}? {{The Inria Aerial Image Labeling Benchmark}}},
  shorttitle = {Can {{Semantic Labeling Methods Generalize}} to {{Any City}}?},
  abstract = {New challenges in remote sensing impose the necessity of designing pixel classification methods that, once trained on a certain dataset, generalize to other areas of the earth. This may include regions where the appearance of the same type of objects is significantly different. In the literature it is common to use a single image and split it into training and test sets to train a classifier and assess its performance, respectively. However, this does not prove the generalization capabilities to other inputs. In this paper, we propose an aerial image labeling dataset that covers a wide range of urban settlement appearances, from different geographic locations. Moreover, the cities included in the test set are different from those of the training set. We also experiment with convolutional neural networks on our dataset.},
  language = {en},
  booktitle = {Proceedings of the {{IEEE International Symposium}} on {{Geoscience}} and {{Remote Sensing}} ({{IGARSS}})},
  author = {Maggiori, Emmanuel and Tarabalka, Yuliya and Charpiat, Guillaume and Alliez, Pierre},
  month = jul,
  year = {2017},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/UV49DI3R/Maggiori et al_2017_Can Semantic Labeling Methods Generalize to Any City.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/K7CHRHV4/hal-01468452.html}
}

@inproceedings{deng_imagenet_2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  doi = {10.1109/CVPR.2009.5206848},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  month = jun,
  year = {2009},
  keywords = {computer vision,Explosions,Image databases,Image resolution,image retrieval,ImageNet database,Information retrieval,Internet,large-scale hierarchical image database,large-scale ontology,Large-scale systems,multimedia computing,multimedia data,Multimedia databases,Ontologies,ontologies (artificial intelligence),Robustness,Spine,subtree,trees (mathematics),very large databases,visual databases,wordNet structure},
  pages = {248-255},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FFXN3NBD/5206848.html}
}

@article{chen_deeplab_2018,
  title = {{{DeepLab}}: {{Semantic Image Segmentation}} with {{Deep Convolutional Nets}}, {{Atrous Convolution}}, and {{Fully Connected CRFs}}},
  volume = {40},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2017.2699184},
  number = {4},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Chen, Liang-Chieh and Papandreou, Georges and {Murphy, Kevin} and {Yuille, Alan}},
  month = apr,
  year = {2018},
  keywords = {Atrous Convolution,Computational modeling,Conditional Random Fields,Context,Convolution,Convolutional Neural Networks,Image resolution,Image segmentation,Neural networks,Semantic Segmentation,Semantics},
  pages = {834-848}
}

@misc{noauthor_pytorch_2016,
  title = {{{PyTorch}}: {{Tensors}} and {{Dynamic}} Neural Networks in {{Python}} with Strong {{GPU}} Acceleration},
  year = {2016},
  note = {http://pytorch.org/}
}

@article{breiman_random_2001,
  title = {Random {{Forests}}},
  volume = {45},
  issn = {0885-6125, 1573-0565},
  doi = {10.1023/A:1010933404324},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\textendash{}156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  language = {en},
  number = {1},
  journal = {Machine Learning},
  author = {Breiman, Leo},
  month = oct,
  year = {2001},
  pages = {5-32},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/DNCGLG9M/Breiman - 2001 - Random Forests.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/EVMJYKUX/A1010933404324.html}
}

@article{friedman_greedy_2001,
  title = {Greedy Function Approximation: {{A}} Gradient Boosting Machine.},
  volume = {29},
  issn = {0090-5364, 2168-8966},
  shorttitle = {Greedy Function Approximation},
  doi = {10.1214/aos/1013203451},
  abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent ``boosting'' paradigm is developed for additive expansions based on any fitting criterion.Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such ``TreeBoost'' models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
  language = {en},
  number = {5},
  journal = {The Annals of Statistics},
  author = {Friedman, Jerome H.},
  month = oct,
  year = {2001},
  keywords = {decision trees,boosting,Function estimation,robust nonparametric regression},
  pages = {1189-1232},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/R7ZESN7H/1013203451.html}
}

@inproceedings{zhang_solving_2004,
  title = {Solving {{Large Scale Linear Prediction Problems Using Stochastic Gradient Descent Algorithms}}},
  abstract = {Linear prediction methods, such as least  squares for regression, logistic regression and  support vector machines for classi  cation,  have been extensively used in statistics and  machine learning. In this paper, we study  stochastic gradient descent (SGD) algorithms  on regularized forms of linear prediction  methods. This class of methods, related  to online algorithms such as perceptron, are  both ecient and very simple to implement.},
  booktitle = {Icml 2004: {{Proceedings}} of the {{Twenty}}-{{First International Conference}} on {{Machine Learning}}. {{Omnipress}}},
  author = {Zhang, Tong},
  year = {2004},
  pages = {919--926},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/PF8XPVMQ/Zhang - 2004 - Solving Large Scale Linear Prediction Problems Usi.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/JA4E2CJP/summary.html}
}

@inproceedings{bottou_large-scale_2010,
  title = {Large-Scale Machine Learning with Stochastic Gradient Descent},
  abstract = {Abstract. During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.},
  booktitle = {In {{COMPSTAT}}},
  author = {Bottou, L{\'e}on},
  year = {2010},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/KUDZJ6EU/Bottou - 2010 - Large-scale machine learning with stochastic gradi.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/IIFEA7IM/summary.html}
}

@article{cortes_support-vector_1995,
  title = {Support-Vector Networks},
  volume = {20},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00994018},
  abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
  language = {en},
  number = {3},
  journal = {Machine Learning},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  month = sep,
  year = {1995},
  pages = {273-297},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/DI8NHFDJ/Cortes et Vapnik - 1995 - Support-vector networks.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/MPIR7B89/10.html}
}

@inproceedings{boser_training_1992,
  address = {New York, NY, USA},
  series = {COLT '92},
  title = {A {{Training Algorithm}} for {{Optimal Margin Classifiers}}},
  isbn = {978-0-89791-497-0},
  doi = {10.1145/130385.130401},
  abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
  booktitle = {Proceedings of the {{Fifth Annual Workshop}} on {{Computational Learning Theory}}},
  publisher = {{ACM}},
  author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir},
  year = {1992},
  pages = {144--152},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VJXG6SYQ/Boser et al. - 1992 - A Training Algorithm for Optimal Margin Classifier.pdf}
}

@inproceedings{chan_active_1999,
  series = {Lecture Notes in Computer Science},
  title = {An {{Active Contour Model}} without {{Edges}}},
  isbn = {978-3-540-66498-7 978-3-540-48236-9},
  doi = {10.1007/3-540-48236-9_13},
  abstract = {In this paper, we propose a new model for active contours to detect objects in a given image, based on techniques of curve evolution, Mumford-Shah functional for segmentation and level sets. Our model can detect objects whose boundaries are not necessarily defined by gra- dient. The model is a combination between more classical active contour models using mean curvature motion techniques, and the Mumford-Shah model for segmentation. We minimize an energy which can be seen as a particular case of the so-called minimal partition problem. In the level set formulation, the problem becomes a ``mean-curvature flow''-like evolving the active contour, which will stop on the desired boundary. However, the stopping term does not depend on the gradient of the image, as in the classical active contour models, but is instead related to a particular segmentation of the image. Finally, we will present various experimental results and in particular some examples for which the classical snakes methods based on the gradient are not applicable.},
  language = {en},
  booktitle = {Scale-{{Space Theories}} in {{Computer Vision}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Chan, Tony and Vese, Luminita},
  month = sep,
  year = {1999},
  pages = {141-151},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/2G52IYC4/Chan et Vese - 1999 - An Active Contour Model without Edges.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9TELTTIK/Chan et Vese - 1999 - An Active Contour Model without Edges.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/2P3BHT3I/3-540-48236-9_13.html;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WB3GNBU8/3-540-48236-9_13.html}
}

@article{grady_random_2006,
  title = {Random Walks for Image Segmentation},
  volume = {28},
  number = {11},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Grady, Leo},
  year = {2006},
  pages = {1768--1783},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/P5WG7JB9/01704833.pdf}
}

@article{giraud_robust_2018,
  title = {Robust Superpixels Using Color and Contour Features along Linear Path},
  issn = {1077-3142},
  doi = {10.1016/j.cviu.2018.01.006},
  abstract = {Superpixel decomposition methods are widely used in computer vision and image processing applications. By grouping homogeneous pixels, the accuracy can be increased and the decrease of the number of elements to process can drastically reduce the computational burden. For most superpixel methods, a trade-off is computed between 1) color homogeneity, 2) adherence to the image contours and 3) shape regularity of the decomposition. In this paper, we propose a framework that jointly enforces all these aspects and provides accurate and regular Superpixels with Contour Adherence using Linear Path (SCALP). During the decomposition, we propose to consider color features along the linear path between the pixel and the corresponding superpixel barycenter. A contour prior is also used to prevent the crossing of image boundaries when associating a pixel to a superpixel. Finally, in order to improve the decomposition accuracy and the robustness to noise, we propose to integrate the pixel neighborhood information, while preserving the same computational complexity. SCALP is extensively evaluated on standard segmentation dataset, and the obtained results outperform the ones of the state-of-the-art methods. SCALP is also extended for supervoxel decomposition on MRI images.},
  journal = {Computer Vision and Image Understanding},
  author = {Giraud, R{\'e}mi and Ta, Vinh-Thong and Papadakis, Nicolas},
  month = jan,
  year = {2018},
  keywords = {Contour detection,Linear path,Segmentation,Superpixels},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/N4R5ULZV/Giraud et al. - 2018 - Robust superpixels using color and contour feature.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/V24V8MCU/S1077314218300067.html}
}

@book{breiman_classification_2017,
  title = {Classification and Regression Trees},
  publisher = {{Routledge}},
  author = {Breiman, Leo},
  year = {2017},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9MV6GN35/9781351460491.html}
}

@article{beucher_morphological_1993,
  title = {The Morphological Approach to Segmentation: The Watershed Transformation. {{Mathematical}} Morphology in Image Processing.},
  volume = {34},
  shorttitle = {The Morphological Approach to Segmentation},
  journal = {Optical Engineering},
  author = {Beucher, Serge and Meyer, Fernand},
  year = {1993},
  pages = {433-481}
}

@inproceedings{dalal_histograms_2005,
  title = {Histograms of Oriented Gradients for Human Detection},
  volume = {1},
  doi = {10.1109/CVPR.2005.177},
  abstract = {We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
  booktitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  author = {Dalal, Navneet and Triggs, Bill},
  month = jun,
  year = {2005},
  keywords = {coarse spatial binning,contrast normalization,edge based descriptors,feature extraction,fine orientation binning,fine-scale gradients,gradient based descriptors,gradient methods,High performance computing,Histograms,histograms of oriented gradients,human detection,Humans,Image databases,Image edge detection,linear SVM,object detection,Object detection,object recognition,Object recognition,overlapping descriptor,pedestrian database,robust visual object recognition,Robustness,support vector machines,Support vector machines,Testing},
  pages = {886-893 vol. 1},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/5DZC6F52/9411012.html}
}

@inproceedings{lowe_object_1999,
  title = {Object Recognition from Local Scale-Invariant Features},
  volume = {2},
  doi = {10.1109/ICCV.1999.790410},
  abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
  booktitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  author = {Lowe, David G.},
  year = {1999},
  keywords = {3D projection,blurred image gradients,candidate object matches,cluttered partially occluded images,computation time,computational geometry,Computer science,Electrical capacitance tomography,feature extraction,Filters,image matching,Image recognition,inferior temporal cortex,Layout,least squares approximations,Lighting,local geometric deformations,local image features,local scale-invariant features,low residual least squares solution,multiple orientation planes,nearest neighbor indexing method,Neurons,object recognition,Object recognition,primate vision,Programmable logic arrays,Reactive power,robust object recognition,staged filtering approach,unknown model parameters},
  pages = {1150-1157 vol.2},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9MPEA4TN/790410.html}
}

@inproceedings{nogueira_learning_2016,
  title = {Learning to Semantically Segment High-Resolution Remote Sensing Images},
  doi = {10.1109/ICPR.2016.7900187},
  abstract = {Land cover classification is a task that requires methods capable of learning high-level features while dealing with high volume of data. Overcoming these challenges, Convolutional Networks (ConvNets) can learn specific and adaptable features depending on the data while, at the same time, learn classifiers. In this work, we propose a novel technique to automatically perform pixel-wise land cover classification. To the best of our knowledge, there is no other work in the literature that perform pixel-wise semantic segmentation based on data-driven feature descriptors for high-resolution remote sensing images. The main idea is to exploit the power of ConvNet feature representations to learn how to semantically segment remote sensing images. First, our method learns each label in a pixel-wise manner by taking into account the spatial context of each pixel. In a predicting phase, the probability of a pixel belonging to a class is also estimated according to its spatial context and the learned patterns. We conducted a systematic evaluation of the proposed algorithm using two remote sensing datasets with very distinct properties. Our results show that the proposed algorithm provides improvements when compared to traditional and state-of-the-art methods that ranges from 5 to 15\% in terms of accuracy.},
  booktitle = {2016 23rd {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Nogueira, Keiller and Dalla Mura, Mauro and Chanussot, Jocelyn and Schwartz, William R. and {dos Santos}, Jefferson A.},
  month = dec,
  year = {2016},
  keywords = {classifier learning,Context,ConvNet feature representation,convolution,convolutional network,Deep Learning,feature descriptor,feature extraction,Feature extraction,Feature Learning,geophysical image processing,High-resolution Images,high-resolution remote sensing image,image classification,image representation,image resolution,image segmentation,Image segmentation,land cover,land cover classification,Land-cover Mapping,learning (artificial intelligence),Machine learning,neural nets,Pixel-wise Classification,pixel-wise semantic segmentation,remote sensing,Remote sensing,Remote Sensing,Semantic Segmentation,Semantics,Visualization},
  pages = {3566-3571},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/GNPR43FA/7900187.html}
}

@inproceedings{volpi_semantic_2015,
  title = {Semantic Segmentation of Urban Scenes by Learning Local Class Interactions},
  doi = {10.1109/CVPRW.2015.7301377},
  abstract = {Traditionally, land-cover mapping from remote sensing images is performed by classifying each atomic region in the image in isolation and by enforcing simple smoothing priors via random fields models as two independent steps. In this paper, we propose to model the segmentation problem by a discriminatively trained Conditional Random Field (CRF). To this end, we employ Structured Support Vector Machines (SSVM) to learn the weights of an informative set of appearance descriptors jointly with local class interactions. We propose a principled strategy to learn pairwise potentials encoding local class preferences from sparsely annotated ground truth. We show that this approach outperform standard baselines and more expressive CRF models, improving by 4-6 points the average class accuracy on a challenging dataset involving urban high resolution satellite imagery.},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Volpi, Michele and Ferrari, Vittorio},
  month = jun,
  year = {2015},
  keywords = {CRF models,geophysical image processing,image segmentation,Image segmentation,Labeling,land cover,land-cover mapping,local class interactions,Materials requirements planning,random fields models,remote sensing,Remote sensing,remote sensing images,Satellites,semantic segmentation problem,Semantics,SSVM,Standards,statistical analysis,structured support vector machines,support vector machines,urban high resolution satellite imagery},
  pages = {1-9},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/BH5LTYDL/7301377.html}
}

@inproceedings{vargas_superpixel-based_2014,
  title = {Superpixel-{{Based Interactive Classification}} of {{Very High Resolution Images}}},
  doi = {10.1109/SIBGRAPI.2014.49},
  abstract = {Very high resolution (VHR) images are large datasets for pixel annotation \textendash{} a process that has depended on the supervised training of an effective pixel classifier. Active learning techniques have mitigated this problem, but pixel descriptors are limited to local image information and the large number of pixels makes the response time to the user's actions impractical, during active learning. To circumvent the problem, we present an active learning strategy that relies on superpixel descriptors and a priori dataset reduction. Firstly, we compare VHR image annotation using superpixel- and pixel-based classifiers, as designed by the same state-of-the-art active learning technique \textendash{} Multi-Class Level Uncertainty (MCLU). Even with the dataset reduction provided by the superpixel representation, MCLU remains unfeasible for user interaction. Therefore, we propose a technique to considerably reduce the superpixel dataset for active learning. Moreover, we subdivide the reduced dataset into a list of subsets with random sample rearrangement to gain both speed and sample diversity during the active learning process.},
  booktitle = {2014 27th {{SIBGRAPI Conference}} on {{Graphics}}, {{Patterns}} and {{Images}}},
  author = {Vargas, John E. and Saito, Priscila T. M. and Falc{\~a}o, Alexandre X. and {de Rezende}, Pedro J. and {dos Santos}, Jefferson A.},
  month = aug,
  year = {2014},
  keywords = {a priori dataset reduction,Accuracy,active learning,active learning techniques,Clustering algorithms,dataset reduction,Histograms,image classification,Image color analysis,image representation,image resolution,interactive systems,learning (artificial intelligence),local image information,MCLU,multiclass level uncertainty,pixel annotation,pixel classifier,superpixel dataset,superpixel descriptors,superpixel representation,superpixel-based interactive classification,supervised classification,supervised training,Support vector machines,Training,Uncertainty,user interaction,Very high resolution image processing,very high resolution images,VHR image annotation},
  pages = {173-179},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HKQN4H75/6915305.html}
}

@article{mountrakis_support_2011,
  title = {Support Vector Machines in Remote Sensing: {{A}} Review},
  volume = {66},
  issn = {0924-2716},
  shorttitle = {Support Vector Machines in Remote Sensing},
  doi = {10.1016/j.isprsjprs.2010.11.001},
  abstract = {A wide range of methods for analysis of airborne- and satellite-derived imagery continues to be proposed and assessed. In this paper, we review remote sensing implementations of support vector machines (SVMs), a promising machine learning methodology. This review is timely due to the exponentially increasing number of works published in recent years. SVMs are particularly appealing in the remote sensing field due to their ability to generalize well even with limited training samples, a common limitation for remote sensing applications. However, they also suffer from parameter assignment issues that can significantly affect obtained results. A summary of empirical results is provided for various applications of over one hundred published works (as of April, 2010). It is our hope that this survey will provide guidelines for future applications of SVMs and possible areas of algorithm enhancement.},
  number = {3},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  author = {Mountrakis, Giorgos and Im, Jungho and Ogole, Caesar},
  month = may,
  year = {2011},
  keywords = {SVM,Remote sensing,Support vector machines,Review,SVMs},
  pages = {247-259},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/CAKTDBGN/Mountrakis et al. - 2011 - Support vector machines in remote sensing A revie.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/AA6GW96W/S0924271610001140.html}
}

@incollection{bengio_practical_2012,
  series = {Lecture Notes in Computer Science},
  title = {Practical {{Recommendations}} for {{Gradient}}-{{Based Training}} of {{Deep Architectures}}},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyperparameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
  language = {en},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Bengio, Yoshua},
  year = {2012},
  pages = {437-478},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/MCFQUWFJ/Bengio - 2012 - Practical Recommendations for Gradient-Based Train.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/E38VEJWM/978-3-642-35289-8_26.html},
  doi = {10.1007/978-3-642-35289-8_26}
}

@book{goodfellow_deep_2016,
  title = {Deep {{Learning}}},
  publisher = {{MIT Press}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016}
}

@inproceedings{merciol_efficient_2017,
  address = {Toulouse, France},
  title = {Efficient and Large-Scale Land Cover Classification Using Multiscale Image Analysis},
  booktitle = {Big {{Data}} from {{Space}}},
  author = {Merciol, Fran{\c c}ois and Balem, Thibaud and Lef{\`e}vre, S{\'e}bastien},
  year = {2017},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/IN9LFRCX/hal-01672868.html}
}

@article{levinshtein_turbopixels_2009,
  title = {{{TurboPixels}}: {{Fast Superpixels Using Geometric Flows}}},
  volume = {31},
  issn = {0162-8828},
  shorttitle = {{{TurboPixels}}},
  doi = {10.1109/TPAMI.2009.96},
  abstract = {We describe a geometric-flow-based algorithm for computing a dense oversegmentation of an image, often referred to as superpixels. It produces segments that, on one hand, respect local image boundaries, while, on the other hand, limiting undersegmentation through a compactness constraint. It is very fast, with complexity that is approximately linear in image size, and can be applied to megapixel sized images with high superpixel densities in a matter of minutes. We show qualitative demonstrations of high-quality results on several complex images. The Berkeley database is used to quantitatively compare its performance to a number of oversegmentation algorithms, showing that it yields less undersegmentation than algorithms that lack a compactness constraint while offering a significant speedup over N-cuts, which does enforce compactness.},
  number = {12},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Levinshtein, Alex and Stere, Adrian and Kutulakos, Kiriakos N. and Fleet, David J. and Dickinson, Sven J. and Siddiqi, Kaleem},
  month = dec,
  year = {2009},
  keywords = {Berkeley database,computational complexity,dense oversegmentation,fast superpixels,geometric flows,geometry,image labeling,image resolution,image segmentation,N-cuts,perceptual grouping,perceptual grouping.,superpixels,Superpixels,TurboPixels},
  pages = {2290-2297},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HVNQPWA5/4912213.html}
}

@article{shi_normalized_2000,
  title = {Normalized {{Cuts}} and {{Image Segmentation}}},
  volume = {22},
  issn = {0162-8828},
  doi = {10.1109/34.868688},
  abstract = {We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.},
  number = {8},
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  author = {Shi, Jianbo and Malik, Jitendra},
  month = aug,
  year = {2000},
  keywords = {image segmentation,graph partitioning.,Grouping},
  pages = {888--905}
}

@inproceedings{liu_entropy_2011,
  title = {Entropy Rate Superpixel Segmentation},
  doi = {10.1109/CVPR.2011.5995323},
  abstract = {We propose a new objective function for superpixel segmentation. This objective function consists of two components: entropy rate of a random walk on a graph and a balancing term. The entropy rate favors formation of compact and homogeneous clusters, while the balancing function encourages clusters with similar sizes. We present a novel graph construction for images and show that this construction induces a matroid - a combinatorial structure that generalizes the concept of linear independence in vector spaces. The segmentation is then given by the graph topology that maximizes the objective function under the matroid constraint. By exploiting submodular and mono-tonic properties of the objective function, we develop an efficient greedy algorithm. Furthermore, we prove an approximation bound of \textonehalf{} for the optimality of the solution. Extensive experiments on the Berkeley segmentation benchmark show that the proposed algorithm outperforms the state of the art in all the standard evaluation metrics.},
  booktitle = {{{CVPR}} 2011},
  author = {Liu, Ming-Yu and Tuzel, Oncel and Ramalingam, Srikumar and Chellappa, Rama},
  month = jun,
  year = {2011},
  keywords = {balancing function,Berkeley segmentation benchmark,Complexity theory,entropy,Entropy,entropy rate,graph construction,graph theory,graph topology,greedy algorithm,greedy algorithms,Greedy algorithms,homogeneous clusters,Image edge detection,image segmentation,Image segmentation,matrix algebra,matroid constraint,Measurement,pattern clustering,Random variables,standard evaluation metrics,superpixel segmentation,vector spaces},
  pages = {2097-2104},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HW9F4V9P/5995323.html}
}

@article{comaniciu_mean_2002,
  title = {Mean Shift: A Robust Approach toward Feature Space Analysis},
  volume = {24},
  issn = {0162-8828},
  shorttitle = {Mean Shift},
  doi = {10.1109/34.1000236},
  abstract = {A general non-parametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure: the mean shift. For discrete data, we prove the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. The relation of the mean shift procedure to the Nadaraya-Watson estimator from kernel regression and the robust M-estimators; of location is also established. Algorithms for two low-level vision tasks discontinuity-preserving smoothing and image segmentation - are described as applications. In these algorithms, the only user-set parameter is the resolution of the analysis, and either gray-level or color images are accepted as input. Extensive experimental results illustrate their excellent performance},
  number = {5},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Comaniciu, Dorin and Meer, Peter},
  month = may,
  year = {2002},
  keywords = {algorithm performance,analysis resolution,arbitrarily shaped cluster delineation,color images,complex multimodal feature space,computational module,computer vision,convergence,Convergence,density function,Density functional theory,density modes detection,discontinuity-preserving image smoothing,discrete data,estimation theory,gray-level images,Image analysis,Image color analysis,Image resolution,image segmentation,Image segmentation,Kernel,kernel regression,location estimation,low-level vision algorithms,mean shift,Nadaraya-Watson estimator,nearest stationary point,nonparametric statistics,nonparametric technique,pattern clustering,Pattern recognition,pattern recognition procedure,recursive mean shift procedure,robust feature space analysis,robust M-estimators,Robustness,smoothing methods,Smoothing methods,user-set parameter},
  pages = {603-619},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/B4S7MHYA/1000236.html}
}

@article{marquez-neila_morphological_2014,
  title = {A {{Morphological Approach}} to {{Curvature}}-{{Based Evolution}} of {{Curves}} and {{Surfaces}}},
  volume = {36},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2013.106},
  abstract = {We introduce new results connecting differential and morphological operators that provide a formal and theoretically grounded approach for stable and fast contour evolution. Contour evolution algorithms have been extensively used for boundary detection and tracking in computer vision. The standard solution based on partial differential equations and level-sets requires the use of numerical methods of integration that are costly computationally and may have stability issues. We present a morphological approach to contour evolution based on a new curvature morphological operator valid for surfaces of any dimension. We approximate the numerical solution of the curve evolution PDE by the successive application of a set of morphological operators defined on a binary level-set and with equivalent infinitesimal behavior. These operators are very fast, do not suffer numerical stability issues, and do not degrade the level set function, so there is no need to reinitialize it. Moreover, their implementation is much easier since they do not require the use of sophisticated numerical algorithms. We validate the approach providing a morphological implementation of the geodesic active contours, the active contours without borders, and turbopixels. In the experiments conducted, the morphological implementations converge to solutions equivalent to those achieved by traditional numerical solutions, but with significant gains in simplicity, speed, and stability.},
  number = {1},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {{M{\'a}rquez-Neila}, P. and Baumela, L. and Alvarez, L.},
  month = jan,
  year = {2014},
  keywords = {computer vision,Three-dimensional displays,Computer vision,Surface morphology,Active contours,Approximation methods,boundary detection,curvature based evolution,curvature morphological operator,curve evolution,fast contour evolution algorithms,geodesic active contours,Level set,level set function,level-sets,Mathematical model,mathematical morphology,morphological snakes,numerical analysis,numerical solutions,numerical stability,Numerical stability,partial differential equations,sophisticated numerical algorithms,stability,tracking,turbopixels},
  pages = {2-17},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/TUT83J8I/6529072.html}
}

@article{bengio_representation_2013,
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  volume = {35},
  issn = {0162-8828},
  shorttitle = {Representation {{Learning}}},
  doi = {10.1109/TPAMI.2013.50},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
  number = {8},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  month = aug,
  year = {2013},
  keywords = {Abstracts,AI,Algorithms,artificial intelligence,Artificial Intelligence,autoencoder,autoencoders,Boltzmann machine,data representation,data structures,Deep learning,density estimation,Feature extraction,feature learning,geometrical connections,Humans,Learning systems,Machine learning,machine learning algorithms,manifold learning,Manifolds,neural nets,Neural networks,Neural Networks (Computer),probabilistic models,probability,representation learning,Speech recognition,unsupervised feature learning,unsupervised learning},
  pages = {1798-1828},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VVVC9RRN/6472238.html}
}

@article{achanta_slic_2012,
  title = {{{SLIC Superpixels Compared}} to {{State}}-of-the-{{Art Superpixel Methods}}},
  volume = {34},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2012.120},
  abstract = {Computer vision applications have come to rely increasingly on superpixels in recent years, but it is not always clear what constitutes a good superpixel algorithm. In an effort to understand the benefits and drawbacks of existing methods, we empirically compare five state-of-the-art superpixel algorithms for their ability to adhere to image boundaries, speed, memory efficiency, and their impact on segmentation performance. We then introduce a new superpixel algorithm, simple linear iterative clustering (SLIC), which adapts a k-means clustering approach to efficiently generate superpixels. Despite its simplicity, SLIC adheres to boundaries as well as or better than previous methods. At the same time, it is faster and more memory efficient, improves segmentation performance, and is straightforward to extend to supervoxel generation.},
  number = {11},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurelien and Fua, Pascal and S{\"u}sstrunk, Sabine},
  month = nov,
  year = {2012},
  keywords = {clustering,segmentation,Image edge detection,Image color analysis,computer vision,image segmentation,Complexity theory,iterative methods,Image segmentation,Superpixels,Clustering algorithms,pattern clustering,Algorithms,Approximation algorithms,image boundary,Image Enhancement,Image Interpretation; Computer-Assisted,k-means,k-means clustering approach,Measurement uncertainty,memory efficiency,Pattern Recognition; Automated,Reproducibility of Results,segmentation performance,Sensitivity and Specificity,Signal Processing; Computer-Assisted,simple linear iterative clustering,SLIC superpixels,superpixel generation,supervoxel generation},
  pages = {2274-2282},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/2EGRB69G/6205760.html}
}

@article{dechesne_semantic_2017,
  title = {Semantic Segmentation of Forest Stands of Pure Species Combining Airborne Lidar Data and Very High Resolution Multispectral Imagery},
  volume = {126},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2017.02.011},
  abstract = {Forest stands are the basic units for forest inventory and mapping. Stands are defined as large forested areas (e.g., $\geqslant$2ha) of homogeneous tree species composition and age. Their accurate delineation is usually performed by human operators through visual analysis of very high resolution (VHR) infra-red images. This task is tedious, highly time consuming, and should be automated for scalability and efficient updating purposes. In this paper, a method based on the fusion of airborne lidar data and VHR multispectral images is proposed for the automatic delineation of forest stands containing one dominant species (purity superior to 75\%). This is the key preliminary task for forest land-cover database update. The multispectral images give information about the tree species whereas 3D lidar point clouds provide geometric information on the trees and allow their individual extraction. Multi-modal features are computed, both at pixel and object levels: the objects are individual trees extracted from lidar data. A supervised classification is then performed at the object level in order to coarsely discriminate the existing tree species in each area of interest. The classification results are further processed to obtain homogeneous areas with smooth borders by employing an energy minimum framework, where additional constraints are joined to form the energy function. The experimental results show that the proposed method provides very satisfactory results both in terms of stand labeling and delineation (overall accuracy ranges between 84\% and 99\%).},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  author = {Dechesne, Cl{\'e}ment and Mallet, Cl{\'e}ment and Le Bris, Arnaud and {Gouet-Brunet}, Val{\'e}rie},
  month = apr,
  year = {2017},
  keywords = {Feature selection,Lidar,Energy minimization,Forest stand delineation,Fusion,Multispectral imagery,Regularisation,Supervised classification,Tree species},
  pages = {129-145},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WPIRC3P4/Dechesne et al. - 2017 - Semantic segmentation of forest stands of pure spe.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/E4W5V7GP/S0924271616302763.html}
}

@article{ham_investigation_2005,
  title = {Investigation of the Random Forest Framework for Classification of Hyperspectral Data},
  volume = {43},
  issn = {0196-2892},
  doi = {10.1109/TGRS.2004.842481},
  abstract = {Statistical classification of byperspectral data is challenging because the inputs are high in dimension and represent multiple classes that are sometimes quite mixed, while the amount and quality of ground truth in the form of labeled data is typically limited. The resulting classifiers are often unstable and have poor generalization. This work investigates two approaches based on the concept of random forests of classifiers implemented within a binary hierarchical multiclassifier system, with the goal of achieving improved generalization of the classifier in analysis of hyperspectral data, particularly when the quantity of training data is limited. A new classifier is proposed that incorporates bagging of training samples and adaptive random subspace feature selection within a binary hierarchical classifier (BHC), such that the number of features that is selected at each node of the tree is dependent on the quantity of associated training data. Results are compared to a random forest implementation based on the framework of classification and regression trees. For both methods, classification results obtained from experiments on data acquired by the National Aeronautics and Space Administration (NASA) Airborne Visible/Infrared Imaging Spectrometer instrument over the Kennedy Space Center, Florida, and by Hyperion on the NASA Earth Observing 1 satellite over the Okavango Delta of Botswana are superior to those from the original best basis BHC algorithm and a random subspace extension of the BHC.},
  number = {3},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  author = {Ham, JiSoo and Chen, Yangchi and Crawford, Melba M. and Ghosh, Joydeep},
  month = mar,
  year = {2005},
  keywords = {adaptive random subspace feature selection,Airborne Visible/Infrared Imaging Spectrometer,Bagging,binary hierarchical classifier,binary hierarchical multiclassifier system,Botswana,classification tree,Classification tree analysis,data acquisition,Data analysis,feature extraction,Florida,forestry,geophysical signal processing,Hyperion,hyperspectral data analysis,hyperspectral data classification,Hyperspectral imaging,image classification,Infrared imaging,Infrared spectra,Kennedy Space Center,multidimensional signal processing,NASA,NASA Earth Observing 1 satellite,Okavango Delta,random forest,random processes,random subspace extension,regression tree,Regression tree analysis,Spectroscopy,statistical classification,Training data,tree node,trees (mathematics),USA,vegetation mapping},
  pages = {492-501},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/YD633IEG/1396322.html}
}

@inproceedings{lee_deeply-supervised_2015,
  title = {Deeply-{{Supervised Nets}}},
  abstract = {We propose deeply-supervised nets (DSN), a method that simultaneously minimizes classification error and improves the directness and transparency of the hidden layer learning process. We focus our ...},
  language = {en},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Lee, Chen-Yu and Xie, Saining and Gallagher, Patrick and Zhang, Zhengyou and Tu, Zhuowen},
  month = feb,
  year = {2015},
  pages = {562-570},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/IPSAFNZ4/Lee et al. - 2015 - Deeply-Supervised Nets.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/C76DVE8G/lee15a.html}
}

@inproceedings{brahimi_multiscale_2018,
  address = {Pilsen, Czech Republic},
  title = {Multiscale {{Fully Convolutional DenseNet}} for {{Semantic Segmentation}}},
  abstract = {In the computer vision field, semantic segmentation represents a very interesting task. Convolutional Neural Network methods have shown their great performances in comparison with other semantic segmentation methods. In this paper, we propose a multiscale fully convolutional DenseNet approach for semantic segmentation. Our approach is based on the successful fully convolutional DenseNet method. It is reinforced by integrating a multiscale kernel prediction after the last dense block which performs model averaging over different spatial scales and provides more flexibility of our network to presume more information. Experiments on two semantic segmentation benchmarks: CamVid and Cityscapes have shown the effectiveness of our approach which has outperformed many recent works.},
  booktitle = {{{WSCG}} 2018, {{International Conference}} on {{Computer Graphics}}, {{Visualization}} and {{Computer Vision}}},
  author = {Brahimi, Sourour and Ben Aoun, Najib and Ben Amar, Chokri and Benoit, Alexandre and Lambert, Patrick},
  month = may,
  year = {2018},
  keywords = {Convolutional Neural Network,Dense Block,Fully Convolutional DenseNet,MultiScale Kernel Prediction,Semantic Segmentation},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/ABBW775H/Brahimi et al. - 2018 - Multiscale Fully Convolutional DenseNet for Semant.pdf}
}

@article{gong_superpixel-based_2017,
  title = {Superpixel-{{Based Difference Representation Learning}} for {{Change Detection}} in {{Multispectral Remote Sensing Images}}},
  volume = {55},
  issn = {0196-2892},
  doi = {10.1109/TGRS.2017.2650198},
  abstract = {With the rapid technological development of various satellite sensors, high-resolution remotely sensed imagery has been an important source of data for change detection in land cover transition. However, it is still a challenging problem to effectively exploit the available spectral information to highlight changes. In this paper, we present a novel change detection framework for high-resolution remote sensing images, which incorporates superpixel-based change feature extraction and hierarchical difference representation learning by neural networks. First, highly homogenous and compact image superpixels are generated using superpixel segmentation, which makes these image blocks adhere well to image boundaries. Second, the change features are extracted to represent the difference information using spectrum, texture, and spatial features between the corresponding superpixels. Third, motivated by the fact that deep neural network has the ability to learn from data sets that have few labeled data, we use it to learn the semantic difference between the changed and unchanged pixels. The labeled data can be selected from the bitemporal multispectral images via a preclassification map generated in advance. And then, a neural network is built to learn the difference and classify the uncertain samples into changed or unchanged ones. Finally, a robust and high-contrast change detection result can be obtained from the network. The experimental results on the real data sets demonstrate its effectiveness, feasibility, and superiority of the proposed technique.},
  number = {5},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  author = {Gong, Maoguo and Zhan, Tao and Zhang, Puzhao and Miao, Qiguang},
  month = may,
  year = {2017},
  keywords = {bitemporal multispectral,change detection,Change detection,change feature extraction,difference representation learning,feature extraction,Feature extraction,geophysical image processing,hierarchical difference representation learning,high resolution remotely sensed imagery,Image analysis,Image resolution,Image segmentation,land cover,land cover transition,multispectral images,multispectral remote sensing images,neural nets,neural network,neural networks,Neural networks,preclassification map,remote sensing,Remote sensing,Robustness,satellite sensors,semantic difference,superpixel based difference representation learning,superpixel segmentation},
  pages = {2658-2673},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/NHSXGZZX/Gong et al. - 2017 - Superpixel-Based Difference Representation Learnin.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/ALMH8FMX/7839934.html}
}

@article{li_superpixel-based_2018,
  title = {Superpixel-{{Based Feature}} for {{Aerial Image Scene Recognition}}},
  volume = {18},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  doi = {10.3390/s18010156},
  abstract = {Image scene recognition is a core technology for many aerial remote sensing applications. Different landforms are inputted as different scenes in aerial imaging, and all landform information is regarded as valuable for aerial image scene recognition. However, the conventional features of the Bag-of-Words model are designed using local points or other related information and thus are unable to fully describe landform areas. This limitation cannot be ignored when the aim is to ensure accurate aerial scene recognition. A novel superpixel-based feature is proposed in this study to characterize aerial image scenes. Then, based on the proposed feature, a scene recognition method of the Bag-of-Words model for aerial imaging is designed. The proposed superpixel-based feature that utilizes landform information establishes top-task superpixel extraction of landforms to bottom-task expression of feature vectors. This characterization technique comprises the following steps: simple linear iterative clustering based superpixel segmentation, adaptive filter bank construction, Lie group-based feature quantification, and visual saliency model-based feature weighting. Experiments of image scene recognition are carried out using real image data captured by an unmanned aerial vehicle (UAV). The recognition accuracy of the proposed superpixel-based feature is 95.1\%, which is higher than those of scene recognition algorithms based on other local features.},
  language = {en},
  number = {1},
  journal = {Sensors},
  author = {Li, Hongguang and Shi, Yang and Zhang, Baochang and Wang, Yufeng},
  month = jan,
  year = {2018},
  keywords = {aerial remote sensing,image scene recognition,superpixel-based feature},
  pages = {156},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/BVSRTUL4/Li et al. - 2018 - Superpixel-Based Feature for Aerial Image Scene Re.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/BB877B54/156.html}
}

@phdthesis{bosilj_indexation_2016,
  title = {{Indexation et recherche d'images par arbres des coupes}},
  abstract = {Cette th{\`e}se explore l'utilisation de repr{\'e}sentations hi{\'e}rarchiques des images issues de la morphologie math{\'e}matique, les arbres des coupes, pour la recherche et la classification d'images. Diff{\'e}rents types de structures arborescentes sont analys{\'e}s et une nouvelle classification en deux superclasses est propos{\'e}e, ainsi qu'une contribution {\`a} l'indexation et {\`a} la repr{\'e}sentation de ces structures par des dendogrammes. Deux contributions {\`a} la recherche d'images sont propos{\'e}es, l'une sur la d{\'e}tection de r{\'e}gions d'int{\'e}r{\^e}t et l'autre sur la description de ces r{\'e}gions. Les r{\'e}gions MSER peuvent {\^e}tre d{\'e}tect{\'e}es par un algorithme s'appuyant sur une repr{\'e}sentation des images par arbres min et max. L'utilisation d'autres structures arborescentes sous-jacentes permet de d{\'e}tecter des r{\'e}gions pr{\'e}sentant des propri{\'e}t{\'e}s de stabilit{\'e} diff{\'e}rentes. Un nouveau d{\'e}tecteur, bas{\'e} sur les arbres des formes, est propos{\'e} et {\'e}valu{\'e} en recherche d'images. Pour la description des r{\'e}gions, le concept de spectres de formes 2D permettant de d{\'e}crire globalement une image est {\'e}tendu afin de proposer un descripteur local, au pouvoir discriminant plus puissant. Ce nouveau descripteur pr{\'e}sente de bonnes propri{\'e}t{\'e}s {\`a} la fois de compacit{\'e} et d'invariance {\`a} la rotation et {\`a} la translation. Une attention particuli{\`e}re a {\'e}t{\'e} port{\'e}e {\`a} la pr{\'e}servation de l'invariance {\`a} l'{\'e}chelle. Le descripteur est {\'e}valu{\'e} {\`a} la fois en classification d'images et en recherche d'images satellitaires. Enfin, une technique de simplification des arbres de coupes est pr{\'e}sent{\'e}e, qui permet {\`a} l'utilisateur de r{\'e}{\'e}valuer les mesures du niveau d'agr{\'e}gation des r{\'e}gions impos{\'e} par les arbres des coupes.},
  language = {fr},
  school = {Universit{\'e} de Bretagne Sud},
  author = {Bosilj, Petra},
  month = jan,
  year = {2016},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/MNQIQTAF/Bosilj - 2016 - Indexation et recherche d’images par arbres des co.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/3VABQXTD/tel-01362165.html}
}

@article{jiang_rednet_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.01054},
  primaryClass = {cs},
  title = {{{RedNet}}: {{Residual Encoder}}-{{Decoder Network}} for Indoor {{RGB}}-{{D Semantic Segmentation}}},
  shorttitle = {{{RedNet}}},
  abstract = {Indoor semantic segmentation has always been a difficult task in computer vision. In this paper, we propose an RGB-D residual encoder-decoder architecture, named RedNet, for indoor RGB-D semantic segmentation. In RedNet, the residual module is applied to both the encoder and decoder as the basic building block, and the skip-connection is used to bypass the spatial feature between the encoder and decoder. In order to incorporate the depth information of the scene, a fusion structure is constructed, which makes inference on RGB image and depth image separately, and fuses their features over several layers. In order to efficiently optimize the network's parameters, we propose a ‘pyramid supervision' training scheme, which applies supervised learning over different layers in the decoder, to cope with the problem of gradients vanishing. Experiment results show that the proposed RedNet(ResNet-50) achieves a state-of-the-art mIoU accuracy of 47.8$\backslash$\% on the SUN RGB-D benchmark dataset.},
  journal = {arXiv:1806.01054 [cs]},
  author = {Jiang, Jindong and Zheng, Lunan and Luo, Fei and Zhang, Zhijun},
  month = jun,
  year = {2018},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/TFN7YKMX/Jiang et al. - 2018 - RedNet Residual Encoder-Decoder Network for indoo.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WGDJJD7N/1806.html}
}

@article{stutz_superpixels_2018,
  title = {Superpixels: {{An}} Evaluation of the State-of-the-Art},
  volume = {166},
  issn = {1077-3142},
  shorttitle = {Superpixels},
  doi = {10.1016/j.cviu.2017.03.007},
  abstract = {Superpixels group perceptually similar pixels to create visually meaningful entities while heavily reducing the number of primitives for subsequent processing steps. As of these properties, superpixel algorithms have received much attention since their naming in~2003 (Ren and Malik, 2003). By today, publicly available superpixel algorithms have turned into standard tools in low-level vision. As such, and due to their quick adoption in a wide range of applications, appropriate benchmarks are crucial for algorithm selection and comparison. Until now, the rapidly growing number of algorithms as well as varying experimental setups hindered the development of a unifying benchmark. We present a comprehensive evaluation of 28 state-of-the-art superpixel algorithms utilizing a benchmark focussing on fair comparison and designed to provide new insights relevant for applications. To this end, we explicitly discuss parameter optimization and the importance of strictly enforcing connectivity. Furthermore, by extending well-known metrics, we are able to summarize algorithm performance independent of the number of generated superpixels, thereby overcoming a major limitation of available benchmarks. Furthermore, we discuss runtime, robustness against noise, blur and affine transformations, implementation details as well as aspects of visual quality. Finally, we present an overall ranking of superpixel algorithms which redefines the state-of-the-art and enables researchers to easily select appropriate algorithms and the corresponding implementations which themselves are made publicly available as part of our benchmark at http://www.davidstutz.de/projects/superpixel-benchmark/.},
  journal = {Computer Vision and Image Understanding},
  author = {Stutz, David and Hermans, Alexander and Leibe, Bastian},
  month = jan,
  year = {2018},
  keywords = {Benchmark,Image segmentation,Superpixels,Evaluation,Perceptual grouping,Superpixel segmentation},
  pages = {1-27},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/IIN8DHSM/Stutz et al. - 2018 - Superpixels An evaluation of the state-of-the-art.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/V3KVFE43/S1077314217300589.html}
}

@article{pedregosa_scikit-learn_2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  volume = {12},
  issn = {ISSN 1533-7928},
  shorttitle = {Scikit-Learn},
  number = {Oct},
  journal = {Journal of Machine Learning Research},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = {2011},
  pages = {2825-2830},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/E7YAAB9W/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/C8YJPV24/pedregosa11a.html}
}

@article{van_der_walt_scikit-image_2014,
  title = {Scikit-Image: Image Processing in {{Python}}},
  volume = {2},
  issn = {2167-8359},
  shorttitle = {Scikit-Image},
  doi = {10.7717/peerj.453},
  abstract = {scikit-image is an image processing library that implements algorithms and utilities for use in research, education and industry applications. It is released under the liberal Modified BSD open source license, provides a well-documented API in the Python programming language, and is developed by an active, international team of collaborators. In this paper we highlight the advantages of open source to achieve the goals of the scikit-image library, and we showcase several real-world image processing applications that use scikit-image. More information can be found on the project homepage, http://scikit-image.org.},
  language = {en},
  journal = {PeerJ},
  author = {{van der Walt}, St{\'e}fan and Sch{\"o}nberger, Johannes L. and {Nunez-Iglesias}, Juan and Boulogne, Fran{\c c}ois and Warner, Joshua D. and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony},
  month = jun,
  year = {2014},
  pages = {e453},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/ERQJBRGH/Walt et al. - 2014 - scikit-image image processing in Python.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/MTVAGSR8/453.html}
}

@article{guyet_extraction_2015,
  title = {{Extraction des zones coh{\'e}rentes par l'analyse spatio-temporelle d'images de t{\'e}l{\'e}d{\'e}tection}},
  volume = {25},
  copyright = {\textcopyright{} Lavoisier 2015},
  issn = {1260-5875, 2116-7060},
  doi = {10.3166/RIG.25.473-494},
  abstract = {Cet article pr{\'e}sente une m{\'e}thode de segmentation de s{\'e}ries temporelles d'images satellite (SITS) en zones coh{\'e}rentes, c'est-{\`a}-dire en des r{\'e}gions g{\'e}ographiques ayant des comportements temporels homog{\`e}nes. L'objectif de cette m{\'e}thode est, d'une part, d'extraire des caract{\'e}ristiques spatio-temporelles d'une r{\'e}gion observ{\'e}e et, d'autre part, d'obtenir cette caract{\'e}risation de mani{\`e}re efficace en temps de calcul pour traiter de grandes masses de donn{\'e}es. Cette m{\'e}thode est appliqu{\'e}e {\`a} la caract{\'e}risation des r{\'e}gions agro-{\'e}cologiques du S{\'e}n{\'e}gal par l'analyse des images MODIS sur un an (23 dates)., This paper presents a segmentation method of satellite images time series (SITS) in coherent areas, \emph{i.e.\emph{ in geographical regions having homogeneous temporal behavior. The aim of the method is firstly to extract space-time characteristics of an observed region and secondly to obtain this characterization efficiently in terms of computing cost. This method is applied to the characterization of agro-ecological regions of Senegal by the analysis of MODIS images on one year (23 dates).}}},
  language = {fr},
  number = {4},
  journal = {Revue Internationale de G{\'e}omatique},
  author = {Guyet, Thomas and Malinowski, Simon and Benyoun{\`e}s, Mohand Cherif},
  year = {2015},
  pages = {473-494},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/J3SNSJ28/lvrig254p473.html}
}

@incollection{camara_terralib_2008,
  series = {Advances in Geographic Information Science},
  title = {{{TerraLib}}: {{An Open Source GIS Library}} for {{Large}}-{{Scale Environmental}} and {{Socio}}-{{Economic Applications}}},
  isbn = {978-3-540-74830-4 978-3-540-74831-1},
  shorttitle = {{{TerraLib}}},
  abstract = {This chapter describes TerraLib, an open source GIS software library. The design goal for TerraLib is to support large-scale applications using socio-economic and environmental data. TerraLib supports coding of geographical applications using spatial databases, and stores data in different database management systems including MySQL and PostgreSQL. Its vector data model is upwards compliant with Open Geospatial Consortium (OGC) standards. It handles spatio-temporal data types (events, moving objects, cell spaces, modifiable objects) and allows spatial, temporal, and attribute queries on the database. TerraLib supports dynamic modeling in generalized cell spaces, has a direct runtime link with the R programming language for statistical analysis, and handles large image data sets. The library is developed in C++, and has programming interfaces in Java and Visual Basic. Using TerraLib, the Brazilian National Institute for Space Research (INPE) developed the TerraView open source GIS, which provides functions for data conversion, display, exploratory spatial data analysis, and spatial and non-spatial queries. Another noteworthy application is TerraAmazon, Brazil's national database for monitoring deforestation in the Amazon rainforest, which manages more than 2 million complex polygons and 60 gigabytes of remote sensing images.},
  language = {en},
  booktitle = {Open {{Source Approaches}} in {{Spatial Data Handling}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {C{\^a}mara, Gilberto and Vinhas, L{\'u}bia and Ferreira, Karine Reis and Queiroz, Gilberto Ribeiro De and Souza, Ricardo Cartaxo Modesto De and Monteiro, Ant{\^o}nio Miguel Vieira and Carvalho, Marcelo T{\'\i}lio De and Casanova, Marco Antonio and Freitas, Ubirajara Moura De},
  year = {2008},
  pages = {247-270},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/V4K43F9F/978-3-540-74831-1_12.html},
  doi = {10.1007/978-3-540-74831-1_12}
}

@inproceedings{jia_caffe_2014,
  address = {New York, NY, USA},
  series = {MM '14},
  title = {Caffe: {{Convolutional Architecture}} for {{Fast Feature Embedding}}},
  isbn = {978-1-4503-3063-3},
  shorttitle = {Caffe},
  doi = {10.1145/2647868.2654889},
  abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
  booktitle = {Proceedings of the {{22Nd ACM International Conference}} on {{Multimedia}}},
  publisher = {{ACM}},
  author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  year = {2014},
  keywords = {open source,computer vision,neural networks,machine learning,parallel computation},
  pages = {675--678},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VIX9RQH3/Jia et al. - 2014 - Caffe Convolutional Architecture for Fast Feature.pdf}
}

@article{paszke_automatic_2017,
  title = {Automatic Differentiation in {{PyTorch}}},
  abstract = {In this article, we describe an automatic differentiation module of PyTorch \textemdash{} a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua...},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  month = oct,
  year = {2017},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/V23AHCLW/Paszke et al. - 2017 - Automatic differentiation in PyTorch.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/PUM22BCB/forum.html}
}

@inproceedings{liu_dense_2017,
  title = {Dense {{Semantic Labeling}} of {{Very}}-{{High}}-{{Resolution Aerial Imagery}} and {{LiDAR}} with {{Fully}}-{{Convolutional Neural Networks}} and {{Higher}}-{{Order CRFs}}},
  doi = {10.1109/CVPRW.2017.200},
  abstract = {The increasing availability of very-high-resolution (VHR) aerial optical images as well as coregistered Li-DAR data opens great opportunities for improving object-level dense semantic labeling of airborne remote sensing imagery. As a result, efficient and effective multisensor fusion techniques are needed to fully exploit these complementary data modalities. Recent researches demonstrated how to process remote sensing images using pre-trained deep convolutional neural networks (DCNNs) at the feature level. In this paper, we propose a decision-level fusion approach using a probabilistic graphical model for the task of dense semantic labeling. Our proposed method first obtains two initial probabilistic labeling predictions from a fully-convolutional neural network and a linear classifier, e.g. logistic regression, respectively. These two predictions are then combined within a higher-order conditional random field (CRF). We utilize graph cut inference to estimate the final dense semantic labeling results. Higher-order CRF modeling helps to resolve fusion ambiguities by explicitly using the spatial contextual information, which can be learned from the training data. Experiments on the ISPRS 2D semantic labeling Potsdam dataset show that our proposed approach compares favorably to the state-of-the-art baseline methods.},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Liu, Yansong and Piramanayagam, Sankaranarayanan and Monteiro, Sildomar T. and Saber, Eli},
  month = jul,
  year = {2017},
  keywords = {airborne remote sensing imagery,conditional random field,convolution,coregistered Li-DAR data,DCNN training,deep convolutional neural networks,dense semantic labeling,fully-convolutional neural networks,geophysical image processing,graph cut inference,graph theory,higher-order CRFs,image classification,image resolution,Labeling,Laser radar,learning (artificial intelligence),linear classifier,multisensor fusion,Neural networks,Optical imaging,optical radar,probabilistic graphical model,probability,radar imaging,Remote sensing,remote sensing by radar,Semantics,sensor fusion,Training,very-high-resolution aerial imagery,VHR aerial optical images},
  pages = {1561-1570},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/2SZZN4SD/Liu et al. - 2017 - Dense Semantic Labeling of Very-High-Resolution Ae.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VBV9PSML/8014934.html}
}

@inproceedings{liu_context-aware_2017,
  title = {Context-Aware Cascade Network for Semantic Labeling in {{VHR}} Image},
  doi = {10.1109/ICIP.2017.8296346},
  abstract = {Semantic labeling for the very high resolution (VHR) image of urban areas is challenging, because of many complex manmade objects with different materials and fine-structured objects located together. Under the framework of convolutional neural networks (CNNs), this paper proposes a novel end-to-end network for semantic labeling. Specifically, our network not only improves the labeling accuracy of complex manmade objects by aggregating multiple context semantics with a cascaded architecture, but also refines fine-structured objects by utilizing the low-level detail in shallow layers of CNNs with a hierarchical pyramid structure. Throughout the network, a dedicated residual correction scheme is employed to amend the latent fitting residual. As a result of these specific components, the whole model works in a global-to-local and coarse-to-fine manner. Experimental results show that our network outperforms the state-of-the-art methods on the large-scale ISPRS Vaihingen 2D Semantic Labeling Challenge dataset.},
  booktitle = {2017 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Liu, Y. and Fan, B. and Wang, L. and Bai, J. and Xiang, S. and Pan, C.},
  month = sep,
  year = {2017},
  keywords = {Automobiles,Context,context-aware cascade network,Convolution,convolutional neural networks,Convolutional Neural Networks,end-to-end network,feature extraction,fine-structured objects,geography,hierarchical pyramid structure,high resolution image,image classification,image segmentation,ISPRS Vaihingen 2D Semantic Labeling Challenge dataset,Labeling,labeling accuracy,learning (artificial intelligence),manmade objects,multiple context semantics,neural nets,object detection,Residual Correction,residual correction scheme,semantic labeling,Semantic Labeling,Semantics,terrain mapping,Training,Vegetation,VHR image,VHR Image},
  pages = {575-579},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/CN83W673/Liu et al. - 2017 - Context-aware cascade network for semantic labelin.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/662P34A3/8296346.html}
}

@article{liu_semantic_2017,
  title = {Semantic Labeling in Very High Resolution Images via a Self-Cascaded Convolutional Neural Network},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2017.12.007},
  abstract = {Semantic labeling for very high resolution (VHR) images in urban areas, is of significant importance in a wide range of remote sensing applications. However, many confusing manmade objects and intricate fine-structured objects make it very difficult to obtain both coherent and accurate labeling results. For this challenging task, we propose a novel deep model with convolutional neural networks (CNNs), i.e., an end-to-end self-cascaded network (ScasNet). Specifically, for confusing manmade objects, ScasNet improves the labeling coherence with sequential global-to-local contexts aggregation. Technically, multi-scale contexts are captured on the output of a CNN encoder, and then they are successively aggregated in a self-cascaded manner. Meanwhile, for fine-structured objects, ScasNet boosts the labeling accuracy with a coarse-to-fine refinement strategy. It progressively refines the target objects using the low-level features learned by CNN's shallow layers. In addition, to correct the latent fitting residual caused by multi-feature fusion inside ScasNet, a dedicated residual correction scheme is proposed. It greatly improves the effectiveness of ScasNet. Extensive experimental results on three public datasets, including two challenging benchmarks, show that ScasNet achieves the state-of-the-art performance.},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  author = {Liu, Yongcheng and Fan, Bin and Wang, Lingfeng and Bai, Jun and Xiang, Shiming and Pan, Chunhong},
  month = dec,
  year = {2017},
  keywords = {Convolutional neural networks (CNNs),End-to-end,Multi-scale contexts,Semantic labeling},
  file = {/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/J8KJT8VE/Liu et al. - 2017 - Semantic labeling in very high resolution images v.pdf;/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/NLFKHDLW/S0924271617303854.html}
}


