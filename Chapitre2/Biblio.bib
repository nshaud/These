
@inproceedings{li_superpixel_2015,
	title = {Superpixel {Segmentation} {Using} {Linear} {Spectral} {Clustering}},
	url = {http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Li_Superpixel_Segmentation_Using_2015_CVPR_paper.html},
	urldate = {2015-11-16},
	author = {Li, Zhengqin and Chen, Jiansheng},
	year = {2015},
	pages = {1356--1363},
	file = {Li Chen 2015 - Superpixel Segmentation Using Linear Spectral Clustering.pdf:/home/naudeber/Bibliographie//undefined/2015/Li Chen 2015 - Superpixel Segmentation Using Linear Spectral Clustering.pdf:application/pdf;Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8NRWMJAB/Li_Superpixel_Segmentation_Using_2015_CVPR_paper.html:text/html}
}

@incollection{vedaldi_quick_2008,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Quick {Shift} and {Kernel} {Methods} for {Mode} {Seeking}},
	copyright = {©2008 Springer Berlin Heidelberg},
	isbn = {978-3-540-88692-1 978-3-540-88693-8},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-88693-8_52},
	abstract = {We show that the complexity of the recently introduced medoid-shift algorithm in clustering N points is O(N 2), with a small constant, if the underlying distance is Euclidean. This makes medoid shift considerably faster than mean shift, contrarily to what previously believed. We then exploit kernel methods to extend both mean shift and the improved medoid shift to a large family of distances, with complexity bounded by the effective rank of the resulting kernel matrix, and with explicit regularization constraints. Finally, we show that, under certain conditions, medoid shift fails to cluster data points belonging to the same mode, resulting in over-fragmentation. We propose remedies for this problem, by introducing a novel, simple and extremely efficient clustering algorithm, called quick shift, that explicitly trades off under- and over-fragmentation. Like medoid shift, quick shift operates in non-Euclidean spaces in a straightforward manner. We also show that the accelerated medoid shift can be used to initialize mean shift for increased efficiency. We illustrate our algorithms to clustering data on manifolds, image segmentation, and the automatic discovery of visual categories.},
	language = {en},
	number = {5305},
	urldate = {2015-11-17},
	booktitle = {Computer {Vision} – {ECCV} 2008},
	publisher = {Springer Berlin Heidelberg},
	author = {Vedaldi, Andrea and Soatto, Stefano},
	editor = {Forsyth, David and Torr, Philip and Zisserman, Andrew},
	month = oct,
	year = {2008},
	doi = {10.1007/978-3-540-88693-8_52},
	keywords = {Image Processing and Computer Vision, Computer Imaging, Vision, Pattern Recognition and Graphics, Computer Graphics, Pattern Recognition, Data Mining and Knowledge Discovery, Computer Appl. in Arts and Humanities},
	pages = {705--718},
	file = {Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VKURP9DA/10.html:text/html;Vedaldi Soatto 2008 - Quick Shift and Kernel Methods for Mode Seeking.pdf:/home/naudeber/Bibliographie//Springer Berlin Heidelberg/2008/Vedaldi Soatto 2008 - Quick Shift and Kernel Methods for Mode Seeking.pdf:application/pdf}
}

@inproceedings{razavian_cnn_2014,
	title = {{CNN} {Features} {Off}-the-{Shelf}: {An} {Astounding} {Baseline} for {Recognition}},
	shorttitle = {{CNN} {Features} {Off}-the-{Shelf}},
	doi = {10.1109/CVPRW.2014.131},
	abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the OverFeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the OverFeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Razavian, A.S. and Azizpour, H. and Sullivan, J. and Carlsson, S.},
	month = jun,
	year = {2014},
	keywords = {Vectors, neural nets, support vector machines, convolutional neural network, image representation, image retrieval, learning (artificial intelligence), CNN features, ILSVRC13, OverFeat network training, attribute detection, augmentation techniques, convolutional nets, feature representation, fine grained recognition, generic image representation, linear SVM classifier, low memory footprint methods, object image classification, scene recognition, visual classification tasks, Birds, Image recognition, Visualization, Training, deep learning, feature extraction, image classification},
	pages = {512--519},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/U3T9SK7M/articleDetails.html:text/html;Razavian et al 2014 - CNN Features Off-the-Shelf.pdf:/home/naudeber/Bibliographie//undefined/2014/Razavian et al 2014 - CNN Features Off-the-Shelf.pdf:application/pdf}
}

@techreport{achanta_slic_2010,
	title = {Slic superpixels},
	url = {http://infoscience.epfl.ch/record/149300},
	urldate = {2015-11-13},
	author = {Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurelien and Fua, Pascal and Süsstrunk, Sabine},
	year = {2010},
	file = {Achanta et al 2010 - Slic superpixels.pdf:/home/naudeber/Bibliographie//undefined/2010/Achanta et al 2010 - Slic superpixels.pdf:application/pdf}
}

@article{felzenszwalb_efficient_2004,
	title = {Efficient {Graph}-{Based} {Image} {Segmentation}},
	volume = {59},
	issn = {0920-5691, 1573-1405},
	doi = {10.1023/B:VISI.0000022288.19776.77},
	abstract = {This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.},
	language = {en},
	number = {2},
	urldate = {2015-11-13},
	journal = {International Journal of Computer Vision},
	author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
	month = sep,
	year = {2004},
	keywords = {Artificial Intelligence (incl. Robotics), Computer Imaging, Graphics and Computer Vision, Image Processing, Automation and Robotics, clustering, perceptual organization, graph algorithm, image segmentation},
	pages = {167--181},
	file = {Felzenszwalb Huttenlocher 2004 - Efficient Graph-Based Image Segmentation.pdf:/home/naudeber/Bibliographie//International Journal of Computer Vision/2004/Felzenszwalb Huttenlocher 2004 - Efficient Graph-Based Image Segmentation.pdf:application/pdf;Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/56QJGA7K/10.1023BVISI.0000022288.19776.html:text/html}
}

@inproceedings{duan_image_2015,
	title = {Image partitioning into convex polygons},
	url = {https://hal.inria.fr/hal-01140320/},
	urldate = {2015-11-13},
	booktitle = {{IEEE} conference on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Duan, Liuyun and Lafarge, Florent},
	year = {2015},
	file = {Duan Lafarge 2015 - Image partitioning into convex polygons.pdf:/home/naudeber/Bibliographie//undefined/2015/Duan Lafarge 2015 - Image partitioning into convex polygons.pdf:application/pdf}
}

@article{simonyan_very_2014,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2015-11-18},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = sep,
	year = {2014},
	keywords = {À lire, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8VM3BPQT/1409.html:text/html;Simonyan Zisserman 2014 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:/home/naudeber/Bibliographie//arXiv1409.1556 [cs]/2014/Simonyan Zisserman 2014 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:application/pdf}
}

@article{marmanis_semantic_2016,
	title = {Semantic {Segmentation} of {Aerial} {Images} with an {Ensemble} of {CNNs}},
	volume = {3},
	url = {http://adsabs.harvard.edu/abs/2016ISPAnIII3..473M},
	doi = {10.5194/isprs-annals-III-3-473-2016},
	abstract = {This paper describes a deep learning approach to semantic segmentation of very high resolution (aerial) images. Deep neural architectures hold the promise of end-to-end learning from raw images, making heuristic feature design obsolete. Over the last decade this idea has seen a revival, and in recent years deep convolutional neural networks (CNNs) have emerged as the method of choice for a range of image interpretation tasks like visual recognition and object detection. Still, standard CNNs do not lend themselves to per-pixel semantic segmentation, mainly because one of their fundamental principles is to gradually aggregate information over larger and larger image regions, making it hard to disentangle contributions from different pixels. Very recently two extensions of the CNN framework have made it possible to trace the semantic information back to a precise pixel position: deconvolutional network layers undo the spatial downsampling, and Fully Convolution Networks (FCNs) modify the fully connected classification layers of the network in such a way that the location of individual activations remains explicit. We design a FCN which takes as input intensity and range data and, with the help of aggressive deconvolution and recycling of early network layers, converts them into a pixelwise classification at full resolution. We discuss design choices and intricacies of such a network, and demonstrate that an ensemble of several networks achieves excellent results on challenging data such as the ISPRS semantic labeling benchmark, using only the raw data as input.},
	urldate = {2016-07-08},
	journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Marmanis, Dimitrios and Wegner, Jan Dirk and Galliani, Silvano and Schindler, Konrad and Datcu, Mihai and Stilla, Uwe},
	month = jun,
	year = {2016},
	pages = {473--480},
	file = {Marmanis et al 2016 - Semantic Segmentation of Aerial Images with an Ensemble of CNNs.pdf:/home/naudeber/Bibliographie//ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences/2016/Marmanis et al 2016 - Semantic Segmentation of Aerial Images with an Ensemble of CNNs.pdf:application/pdf}
}

@incollection{bergh_seeds:_2012,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{SEEDS}: {Superpixels} {Extracted} via {Energy}-{Driven} {Sampling}},
	copyright = {©2012 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-33785-7 978-3-642-33786-4},
	shorttitle = {{SEEDS}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-33786-4_2},
	abstract = {Superpixel algorithms aim to over-segment the image by grouping pixels that belong to the same object. Many state-of-the-art superpixel algorithms rely on minimizing objective functions to enforce color homogeneity. The optimization is accomplished by sophisticated methods that progressively build the superpixels, typically by adding cuts or growing superpixels. As a result, they are computationally too expensive for real-time applications. We introduce a new approach based on a simple hill-climbing optimization. Starting from an initial superpixel partitioning, it continuously refines the superpixels by modifying the boundaries. We define a robust and fast to evaluate energy function, based on enforcing color similarity between the boundaries and the superpixel color histogram. In a series of experiments, we show that we achieve an excellent compromise between accuracy and efficiency. We are able to achieve a performance comparable to the state-of-the-art, but in real-time on a single Intel i7 CPU at 2.8GHz.},
	language = {en},
	number = {7578},
	urldate = {2015-11-19},
	booktitle = {Computer {Vision} – {ECCV} 2012},
	publisher = {Springer Berlin Heidelberg},
	author = {Bergh, Michael Van den and Boix, Xavier and Roig, Gemma and Capitani, Benjamin de and Gool, Luc Van},
	editor = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
	month = oct,
	year = {2012},
	doi = {10.1007/978-3-642-33786-4_2},
	keywords = {Image Processing and Computer Vision, Computer Imaging, Vision, Pattern Recognition and Graphics, Computer Graphics, Artificial Intelligence (incl. Robotics), Pattern Recognition, Algorithm Analysis and Problem Complexity, segmentation, superpixels},
	pages = {13--26},
	file = {Bergh et al 2012 - SEEDS.pdf:/home/naudeber/Bibliographie//Springer Berlin Heidelberg/2012/Bergh et al 2012 - SEEDS.pdf:application/pdf;Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/K6W7IK43/10.html:text/html}
}

@inproceedings{yosinski_how_2014,
	title = {How transferable are features in deep neural networks?},
	url = {http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks},
	urldate = {2015-11-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	year = {2014},
	pages = {3320--3328},
	file = {Yosinski et al 2014 - How transferable are features in deep neural networks.pdf:/home/naudeber/Bibliographie//undefined/2014/Yosinski et al 2014 - How transferable are features in deep neural networks.pdf:application/pdf}
}

@article{baatz_multiresolution_2000,
	title = {Multiresolution {Segmentation}: an optimization approach for high quality multi-scale image segmentation},
	journal = {Angewandte Geographische Informationsverarbeitung XII: Beiträge zum AGIT-Symposium Salzburg},
	author = {Baatz, Martin and Schäpe, Arno},
	year = {2000},
	pages = {12--23},
	file = {Baatz Schäpe 2000 - Multiresolution Segmentation.pdf:/home/naudeber/Bibliographie//Angewandte Geographische Informationsverarbeitung XII Beiträge zum AGIT-Symposium Salzburg/2000/Baatz Schäpe 2000 - Multiresolution Segmentation.pdf:application/pdf}
}

@article{tilton_best_2012,
	title = {Best {Merge} {Region}-{Growing} {Segmentation} {With} {Integrated} {Nonadjacent} {Region} {Object} {Aggregation}},
	volume = {50},
	issn = {0196-2892, 1558-0644},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6182584},
	doi = {10.1109/TGRS.2012.2190079},
	number = {11},
	urldate = {2016-01-06},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Tilton, James C. and Tarabalka, Yuliya and Montesano, Paul M. and Gofman, Emanuel},
	month = nov,
	year = {2012},
	pages = {4454--4467}
}

@article{rottensteiner_isprs_2012,
	title = {The {ISPRS} benchmark on urban object classification and 3D building reconstruction},
	volume = {1},
	url = {https://t3sec3.rrzn.uni-hannover.de/cmsv021a.rrzn.uni-hannover.de/uploads/tx_tkpublikationen/isprsannals-I-3-293-2012.pdf},
	urldate = {2016-01-07},
	journal = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Rottensteiner, Franz and Sohn, Gunho and Jung, Jaewook and Gerke, Markus and Baillard, Caroline and Benitez, Sebastien and Breitkopf, Uwe},
	year = {2012},
	pages = {3},
	file = {Rottensteiner et al 2012 - The ISPRS benchmark on urban object classification and 3D building.pdf:/home/naudeber/Bibliographie//ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci/2012/Rottensteiner et al 2012 - The ISPRS benchmark on urban object classification and 3D building.pdf:application/pdf}
}

@inproceedings{lin_efficient_2015,
	title = {Efficient piecewise training of deep structured models for semantic segmentation},
	url = {http://arxiv.org/abs/1504.01013},
	abstract = {Recent advances in semantic image segmentation have mostly been achieved by training deep convolutional neural networks (CNNs) for the task. We show how to improve semantic segmentation through the use of contextual information. Specifically, we explore `patch-patch' context and `patch-background' context with deep CNNs. For learning the patch-patch context between image regions, we formulate Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied to avoid repeated expensive CRF inference for back propagation. In order to capture the patch-background context, we show that a network design with traditional multi-scale image input and sliding pyramid pooling is effective for improving performance. Our experiment results set new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. Particularly, we achieve an intersection-over-union score of \$77.8\$ on the challenging PASCAL VOC 2012 dataset.},
	urldate = {2016-03-15},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Lin, Guosheng and Shen, Chunhua and Van Den Hengel, Anton and Reid, Ian},
	month = apr,
	year = {2015},
	keywords = {À lire, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9VAWMIBH/1504.html:text/html;Lin et al 2015 - Efficient piecewise training of deep structured models for semantic segmentation.pdf:/home/naudeber/Bibliographie//undefined/2015/Lin et al 2015 - Efficient piecewise training of deep structured models for semantic segmentation.pdf:application/pdf}
}

@techreport{gerke_use_2015,
	title = {Use of the {Stair} {Vision} {Library} within the {ISPRS} 2D {Semantic} {Labeling} {Benchmark} ({Vaihingen})},
	url = {https://www.researchgate.net/profile/Markus_Gerke/publication/270104226_Use_of_the_Stair_Vision_Library_within_the_ISPRS_2D_Semantic_Labeling_Benchmark_(Vaihingen)/links/54ae59c50cf2828b29fcdf4b.pdf},
	urldate = {2016-05-26},
	institution = {International Institute for Geo-Information Science and Earth Observation},
	author = {Gerke, Markus},
	year = {2015},
	file = {Gerke 2015 - Use of the Stair Vision Library within the ISPRS 2D Semantic Labeling Benchmark.pdf:/home/naudeber/Bibliographie//International Institute for Geo-Information Science and Earth Observation/2015/Gerke 2015 - Use of the Stair Vision Library within the ISPRS 2D Semantic Labeling Benchmark.pdf:application/pdf}
}

@article{marmanis_classification_2017,
	title = {Classification {With} an {Edge}: {Improving} {Semantic} {Image} {Segmentation} with {Boundary} {Detection}},
	shorttitle = {Classification {With} an {Edge}},
	url = {http://arxiv.org/abs/1612.01337},
	abstract = {We present an end-to-end trainable deep convolutional neural network (DCNN) for semantic segmentation with built-in awareness of semantically meaningful boundaries. Semantic segmentation is a fundamental remote sensing task, and most state-of-the-art methods rely on DCNNs as their workhorse. A major reason for their success is that deep networks learn to accumulate contextual information over very large windows (receptive fields). However, this success comes at a cost, since the associated loss of effecive spatial resolution washes out high-frequency details and leads to blurry object boundaries. Here, we propose to counter this effect by combining semantic segmentation with semantically informed edge detection, thus making class-boundaries explicit in the model, First, we construct a comparatively simple, memory-efficient model by adding boundary detection to the Segnet encoder-decoder architecture. Second, we also include boundary detection in FCN-type models and set up a high-end classifier ensemble. We show that boundary detection significantly improves semantic segmentation with CNNs. Our high-end ensemble achieves {\textgreater} 90\% overall accuracy on the ISPRS Vaihingen benchmark.},
	urldate = {2016-12-21},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Marmanis, Dimitrios and Schindler, Konrad and Wegner, Jan Dirk and Galliani, Silvano and Datcu, Mihai and Stilla, Uwe},
	year = {2017},
	note = {arXiv: 1612.01337},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/Z3Z4WVNI/1612.html:text/html;Marmanis et al 2016 - Classification With an Edge.pdf:/home/naudeber/Bibliographie//arXiv1612.01337 [cs]/2016/Marmanis et al 2016 - Classification With an Edge.pdf:application/pdf}
}

@inproceedings{quang_efficient_2015,
	title = {An {Efficient} {Framework} for {Pixel}-wise {Building} {Segmentation} from {Aerial} {Images}},
	url = {http://dl.acm.org/citation.cfm?id=2833272},
	urldate = {2016-05-26},
	booktitle = {Proceedings of the {Sixth} {International} {Symposium} on {Information} and {Communication} {Technology}},
	publisher = {ACM},
	author = {Quang, Nguyen Tien and Thuy, Nguyen Thi and Sang, Dinh Viet and Binh, Huynh Thi Thanh},
	year = {2015},
	pages = {43},
	file = {Quang et al 2015 - An Efficient Framework for Pixel-wise Building Segmentation from Aerial Images.pdf:/home/naudeber/Bibliographie//ACM/2015/Quang et al 2015 - An Efficient Framework for Pixel-wise Building Segmentation from Aerial Images.pdf:application/pdf}
}

@techreport{boulch_dag_2015,
	title = {{DAG} of convolutional networks for semantic labeling},
	url = {https://www.itc.nl/external/ISPRS_WGIII4/ISPRSIII_4_Test_results/papers/onera_boulch.pdf},
	urldate = {2016-05-26},
	institution = {Office national d'études et de recherches aérospatiales},
	author = {Boulch, Alexandre},
	year = {2015},
	file = {Boulch 2015 - DAG of convolutional networks for semantic labeling.pdf:/home/naudeber/Bibliographie//Office national d'études et de recherches aérospatiales/2015/Boulch 2015 - DAG of convolutional networks for semantic labeling.pdf:application/pdf}
}

@inproceedings{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://jmlr.org/proceedings/papers/v37/ioffe15.html},
	urldate = {2016-05-27},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
	pages = {448--456},
	file = {Ioffe Szegedy 2015 - Batch Normalization.pdf:/home/naudeber/Bibliographie//undefined/2015/Ioffe Szegedy 2015 - Batch Normalization.pdf:application/pdf;Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/SZXJDI3V/ioffe15.html:text/html}
}

@inproceedings{chatfield_return_2014,
	title = {Return of the {Devil} in the {Details}: {Delving} {Deep} into {Convolutional} {Nets}},
	isbn = {978-1-901725-52-0},
	shorttitle = {Return of the {Devil} in the {Details}},
	url = {http://www.bmva.org/bmvc/2014/papers/paper054/index.html},
	doi = {10.5244/C.28.6},
	language = {en},
	urldate = {2016-05-27},
	booktitle = {Proceedings of the {British} {Machine} {Vision} {Conference}},
	publisher = {British Machine Vision Association},
	author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	year = {2014},
	pages = {6.1--6.12}
}

@article{campos-taberner_processing_2016,
	title = {Processing of {Extremely} {High}-{Resolution} {LiDAR} and {RGB} {Data}: {Outcome} of the 2015 {IEEE} {GRSS} {Data} {Fusion} {Contest} {Part} {A}: 2-{D} {Contest}},
	volume = {9},
	issn = {1939-1404},
	shorttitle = {Processing of {Extremely} {High}-{Resolution} {LiDAR} and {RGB} {Data}},
	doi = {10.1109/JSTARS.2016.2569162},
	abstract = {In this paper, we discuss the scientific outcomes of the 2015 data fusion contest organized by the Image Analysis and Data Fusion Technical Committee (IADF TC) of the IEEE Geoscience and Remote Sensing Society (IEEE GRSS). As for previous years, the IADF TC organized a data fusion contest aiming at fostering new ideas and solutions for multisource studies. The 2015 edition of the contest proposed a multiresolution and multisensorial challenge involving extremely high-resolution RGB images and a three-dimensional (3-D) LiDAR point cloud. The competition was framed in two parallel tracks, considering 2-D and 3-D products, respectively. In this paper, we discuss the scientific results obtained by the winners of the 2-D contest, which studied either the complementarity of RGB and LiDAR with deep neural networks (winning team) or provided a comprehensive benchmarking evaluation of new classification strategies for extremely high-resolution multimodal data (runner-up team). The data and the previously undisclosed ground truth will remain available for the community and can be obtained at http://www.grss-ieee.org/community/technical-committees/data-fusion/2015-ieee-grss-data-fusion-contest/. The 3-D part of the contest is discussed in the Part-B paper [1].},
	number = {12},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Campos-Taberner, M. and Romero-Soriano, A. and Gatta, C. and Camps-Valls, G. and Lagrange, A. and Le Saux, B. and Beaupère, A. and Boulch, A. and Chan-Hon-Tong, A. and Herbin, S. and Randrianarivo, H. and Ferecatu, M. and Shimoni, M. and Moser, G. and Tuia, D.},
	month = dec,
	year = {2016},
	keywords = {deep neural networks, Laser radar, Data integration, Earth, Spatial resolution, Three-dimensional displays, LiDAR, extremely high spatial resolution, image analysis and data fusion (IADF), landcover classification, multimodal-data fusion, multiresolution-, multisource-, remote sensing},
	pages = {5547--5559},
	file = {Campos-Taberner et al 2016 - Processing of Extremely High-Resolution LiDAR and RGB Data.pdf:/home/naudeber/Bibliographie//IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing/2016/Campos-Taberner et al 2016 - Processing of Extremely High-Resolution LiDAR and RGB Data.pdf:application/pdf;IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FC7U9666/articleDetails.html:text/html}
}

@article{sherrah_fully_2016,
	title = {Fully {Convolutional} {Networks} for {Dense} {Semantic} {Labelling} of {High}-{Resolution} {Aerial} {Imagery}},
	url = {http://arxiv.org/abs/1606.02585},
	abstract = {The trend towards higher resolution remote sensing imagery facilitates a transition from land-use classification to object-level scene understanding. Rather than relying purely on spectral content, appearance-based image features come into play. In this work, deep convolutional neural networks (CNNs) are applied to semantic labelling of high-resolution remote sensing data. Recent advances in fully convolutional networks (FCNs) are adapted to overhead data and shown to be as effective as in other domains. A full-resolution labelling is inferred using a deep FCN with no downsampling, obviating the need for deconvolution or interpolation. To make better use of image features, a pre-trained CNN is fine-tuned on remote sensing data in a hybrid network context, resulting in superior results compared to a network trained from scratch. The proposed approach is applied to the problem of labelling high-resolution aerial imagery, where fine boundary detail is important. The dense labelling yields state-of-the-art accuracy for the ISPRS Vaihingen and Potsdam benchmark data sets.},
	urldate = {2016-09-26},
	journal = {arXiv:1606.02585 [cs]},
	author = {Sherrah, Jamie},
	month = jun,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/R8U6HVPF/1606.html:text/html;Sherrah 2016 - Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution.pdf:/home/naudeber/Bibliographie//arXiv1606.02585 [cs]/2016/Sherrah 2016 - Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution.pdf:application/pdf}
}

@inproceedings{nogueira_improving_2015,
	title = {Improving {Spatial} {Feature} {Representation} from {Aerial} {Scenes} by {Using} {Convolutional} {Networks}},
	isbn = {978-1-4673-7962-5},
	url = {http://ieeexplore.ieee.org/document/7314576/},
	doi = {10.1109/SIBGRAPI.2015.39},
	urldate = {2017-01-09},
	publisher = {IEEE},
	author = {Nogueira, Keiller and Miranda, Waner O. and Santos, Jefersson A. Dos},
	month = aug,
	year = {2015},
	pages = {289--296},
	file = {Nogueira et al 2015 - Improving Spatial Feature Representation from Aerial Scenes by Using.pdf:/home/naudeber/Bibliographie//IEEE/2015/Nogueira et al 2015 - Improving Spatial Feature Representation from Aerial Scenes by Using.pdf:application/pdf}
}

@inproceedings{neubert_compact_2014,
	title = {Compact {Watershed} and {Preemptive} {SLIC}: {On} {Improving} {Trade}-offs of {Superpixel} {Segmentation} {Algorithms}.},
	shorttitle = {Compact {Watershed} and {Preemptive} {SLIC}},
	url = {https://www.tu-chemnitz.de/etit/proaut/forschung/rsrc/cws_pSLIC_ICPR.pdf},
	urldate = {2017-01-09},
	booktitle = {{ICPR}},
	author = {Neubert, Peer and Protzel, Peter},
	year = {2014},
	pages = {996--1001},
	file = {Neubert Protzel 2014 - Compact Watershed and Preemptive SLIC.pdf:/home/naudeber/Bibliographie//undefined/2014/Neubert Protzel 2014 - Compact Watershed and Preemptive SLIC.pdf:application/pdf}
}

@inproceedings{neubert_superpixel_2012,
	title = {Superpixel benchmark and comparison},
	url = {http://books.google.com/books?hl=en&lr=&id=39rFs0LJiRAC&oi=fnd&pg=PA205&dq=%22Liu+et+al.%22+%22algorithm+using+the+same+implementation,+data+set+and+error%22+%22image+edges+by+placing+them+inside+a+superpixel.+Depending%22+%22of+the+segmentation+compared+to+human+ground+truth%22+&ots=DmTz25PMw2&sig=TQoa_LmdyN4zyJIfJhugNHaSHEM},
	urldate = {2017-01-09},
	booktitle = {Proc. {Forum} {Bildverarbeitung}},
	author = {Neubert, Peer and Protzel, Peter},
	year = {2012},
	pages = {1--12},
	file = {Neubert Protzel 2012 - Superpixel benchmark and comparison.pdf:/home/naudeber/Bibliographie//undefined/2012/Neubert Protzel 2012 - Superpixel benchmark and comparison_2.pdf:application/pdf}
}

@inproceedings{audebert_semantic_2016,
	title = {Semantic {Segmentation} of {Earth} {Observation} {Data} {Using} {Multimodal} and {Multi}-scale {Deep} {Networks}},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-54181-5_12},
	doi = {10.1007/978-3-319-54181-5_12},
	abstract = {This work investigates the use of deep fully convolutional neural networks (DFCNN) for pixel-wise scene labeling of Earth Observation images. Especially, we train a variant of the SegNet architecture on remote sensing data over an urban area and study different strategies for performing accurate semantic segmentation. Our contributions are the following: (1) we transfer efficiently a DFCNN from generic everyday images to remote sensing images; (2) we introduce a multi-kernel convolutional layer for fast aggregation of predictions at multiple scales; (3) we perform data fusion from heterogeneous sensors (optical and laser) using residual correction. Our framework improves state-of-the-art accuracy on the ISPRS Vaihingen 2D Semantic Labeling dataset.},
	language = {en},
	urldate = {2017-03-31},
	booktitle = {Computer {Vision} – {ACCV} 2016},
	publisher = {Springer, Cham},
	author = {Audebert, Nicolas and Le Saux, Bertrand and Lefèvre, Sébastien},
	month = nov,
	year = {2016},
	pages = {180--196},
	file = {Audebert et al_2016_Semantic Segmentation of Earth Observation Data Using Multimodal and.pdf:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/394SFXNP/Audebert et al_2016_Semantic Segmentation of Earth Observation Data Using Multimodal and.pdf:application/pdf;Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/PC7RWZAX/978-3-319-54181-5_12.html:text/html}
}

@article{volpi_dense_2017,
	title = {Dense {Semantic} {Labeling} of {Subdecimeter} {Resolution} {Images} {With} {Convolutional} {Neural} {Networks}},
	volume = {55},
	issn = {0196-2892},
	doi = {10.1109/TGRS.2016.2616585},
	abstract = {Semantic labeling (or pixel-level land-cover classification) in ultrahigh-resolution imagery ({\textless};10 cm) requires statistical models able to learn high-level concepts from spatial data, with large appearance variations. Convolutional neural networks (CNNs) achieve this goal by learning discriminatively a hierarchy of representations of increasing abstraction. In this paper, we present a CNN-based system relying on a downsample-then-upsample architecture. Specifically, it first learns a rough spatial map of high-level representations by means of convolutions and then learns to upsample them back to the original resolution by deconvolutions. By doing so, the CNN learns to densely label every pixel at the original resolution of the image. This results in many advantages, including: 1) the state-of-the-art numerical accuracy; 2) the improved geometric accuracy of predictions; and 3) high efficiency at inference time. We test the proposed system on the Vaihingen and Potsdam subdecimeter resolution data sets, involving the semantic labeling of aerial images of 9- and 5-cm resolution, respectively. These data sets are composed by many large and fully annotated tiles, allowing an unbiased evaluation of models making use of spatial information. We do so by comparing two standard CNN architectures with the proposed one: standard patch classification, prediction of local label patches by employing only convolutions, and full patch labeling by employing deconvolutions. All the systems compare favorably or outperform a state-of-the-art baseline relying on superpixels and powerful appearance descriptors. The proposed full patch labeling CNN outperforms these models by a large margin, also showing a very appealing inference time.},
	number = {2},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Volpi, M. and Tuia, D.},
	month = feb,
	year = {2017},
	keywords = {aerial images, classification, CNN-based system, convolutional neural network, Convolutional neural networks (CNNs), Data models, deconvolution networks, deep learning, feature extraction, geophysical image processing, image classification, Image resolution, Labeling, land cover, Machine learning, neural nets, patch labeling, pixel-level land cover classification, Potsdam subdecimeter resolution dataset, remote sensing, semantic labeling, semantic Web, Semantics, standard patch classification, statistical model, subdecimeter resolution, subdecimeter resolution images, ultrahigh-resolution imagery, Vaihingen subdecimeter resolution dataset},
	pages = {881--893},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/UVV6QJZ8/7725499.html:text/html}
}

@inproceedings{paisitkriangkrai_effective_2015,
	title = {Effective semantic pixel labelling with convolutional networks and {Conditional} {Random} {Fields}},
	doi = {10.1109/CVPRW.2015.7301381},
	abstract = {Large amounts of available training data and increasing computing power have led to the recent success of deep convolutional neural networks (CNN) on a large number of applications. In this paper, we propose an effective semantic pixel labelling using CNN features, hand-crafted features and Conditional Random Fields (CRFs). Both CNN and hand-crafted features are applied to dense image patches to produce per-pixel class probabilities. The CRF infers a labelling that smooths regions while respecting the edges present in the imagery. The method is applied to the ISPRS 2D semantic labelling challenge dataset with competitive classification accuracy.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Paisitkriangkrai, Sakrapee and Sherrah, Jamie and Janney, Pranam and Van Den Hengel, Anton},
	month = jun,
	year = {2015},
	keywords = {Accuracy, classification accuracy, CNN features, computing power, conditional random fields, Convolutional networks, CRF, deep-convolutional neural networks, dense-image patches, edge detection, feature extraction, hand-crafted features, image classification, Image edge detection, image region smoothing, ISPRS 2D semantic labelling challenge dataset, Labeling, neural nets, per-pixel class probabilities, probability, semantic pixel labelling, Semantics, smoothing methods, Training, training data, Visualization},
	pages = {36--43},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/5WKQRPFF/7301381.html:text/html}
}

@inproceedings{penatti_deep_2015,
	title = {Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?},
	doi = {10.1109/CVPRW.2015.7301382},
	abstract = {In this paper, we evaluate the generalization power of deep features (ConvNets) in two new scenarios: aerial and remote sensing image classification. We evaluate experimentally ConvNets trained for recognizing everyday objects for the classification of aerial and remote sensing images. ConvNets obtained the best results for aerial images, while for remote sensing, they performed well but were outperformed by low-level color descriptors, such as BIC. We also present a correlation analysis, showing the potential for combining/fusing different ConvNets with other descriptors or even for combining multiple ConvNets. A preliminary set of experiments fusing ConvNets obtains state-of-the-art results for the well-known UCMerced dataset.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Penatti, Otávio A. B. Penatti Otávio A. B. and Nogueira, Keiller and Santos, Jefersson A.},
	month = jun,
	year = {2015},
	keywords = {Accuracy, Image color analysis, Visualization, correlation methods, geophysical image processing, image colour analysis, object recognition, BIC, UCMerced dataset, aerial images, aerial scenes domains, correlation analysis, deep features, generalization power, low-level color descriptors, multiple ConvNets, remote sensing image classification, Correlation, Histograms, feature extraction, image classification, remote sensing},
	pages = {44--51},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/AHDUQXD8/7301382.html:text/html}
}

@inproceedings{lagrange_benchmarking_2015,
	title = {Benchmarking classification of earth-observation data: {From} learning explicit features to convolutional networks},
	shorttitle = {Benchmarking classification of earth-observation data},
	doi = {10.1109/IGARSS.2015.7326745},
	abstract = {In this paper, we address the task of semantic labeling of multisource earth-observation (EO) data. Precisely, we benchmark several concurrent methods of the last 15 years, from expert classifiers, spectral support-vector classification and high-level features to deep neural networks. We establish that (1) combining multisensor features is essential for retrieving some specific classes, (2) in the image domain, deep convolutional networks obtain significantly better overall performances and (3) transfer of learning from large generic-purpose image sets is highly effective to build EO data classifiers.},
	booktitle = {2015 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium} ({IGARSS})},
	author = {Lagrange, Adrien and Le Saux, Bertrand and Beaupère, Anne and Boulch, Alexandre and Chan-Hon-Tong, Adrien and Herbin, Stéphane and Randrianarivo, Hicham and Ferecatu, Marin},
	month = jul,
	year = {2015},
	keywords = {neural nets, support vector machines, geophysical image processing, terrain mapping, deep convolutional networks, deep neural networks, expert classifiers, generic-purpose image sets, high-level features, image domain, learning explicit features, multisensor features, multisource earth-observation data benchmarking classification, semantic labeling, spectral support-vector classification, Buildings, Laser radar, Neural networks, Pattern analysis, Semantics, feature extraction, image classification, remote sensing},
	pages = {4173--4176},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WXE5EVSM/7326745.html:text/html}
}

@inproceedings{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	doi = {10.1109/ICCV.2015.123},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1\%, [26]) on this dataset.},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	keywords = {Adaptation models, Biological neural networks, Computational modeling, Gaussian distribution, human-level performance, ILSVRC 2014 winner, image classification, ImageNet 2012 classification dataset, ImageNet classification, model fitting, network architectures, neural nets, overfitting risk, parametric rectified linear unit, PReLU, rectified activation units, rectifier neural networks, rectifier nonlinearities, robust initialization method, state-of-the-art neural networks, Testing, Training},
	pages = {1026--1034},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/NXDQWID9/7410480.html:text/html}
}

@article{badrinarayanan_segnet:_2017,
	title = {{SegNet}: {A} {Deep} {Convolutional} {Encoder}-{Decoder} {Architecture} for {Scene} {Segmentation}},
	volume = {39},
	issn = {0162-8828},
	shorttitle = {{SegNet}},
	doi = {10.1109/TPAMI.2016.2644615},
	abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.a- .uk/projects/segnet/.},
	number = {12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
	month = dec,
	year = {2017},
	keywords = {Neural networks, Computer architecture, Semantics, Training, image segmentation, Decoding, Roads, Decoder, Deep Convolutional Neural Networks, Encoder, Indoor Scenes, Pooling, Road Scenes, Semantic Pixel-Wise Segmentation, Upsampling},
	pages = {2481--2495},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9SZV73FK/7803544.html:text/html}
}

@article{fauvel_advances_2013,
	title = {Advances in {Spectral}-{Spatial} {Classification} of {Hyperspectral} {Images}},
	volume = {101},
	issn = {0018-9219},
	doi = {10.1109/JPROC.2012.2197589},
	abstract = {Recent advances in spectral-spatial classification of hyperspectral images are presented in this paper. Several techniques are investigated for combining both spatial and spectral information. Spatial information is extracted at the object (set of pixels) level rather than at the conventional pixel level. Mathematical morphology is first used to derive the morphological profile of the image, which includes characteristics about the size, orientation, and contrast of the spatial structures present in the image. Then, the morphological neighborhood is defined and used to derive additional features for classification. Classification is performed with support vector machines (SVMs) using the available spectral information and the extracted spatial information. Spatial postprocessing is next investigated to build more homogeneous and spatially consistent thematic maps. To that end, three presegmentation techniques are applied to define regions that are used to regularize the preliminary pixel-wise thematic map. Finally, a multiple-classifier (MC) system is defined to produce relevant markers that are exploited to segment the hyperspectral image with the minimum spanning forest algorithm. Experimental results conducted on three real hyperspectral images with different spatial and spectral resolutions and corresponding to various contexts are presented. They highlight the importance of spectral-spatial strategies for the accurate classification of hyperspectral images and validate the proposed methods.},
	number = {3},
	journal = {Proceedings of the IEEE},
	author = {Fauvel, M. and Tarabalka, Y. and Benediktsson, J. A. and Chanussot, J. and Tilton, J. C.},
	month = mar,
	year = {2013},
	keywords = {classification, Classification algorithms, conventional pixel level, feature extraction, geophysical image processing, homogeneous thematic maps, hyperspectral image, hyperspectral image segment, hyperspectral imaging, image classification, image morphological profile, Image resolution, image segmentation, Kernel, kernel methods, Mathematical morphology, morphological neighborhood, multiple-classifier system, Nearest neighbor searches, object level, pixel-wise thematic map, presegmentation techniques, remote sensing, segmentation, spanning forest algorithm, spatial information, spatial postprocessing, Spatial resolution, spatial structure contrast, spatial structure orientation, spatial structure size, spatially consistent thematic maps, Spectral analysis, spectral information, spectral-spatial classification, spectral–spatial classifier, support vector machines, vegetation mapping},
	pages = {652--675},
	file = {Fauvel et al_2013_Advances in Spectral-Spatial Classification of Hyperspectral Images.pdf:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/8XPE5X8G/Fauvel et al_2013_Advances in Spectral-Spatial Classification of Hyperspectral Images.pdf:application/pdf;IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/D7SGGI9Z/6297992.html:text/html}
}

@article{nogueira_towards_2017,
	title = {Towards better exploiting convolutional neural networks for remote sensing scene classification},
	volume = {61},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320316301509},
	doi = {10.1016/j.patcog.2016.07.001},
	abstract = {We present an analysis of three possible strategies for exploiting the power of existing convolutional neural networks (ConvNets or CNNs) in different scenarios from the ones they were trained: full training, fine tuning, and using ConvNets as feature extractors. In many applications, especially including remote sensing, it is not feasible to fully design and train a new ConvNet, as this usually requires a considerable amount of labeled data and demands high computational costs. Therefore, it is important to understand how to better use existing ConvNets. We perform experiments with six popular ConvNets using three remote sensing datasets. We also compare ConvNets in each strategy with existing descriptors and with state-of-the-art baselines. Results point that fine tuning tends to be the best performing strategy. In fact, using the features from the fine-tuned ConvNet with linear SVM obtains the best results. We also achieved state-of-the-art results for the three datasets used.},
	urldate = {2017-07-18},
	journal = {Pattern Recognition},
	author = {Nogueira, Keiller and Penatti, Otávio A. B. and dos Santos, Jefersson A.},
	month = jan,
	year = {2017},
	keywords = {convolutional neural networks, deep learning, feature extraction, remote sensing, hyperspectral images, Fine-tune, Aerial scenes},
	pages = {539--556},
	file = {ScienceDirect Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/4ZZ76SHZ/S0031320316301509.html:text/html}
}

@inproceedings{long_fully_2015,
	title = {Fully convolutional networks for semantic segmentation},
	doi = {10.1109/CVPR.2015.7298965},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Long, J. and Shelhamer, E. and Darrell, T.},
	month = jun,
	year = {2015},
	keywords = {Pascal VOC, learning (artificial intelligence), Computer architecture, convolution, Adaptation models, Semantics, Training, image classification, image segmentation, inference mechanisms, NYUDv2, SIFT flow, contemporary classification networks, fully convolutional networks, inference, learning, pixels-to-pixels, semantic segmentation, visual models, Deconvolution},
	pages = {3431--3440},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/BGDX4IA8/7298965.html:text/html}
}

@inproceedings{maggiori_can_2017,
	title = {Can {Semantic} {Labeling} {Methods} {Generalize} to {Any} {City}? {The} {Inria} {Aerial} {Image} {Labeling} {Benchmark}},
	shorttitle = {Can {Semantic} {Labeling} {Methods} {Generalize} to {Any} {City}?},
	url = {https://hal.inria.fr/hal-01468452/document},
	abstract = {New challenges in remote sensing impose the necessity of designing pixel classification methods that, once trained on a certain dataset, generalize to other areas of the earth. This may include regions where the appearance of the same type of objects is significantly different. In the literature it is common to use a single image and split it into training and test sets to train a classifier and assess its performance, respectively. However, this does not prove the generalization capabilities to other inputs. In this paper, we propose an aerial image labeling dataset that covers a wide range of urban settlement appearances, from different geographic locations. Moreover, the cities included in the test set are different from those of the training set. We also experiment with convolutional neural networks on our dataset.},
	language = {en},
	urldate = {2017-11-09},
	booktitle = {Proceedings of the {IEEE} {International} {Symposium} on {Geoscience} and {Remote} {Sensing} ({IGARSS})},
	author = {Maggiori, Emmanuel and Tarabalka, Yuliya and Charpiat, Guillaume and Alliez, Pierre},
	month = jul,
	year = {2017},
	file = {Maggiori et al_2017_Can Semantic Labeling Methods Generalize to Any City.pdf:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/UV49DI3R/Maggiori et al_2017_Can Semantic Labeling Methods Generalize to Any City.pdf:application/pdf;Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/K7CHRHV4/hal-01468452.html:text/html}
}

@inproceedings{deng_imagenet:_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, J. and Dong, W. and Socher, R. and Li, L. J. and Li, Kai and Fei-Fei, Li},
	month = jun,
	year = {2009},
	keywords = {computer vision, Explosions, Image databases, Image resolution, image retrieval, ImageNet database, Information retrieval, Internet, large-scale hierarchical image database, large-scale ontology, Large-scale systems, multimedia computing, multimedia data, Multimedia databases, Ontologies, ontologies (artificial intelligence), Robustness, Spine, subtree, trees (mathematics), very large databases, visual databases, wordNet structure},
	pages = {248--255},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/FFXN3NBD/5206848.html:text/html}
}

@article{l._c._chen_deeplab:_2018,
	title = {{DeepLab}: {Semantic} {Image} {Segmentation} with {Deep} {Convolutional} {Nets}, {Atrous} {Convolution}, and {Fully} {Connected} {CRFs}},
	volume = {40},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2017.2699184},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {{L. C. Chen} and {G. Papandreou} and {I. Kokkinos} and {K. Murphy} and {A. L. Yuille}},
	month = apr,
	year = {2018},
	keywords = {Context, Neural networks, Image resolution, Computational modeling, Semantics, Image segmentation, Atrous Convolution, Conditional Random Fields, Convolution, Convolutional Neural Networks, Semantic Segmentation},
	pages = {834--848}
}

@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	language = {en},
	number = {1},
	urldate = {2018-04-05},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = oct,
	year = {2001},
	pages = {5--32},
	file = {Full Text PDF:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/DNCGLG9M/Breiman - 2001 - Random Forests.pdf:application/pdf;Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/EVMJYKUX/A1010933404324.html:text/html}
}

@article{friedman_greedy_2001,
	title = {Greedy function approximation: {A} gradient boosting machine.},
	volume = {29},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Greedy function approximation},
	url = {https://projecteuclid.org/euclid.aos/1013203451},
	doi = {10.1214/aos/1013203451},
	abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent “boosting” paradigm is developed for additive expansions based on any fitting criterion.Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such “TreeBoost” models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
	language = {en},
	number = {5},
	urldate = {2018-04-05},
	journal = {The Annals of Statistics},
	author = {Friedman, Jerome H.},
	month = oct,
	year = {2001},
	mrnumber = {MR1873328},
	zmnumber = {1043.62034},
	keywords = {boosting, decision trees, Function estimation, robust nonparametric regression},
	pages = {1189--1232},
	file = {Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/R7ZESN7H/1013203451.html:text/html}
}

@inproceedings{zhang_solving_2004,
	title = {Solving {Large} {Scale} {Linear} {Prediction} {Problems} {Using} {Stochastic} {Gradient} {Descent} {Algorithms}},
	abstract = {Linear prediction methods, such as least  squares for regression, logistic regression and  support vector machines for classi  cation,  have been extensively used in statistics and  machine learning. In this paper, we study  stochastic gradient descent (SGD) algorithms  on regularized forms of linear prediction  methods. This class of methods, related  to online algorithms such as perceptron, are  both ecient and very simple to implement.},
	booktitle = {Icml 2004: {Proceedings} of the {Twenty}-{First} {International} {Conference} on {Machine} {Learning}. {Omnipress}},
	author = {Zhang, Tong},
	year = {2004},
	pages = {919--926},
	file = {Citeseer - Full Text PDF:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/PF8XPVMQ/Zhang - 2004 - Solving Large Scale Linear Prediction Problems Usi.pdf:application/pdf;Citeseer - Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/JA4E2CJP/summary.html:text/html}
}

@inproceedings{bottou_large-scale_2010,
	title = {Large-scale machine learning with stochastic gradient descent},
	abstract = {Abstract. During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.},
	booktitle = {in {COMPSTAT}},
	author = {Bottou, Léon},
	year = {2010},
	file = {Citeseer - Full Text PDF:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/KUDZJ6EU/Bottou - 2010 - Large-scale machine learning with stochastic gradi.pdf:application/pdf;Citeseer - Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/IIFEA7IM/summary.html:text/html}
}

@article{cortes_support-vector_1995,
	title = {Support-vector networks},
	volume = {20},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1007/BF00994018},
	doi = {10.1007/BF00994018},
	abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
	language = {en},
	number = {3},
	urldate = {2018-04-05},
	journal = {Machine Learning},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	month = sep,
	year = {1995},
	pages = {273--297},
	file = {Full Text PDF:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/DI8NHFDJ/Cortes et Vapnik - 1995 - Support-vector networks.pdf:application/pdf;Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/MPIR7B89/10.html:text/html}
}

@inproceedings{boser_training_1992,
	address = {New York, NY, USA},
	series = {{COLT} '92},
	title = {A {Training} {Algorithm} for {Optimal} {Margin} {Classifiers}},
	isbn = {978-0-89791-497-0},
	url = {http://doi.acm.org/10.1145/130385.130401},
	doi = {10.1145/130385.130401},
	abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
	urldate = {2018-04-05},
	booktitle = {Proceedings of the {Fifth} {Annual} {Workshop} on {Computational} {Learning} {Theory}},
	publisher = {ACM},
	author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
	year = {1992},
	pages = {144--152},
	file = {ACM Full Text PDF:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VJXG6SYQ/Boser et al. - 1992 - A Training Algorithm for Optimal Margin Classifier.pdf:application/pdf}
}

@inproceedings{chan_active_1999,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {An {Active} {Contour} {Model} without {Edges}},
	isbn = {978-3-540-66498-7 978-3-540-48236-9},
	url = {https://link.springer.com/chapter/10.1007/3-540-48236-9_13},
	doi = {10.1007/3-540-48236-9_13},
	abstract = {In this paper, we propose a new model for active contours to detect objects in a given image, based on techniques of curve evolution, Mumford-Shah functional for segmentation and level sets. Our model can detect objects whose boundaries are not necessarily defined by gra- dient. The model is a combination between more classical active contour models using mean curvature motion techniques, and the Mumford-Shah model for segmentation. We minimize an energy which can be seen as a particular case of the so-called minimal partition problem. In the level set formulation, the problem becomes a “mean-curvature flow”-like evolving the active contour, which will stop on the desired boundary. However, the stopping term does not depend on the gradient of the image, as in the classical active contour models, but is instead related to a particular segmentation of the image. Finally, we will present various experimental results and in particular some examples for which the classical snakes methods based on the gradient are not applicable.},
	language = {en},
	urldate = {2018-04-06},
	booktitle = {Scale-{Space} {Theories} in {Computer} {Vision}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Chan, Tony and Vese, Luminita},
	month = sep,
	year = {1999},
	pages = {141--151},
	file = {Full Text PDF:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9TELTTIK/Chan et Vese - 1999 - An Active Contour Model without Edges.pdf:application/pdf;Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/2P3BHT3I/3-540-48236-9_13.html:text/html}
}

@article{grady_random_2006,
	title = {Random walks for image segmentation},
	volume = {28},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Grady, Leo},
	year = {2006},
	pages = {1768--1783},
	file = {01704833.pdf:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/P5WG7JB9/01704833.pdf:application/pdf}
}

@article{giraud_robust_2018,
	title = {Robust superpixels using color and contour features along linear path},
	issn = {1077-3142},
	url = {http://www.sciencedirect.com/science/article/pii/S1077314218300067},
	doi = {10.1016/j.cviu.2018.01.006},
	abstract = {Superpixel decomposition methods are widely used in computer vision and image processing applications. By grouping homogeneous pixels, the accuracy can be increased and the decrease of the number of elements to process can drastically reduce the computational burden. For most superpixel methods, a trade-off is computed between 1) color homogeneity, 2) adherence to the image contours and 3) shape regularity of the decomposition. In this paper, we propose a framework that jointly enforces all these aspects and provides accurate and regular Superpixels with Contour Adherence using Linear Path (SCALP). During the decomposition, we propose to consider color features along the linear path between the pixel and the corresponding superpixel barycenter. A contour prior is also used to prevent the crossing of image boundaries when associating a pixel to a superpixel. Finally, in order to improve the decomposition accuracy and the robustness to noise, we propose to integrate the pixel neighborhood information, while preserving the same computational complexity. SCALP is extensively evaluated on standard segmentation dataset, and the obtained results outperform the ones of the state-of-the-art methods. SCALP is also extended for supervoxel decomposition on MRI images.},
	urldate = {2018-04-06},
	journal = {Computer Vision and Image Understanding},
	author = {Giraud, Rémi and Ta, Vinh-Thong and Papadakis, Nicolas},
	month = jan,
	year = {2018},
	keywords = {Contour detection, Linear path, Segmentation, Superpixels},
	file = {ScienceDirect Full Text PDF:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/N4R5ULZV/Giraud et al. - 2018 - Robust superpixels using color and contour feature.pdf:application/pdf;ScienceDirect Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/V24V8MCU/S1077314218300067.html:text/html}
}

@book{breiman_classification_2017,
	title = {Classification and regression trees},
	publisher = {Routledge},
	author = {Breiman, Leo},
	year = {2017},
	file = {Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9MV6GN35/9781351460491.html:text/html}
}

@article{beucher_morphological_1993,
	title = {The morphological approach to segmentation: the watershed transformation. {Mathematical} morphology in image processing.},
	volume = {34},
	shorttitle = {The morphological approach to segmentation},
	journal = {Optical Engineering},
	author = {Beucher, S and Meyer, F},
	year = {1993},
	pages = {433--481}
}

@inproceedings{dalal_histograms_2005,
	title = {Histograms of oriented gradients for human detection},
	volume = {1},
	doi = {10.1109/CVPR.2005.177},
	abstract = {We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	author = {Dalal, N. and Triggs, B.},
	month = jun,
	year = {2005},
	keywords = {Image edge detection, object detection, support vector machines, object recognition, Histograms, feature extraction, Robustness, Testing, Image databases, coarse spatial binning, contrast normalization, edge based descriptors, fine orientation binning, fine-scale gradients, gradient based descriptors, gradient methods, High performance computing, histograms of oriented gradients, human detection, Humans, linear SVM, Object detection, Object recognition, overlapping descriptor, pedestrian database, robust visual object recognition, Support vector machines},
	pages = {886--893 vol. 1},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/5DZC6F52/9411012.html:text/html}
}

@inproceedings{lowe_object_1999,
	title = {Object recognition from local scale-invariant features},
	volume = {2},
	doi = {10.1109/ICCV.1999.790410},
	abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
	booktitle = {Proceedings of the {Seventh} {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Lowe, D. G.},
	year = {1999},
	keywords = {Image recognition, object recognition, Computer science, feature extraction, Layout, computational geometry, Object recognition, 3D projection, blurred image gradients, candidate object matches, cluttered partially occluded images, computation time, Electrical capacitance tomography, Filters, image matching, inferior temporal cortex, least squares approximations, Lighting, local geometric deformations, local image features, local scale-invariant features, low residual least squares solution, multiple orientation planes, nearest neighbor indexing method, Neurons, primate vision, Programmable logic arrays, Reactive power, robust object recognition, staged filtering approach, unknown model parameters},
	pages = {1150--1157 vol.2},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/9MPEA4TN/790410.html:text/html}
}

@inproceedings{nogueira_learning_2016,
	title = {Learning to semantically segment high-resolution remote sensing images},
	doi = {10.1109/ICPR.2016.7900187},
	abstract = {Land cover classification is a task that requires methods capable of learning high-level features while dealing with high volume of data. Overcoming these challenges, Convolutional Networks (ConvNets) can learn specific and adaptable features depending on the data while, at the same time, learn classifiers. In this work, we propose a novel technique to automatically perform pixel-wise land cover classification. To the best of our knowledge, there is no other work in the literature that perform pixel-wise semantic segmentation based on data-driven feature descriptors for high-resolution remote sensing images. The main idea is to exploit the power of ConvNet feature representations to learn how to semantically segment remote sensing images. First, our method learns each label in a pixel-wise manner by taking into account the spatial context of each pixel. In a predicting phase, the probability of a pixel belonging to a class is also estimated according to its spatial context and the learned patterns. We conducted a systematic evaluation of the proposed algorithm using two remote sensing datasets with very distinct properties. Our results show that the proposed algorithm provides improvements when compared to traditional and state-of-the-art methods that ranges from 5 to 15\% in terms of accuracy.},
	booktitle = {2016 23rd {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Nogueira, K. and Mura, M. Dalla and Chanussot, J. and Schwartz, W. R. and Santos, J. A. dos},
	month = dec,
	year = {2016},
	keywords = {classifier learning, Context, ConvNet feature representation, convolution, convolutional network, Deep Learning, feature descriptor, feature extraction, Feature extraction, Feature Learning, geophysical image processing, High-resolution Images, high-resolution remote sensing image, image classification, image representation, image resolution, image segmentation, Image segmentation, land cover, land cover classification, Land-cover Mapping, learning (artificial intelligence), Machine learning, neural nets, Pixel-wise Classification, pixel-wise semantic segmentation, remote sensing, Remote sensing, Remote Sensing, Semantic Segmentation, Semantics, Visualization},
	pages = {3566--3571},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/GNPR43FA/7900187.html:text/html}
}

@inproceedings{volpi_semantic_2015,
	title = {Semantic segmentation of urban scenes by learning local class interactions},
	doi = {10.1109/CVPRW.2015.7301377},
	abstract = {Traditionally, land-cover mapping from remote sensing images is performed by classifying each atomic region in the image in isolation and by enforcing simple smoothing priors via random fields models as two independent steps. In this paper, we propose to model the segmentation problem by a discriminatively trained Conditional Random Field (CRF). To this end, we employ Structured Support Vector Machines (SSVM) to learn the weights of an informative set of appearance descriptors jointly with local class interactions. We propose a principled strategy to learn pairwise potentials encoding local class preferences from sparsely annotated ground truth. We show that this approach outperform standard baselines and more expressive CRF models, improving by 4-6 points the average class accuracy on a challenging dataset involving urban high resolution satellite imagery.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Volpi, M. and Ferrari, V.},
	month = jun,
	year = {2015},
	keywords = {CRF models, geophysical image processing, image segmentation, Image segmentation, Labeling, land cover, land-cover mapping, local class interactions, Materials requirements planning, random fields models, remote sensing, Remote sensing, remote sensing images, Satellites, semantic segmentation problem, Semantics, SSVM, Standards, statistical analysis, structured support vector machines, support vector machines, urban high resolution satellite imagery},
	pages = {1--9},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/BH5LTYDL/7301377.html:text/html}
}

@article{marmanis_deep_2016,
	title = {Deep {Learning} {Earth} {Observation} {Classification} {Using} {ImageNet} {Pretrained} {Networks}},
	volume = {13},
	issn = {1545-598X},
	doi = {10.1109/LGRS.2015.2499239},
	abstract = {Deep learning methods such as convolutional neural networks (CNNs) can deliver highly accurate classification results when provided with large enough data sets and respective labels. However, using CNNs along with limited labeled data can be problematic, as this leads to extensive overfitting. In this letter, we propose a novel method by considering a pretrained CNN designed for tackling an entirely different classification problem, namely, the ImageNet challenge, and exploit it to extract an initial set of representations. The derived representations are then transferred into a supervised CNN classifier, along with their class labels, effectively training the system. Through this two-stage framework, we successfully deal with the limited-data problem in an end-to-end processing scheme. Comparative results over the UC Merced Land Use benchmark prove that our method significantly outperforms the previously best stated results, improving the overall accuracy from 83.1\% up to 92.4\%. Apart from statistical improvements, our method introduces a novel feature fusion algorithm that effectively tackles the large data dimensionality by using a simple and computationally efficient approach.},
	number = {1},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Marmanis, D. and Datcu, M. and Esch, T. and Stilla, U.},
	month = jan,
	year = {2016},
	keywords = {Adaptation models, Arrays, convolutional neural networks, Convolutional neural networks (CNNs), Data models, deep learning (DL), deep learning earth observation classification, deep learning method, end-to-end processing scheme, feature extraction, Feature extraction, fusion algorithm, geophysical techniques, ImageNet pretrained networks, land-use classification, limited labeled data, limited-data problem, neural nets, Neural networks, pretrained network, Remote sensing, remote sensing (RS), supervised CNN classifier, Training, UC Merced Land Use benchmark},
	pages = {105--109},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/T3TS89A6/7342907.html:text/html}
}

@inproceedings{vargas_superpixel-based_2014,
	title = {Superpixel-{Based} {Interactive} {Classification} of {Very} {High} {Resolution} {Images}},
	doi = {10.1109/SIBGRAPI.2014.49},
	abstract = {Very high resolution (VHR) images are large datasets for pixel annotation – a process that has depended on the supervised training of an effective pixel classifier. Active learning techniques have mitigated this problem, but pixel descriptors are limited to local image information and the large number of pixels makes the response time to the user's actions impractical, during active learning. To circumvent the problem, we present an active learning strategy that relies on superpixel descriptors and a priori dataset reduction. Firstly, we compare VHR image annotation using superpixel- and pixel-based classifiers, as designed by the same state-of-the-art active learning technique – Multi-Class Level Uncertainty (MCLU). Even with the dataset reduction provided by the superpixel representation, MCLU remains unfeasible for user interaction. Therefore, we propose a technique to considerably reduce the superpixel dataset for active learning. Moreover, we subdivide the reduced dataset into a list of subsets with random sample rearrangement to gain both speed and sample diversity during the active learning process.},
	booktitle = {2014 27th {SIBGRAPI} {Conference} on {Graphics}, {Patterns} and {Images}},
	author = {Vargas, J. E. and Saito, P. T. M. and Falcão, A. X. and Rezende, P. J. d and Santos, J. A. d},
	month = aug,
	year = {2014},
	keywords = {a priori dataset reduction, Accuracy, active learning, active learning techniques, Clustering algorithms, dataset reduction, Histograms, image classification, Image color analysis, image representation, image resolution, interactive systems, learning (artificial intelligence), local image information, MCLU, multiclass level uncertainty, pixel annotation, pixel classifier, superpixel dataset, superpixel descriptors, superpixel representation, superpixel-based interactive classification, supervised classification, supervised training, Support vector machines, Training, Uncertainty, user interaction, Very high resolution image processing, very high resolution images, VHR image annotation},
	pages = {173--179},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HKQN4H75/6915305.html:text/html}
}

@article{mountrakis_support_2011,
	title = {Support vector machines in remote sensing: {A} review},
	volume = {66},
	issn = {0924-2716},
	shorttitle = {Support vector machines in remote sensing},
	url = {http://www.sciencedirect.com/science/article/pii/S0924271610001140},
	doi = {10.1016/j.isprsjprs.2010.11.001},
	abstract = {A wide range of methods for analysis of airborne- and satellite-derived imagery continues to be proposed and assessed. In this paper, we review remote sensing implementations of support vector machines (SVMs), a promising machine learning methodology. This review is timely due to the exponentially increasing number of works published in recent years. SVMs are particularly appealing in the remote sensing field due to their ability to generalize well even with limited training samples, a common limitation for remote sensing applications. However, they also suffer from parameter assignment issues that can significantly affect obtained results. A summary of empirical results is provided for various applications of over one hundred published works (as of April, 2010). It is our hope that this survey will provide guidelines for future applications of SVMs and possible areas of algorithm enhancement.},
	number = {3},
	urldate = {2018-04-12},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Mountrakis, Giorgos and Im, Jungho and Ogole, Caesar},
	month = may,
	year = {2011},
	keywords = {Remote sensing, Review, Support vector machines, SVM, SVMs},
	pages = {247--259},
	file = {ScienceDirect Full Text PDF:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/CAKTDBGN/Mountrakis et al. - 2011 - Support vector machines in remote sensing A revie.pdf:application/pdf;ScienceDirect Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/AA6GW96W/S0924271610001140.html:text/html}
}

@incollection{bengio_practical_2012,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Practical {Recommendations} for {Gradient}-{Based} {Training} of {Deep} {Architectures}},
	isbn = {978-3-642-35288-1 978-3-642-35289-8},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-35289-8_26},
	abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyperparameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
	language = {en},
	urldate = {2018-04-12},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Bengio, Yoshua},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8_26},
	pages = {437--478},
	file = {Full Text PDF:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/MCFQUWFJ/Bengio - 2012 - Practical Recommendations for Gradient-Based Train.pdf:application/pdf;Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/E38VEJWM/978-3-642-35289-8_26.html:text/html}
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	note = {{\textbackslash}url\{http://www.deeplearningbook.org\}}
}

@inproceedings{merciol_efficient_2017,
	address = {Toulouse, France},
	title = {Efficient and large-scale land cover classification using multiscale image analysis},
	url = {https://hal.archives-ouvertes.fr/hal-01672868},
	urldate = {2018-04-12},
	booktitle = {Big {Data} from {Space}},
	author = {Merciol, François and Balem, Thibaud and Lefèvre, Sébastien},
	year = {2017},
	file = {HAL Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/IN9LFRCX/hal-01672868.html:text/html}
}

@article{levinshtein_turbopixels:_2009,
	title = {{TurboPixels}: {Fast} {Superpixels} {Using} {Geometric} {Flows}},
	volume = {31},
	issn = {0162-8828},
	shorttitle = {{TurboPixels}},
	doi = {10.1109/TPAMI.2009.96},
	abstract = {We describe a geometric-flow-based algorithm for computing a dense oversegmentation of an image, often referred to as superpixels. It produces segments that, on one hand, respect local image boundaries, while, on the other hand, limiting undersegmentation through a compactness constraint. It is very fast, with complexity that is approximately linear in image size, and can be applied to megapixel sized images with high superpixel densities in a matter of minutes. We show qualitative demonstrations of high-quality results on several complex images. The Berkeley database is used to quantitatively compare its performance to a number of oversegmentation algorithms, showing that it yields less undersegmentation than algorithms that lack a compactness constraint while offering a significant speedup over N-cuts, which does enforce compactness.},
	number = {12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Levinshtein, A. and Stere, A. and Kutulakos, K. N. and Fleet, D. J. and Dickinson, S. J. and Siddiqi, K.},
	month = dec,
	year = {2009},
	keywords = {Berkeley database, computational complexity, dense oversegmentation, fast superpixels, geometric flows, geometry, image labeling, image resolution, image segmentation, N-cuts, perceptual grouping, perceptual grouping., superpixels, Superpixels, TurboPixels},
	pages = {2290--2297},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HVNQPWA5/4912213.html:text/html}
}

@article{shi_normalized_2000,
	title = {Normalized {Cuts} and {Image} {Segmentation}},
	volume = {22},
	issn = {0162-8828},
	url = {http://dx.doi.org/10.1109/34.868688},
	doi = {10.1109/34.868688},
	abstract = {We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.},
	number = {8},
	urldate = {2018-04-12},
	journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	author = {Shi, Jianbo and Malik, Jitendra},
	month = aug,
	year = {2000},
	keywords = {graph partitioning., Grouping, image segmentation},
	pages = {888--905}
}

@inproceedings{liu_entropy_2011,
	title = {Entropy rate superpixel segmentation},
	doi = {10.1109/CVPR.2011.5995323},
	abstract = {We propose a new objective function for superpixel segmentation. This objective function consists of two components: entropy rate of a random walk on a graph and a balancing term. The entropy rate favors formation of compact and homogeneous clusters, while the balancing function encourages clusters with similar sizes. We present a novel graph construction for images and show that this construction induces a matroid - a combinatorial structure that generalizes the concept of linear independence in vector spaces. The segmentation is then given by the graph topology that maximizes the objective function under the matroid constraint. By exploiting submodular and mono-tonic properties of the objective function, we develop an efficient greedy algorithm. Furthermore, we prove an approximation bound of ½ for the optimality of the solution. Extensive experiments on the Berkeley segmentation benchmark show that the proposed algorithm outperforms the state of the art in all the standard evaluation metrics.},
	booktitle = {{CVPR} 2011},
	author = {Liu, M. Y. and Tuzel, O. and Ramalingam, S. and Chellappa, R.},
	month = jun,
	year = {2011},
	keywords = {balancing function, Berkeley segmentation benchmark, Complexity theory, entropy, Entropy, entropy rate, graph construction, graph theory, graph topology, greedy algorithm, greedy algorithms, Greedy algorithms, homogeneous clusters, Image edge detection, image segmentation, Image segmentation, matrix algebra, matroid constraint, Measurement, pattern clustering, Random variables, standard evaluation metrics, superpixel segmentation, vector spaces},
	pages = {2097--2104},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/HW9F4V9P/5995323.html:text/html}
}

@article{comaniciu_mean_2002,
	title = {Mean shift: a robust approach toward feature space analysis},
	volume = {24},
	issn = {0162-8828},
	shorttitle = {Mean shift},
	doi = {10.1109/34.1000236},
	abstract = {A general non-parametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure: the mean shift. For discrete data, we prove the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. The relation of the mean shift procedure to the Nadaraya-Watson estimator from kernel regression and the robust M-estimators; of location is also established. Algorithms for two low-level vision tasks discontinuity-preserving smoothing and image segmentation - are described as applications. In these algorithms, the only user-set parameter is the resolution of the analysis, and either gray-level or color images are accepted as input. Extensive experimental results illustrate their excellent performance},
	number = {5},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Comaniciu, D. and Meer, P.},
	month = may,
	year = {2002},
	keywords = {algorithm performance, analysis resolution, arbitrarily shaped cluster delineation, color images, complex multimodal feature space, computational module, computer vision, convergence, Convergence, density function, Density functional theory, density modes detection, discontinuity-preserving image smoothing, discrete data, estimation theory, gray-level images, Image analysis, Image color analysis, Image resolution, image segmentation, Image segmentation, Kernel, kernel regression, location estimation, low-level vision algorithms, mean shift, Nadaraya-Watson estimator, nearest stationary point, nonparametric statistics, nonparametric technique, pattern clustering, Pattern recognition, pattern recognition procedure, recursive mean shift procedure, robust feature space analysis, robust M-estimators, Robustness, smoothing methods, Smoothing methods, user-set parameter},
	pages = {603--619},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/B4S7MHYA/1000236.html:text/html}
}

@article{marquez-neila_morphological_2014,
	title = {A {Morphological} {Approach} to {Curvature}-{Based} {Evolution} of {Curves} and {Surfaces}},
	volume = {36},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2013.106},
	abstract = {We introduce new results connecting differential and morphological operators that provide a formal and theoretically grounded approach for stable and fast contour evolution. Contour evolution algorithms have been extensively used for boundary detection and tracking in computer vision. The standard solution based on partial differential equations and level-sets requires the use of numerical methods of integration that are costly computationally and may have stability issues. We present a morphological approach to contour evolution based on a new curvature morphological operator valid for surfaces of any dimension. We approximate the numerical solution of the curve evolution PDE by the successive application of a set of morphological operators defined on a binary level-set and with equivalent infinitesimal behavior. These operators are very fast, do not suffer numerical stability issues, and do not degrade the level set function, so there is no need to reinitialize it. Moreover, their implementation is much easier since they do not require the use of sophisticated numerical algorithms. We validate the approach providing a morphological implementation of the geodesic active contours, the active contours without borders, and turbopixels. In the experiments conducted, the morphological implementations converge to solutions equivalent to those achieved by traditional numerical solutions, but with significant gains in simplicity, speed, and stability.},
	number = {1},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Márquez-Neila, P. and Baumela, L. and Alvarez, L.},
	month = jan,
	year = {2014},
	keywords = {Active contours, Approximation methods, boundary detection, computer vision, Computer vision, curvature based evolution, curvature morphological operator, curve evolution, fast contour evolution algorithms, geodesic active contours, Level set, level set function, level-sets, Mathematical model, mathematical morphology, morphological snakes, numerical analysis, numerical solutions, numerical stability, Numerical stability, partial differential equations, sophisticated numerical algorithms, stability, Surface morphology, Three-dimensional displays, tracking, turbopixels},
	pages = {2--17},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/TUT83J8I/6529072.html:text/html}
}

@article{bengio_representation_2013,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	volume = {35},
	issn = {0162-8828},
	shorttitle = {Representation {Learning}},
	doi = {10.1109/TPAMI.2013.50},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bengio, Y. and Courville, A. and Vincent, P.},
	month = aug,
	year = {2013},
	keywords = {Abstracts, AI, Algorithms, artificial intelligence, Artificial Intelligence, autoencoder, autoencoders, Boltzmann machine, data representation, data structures, Deep learning, density estimation, Feature extraction, feature learning, geometrical connections, Humans, Learning systems, Machine learning, machine learning algorithms, manifold learning, Manifolds, neural nets, Neural networks, Neural Networks (Computer), probabilistic models, probability, representation learning, Speech recognition, unsupervised feature learning, unsupervised learning},
	pages = {1798--1828},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/VVVC9RRN/6472238.html:text/html}
}

@article{achanta_slic_2012,
	title = {{SLIC} {Superpixels} {Compared} to {State}-of-the-{Art} {Superpixel} {Methods}},
	volume = {34},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2012.120},
	abstract = {Computer vision applications have come to rely increasingly on superpixels in recent years, but it is not always clear what constitutes a good superpixel algorithm. In an effort to understand the benefits and drawbacks of existing methods, we empirically compare five state-of-the-art superpixel algorithms for their ability to adhere to image boundaries, speed, memory efficiency, and their impact on segmentation performance. We then introduce a new superpixel algorithm, simple linear iterative clustering (SLIC), which adapts a k-means clustering approach to efficiently generate superpixels. Despite its simplicity, SLIC adheres to boundaries as well as or better than previous methods. At the same time, it is faster and more memory efficient, improves segmentation performance, and is straightforward to extend to supervoxel generation.},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Achanta, R. and Shaji, A. and Smith, K. and Lucchi, A. and Fua, P. and Süsstrunk, S.},
	month = nov,
	year = {2012},
	keywords = {Algorithms, Approximation algorithms, clustering, Clustering algorithms, Complexity theory, computer vision, image boundary, Image color analysis, Image edge detection, Image Enhancement, Image Interpretation, Computer-Assisted, image segmentation, Image segmentation, iterative methods, k-means, k-means clustering approach, Measurement uncertainty, memory efficiency, pattern clustering, Pattern Recognition, Automated, Reproducibility of Results, segmentation, segmentation performance, Sensitivity and Specificity, Signal Processing, Computer-Assisted, simple linear iterative clustering, SLIC superpixels, superpixel generation, Superpixels, supervoxel generation},
	pages = {2274--2282},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/2EGRB69G/6205760.html:text/html}
}

@article{dechesne_semantic_2017,
	title = {Semantic segmentation of forest stands of pure species combining airborne lidar data and very high resolution multispectral imagery},
	volume = {126},
	issn = {0924-2716},
	url = {http://www.sciencedirect.com/science/article/pii/S0924271616302763},
	doi = {10.1016/j.isprsjprs.2017.02.011},
	abstract = {Forest stands are the basic units for forest inventory and mapping. Stands are defined as large forested areas (e.g., ⩾2ha) of homogeneous tree species composition and age. Their accurate delineation is usually performed by human operators through visual analysis of very high resolution (VHR) infra-red images. This task is tedious, highly time consuming, and should be automated for scalability and efficient updating purposes. In this paper, a method based on the fusion of airborne lidar data and VHR multispectral images is proposed for the automatic delineation of forest stands containing one dominant species (purity superior to 75\%). This is the key preliminary task for forest land-cover database update. The multispectral images give information about the tree species whereas 3D lidar point clouds provide geometric information on the trees and allow their individual extraction. Multi-modal features are computed, both at pixel and object levels: the objects are individual trees extracted from lidar data. A supervised classification is then performed at the object level in order to coarsely discriminate the existing tree species in each area of interest. The classification results are further processed to obtain homogeneous areas with smooth borders by employing an energy minimum framework, where additional constraints are joined to form the energy function. The experimental results show that the proposed method provides very satisfactory results both in terms of stand labeling and delineation (overall accuracy ranges between 84\% and 99\%).},
	urldate = {2018-04-13},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Dechesne, Clément and Mallet, Clément and Le Bris, Arnaud and Gouet-Brunet, Valérie},
	month = apr,
	year = {2017},
	keywords = {Energy minimization, Feature selection, Forest stand delineation, Fusion, Lidar, Multispectral imagery, Regularisation, Supervised classification, Tree species},
	pages = {129--145},
	file = {ScienceDirect Full Text PDF:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/WPIRC3P4/Dechesne et al. - 2017 - Semantic segmentation of forest stands of pure spe.pdf:application/pdf;ScienceDirect Snapshot:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/E4W5V7GP/S0924271616302763.html:text/html}
}

@article{ham_investigation_2005,
	title = {Investigation of the random forest framework for classification of hyperspectral data},
	volume = {43},
	issn = {0196-2892},
	doi = {10.1109/TGRS.2004.842481},
	abstract = {Statistical classification of byperspectral data is challenging because the inputs are high in dimension and represent multiple classes that are sometimes quite mixed, while the amount and quality of ground truth in the form of labeled data is typically limited. The resulting classifiers are often unstable and have poor generalization. This work investigates two approaches based on the concept of random forests of classifiers implemented within a binary hierarchical multiclassifier system, with the goal of achieving improved generalization of the classifier in analysis of hyperspectral data, particularly when the quantity of training data is limited. A new classifier is proposed that incorporates bagging of training samples and adaptive random subspace feature selection within a binary hierarchical classifier (BHC), such that the number of features that is selected at each node of the tree is dependent on the quantity of associated training data. Results are compared to a random forest implementation based on the framework of classification and regression trees. For both methods, classification results obtained from experiments on data acquired by the National Aeronautics and Space Administration (NASA) Airborne Visible/Infrared Imaging Spectrometer instrument over the Kennedy Space Center, Florida, and by Hyperion on the NASA Earth Observing 1 satellite over the Okavango Delta of Botswana are superior to those from the original best basis BHC algorithm and a random subspace extension of the BHC.},
	number = {3},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Ham, J. and Chen, Yangchi and Crawford, M. M. and Ghosh, J.},
	month = mar,
	year = {2005},
	keywords = {adaptive random subspace feature selection, Airborne Visible/Infrared Imaging Spectrometer, Bagging, binary hierarchical classifier, binary hierarchical multiclassifier system, Botswana, classification tree, Classification tree analysis, data acquisition, Data analysis, feature extraction, Florida, forestry, geophysical signal processing, Hyperion, hyperspectral data analysis, hyperspectral data classification, Hyperspectral imaging, image classification, Infrared imaging, Infrared spectra, Kennedy Space Center, multidimensional signal processing, NASA, NASA Earth Observing 1 satellite, Okavango Delta, random forest, random processes, random subspace extension, regression tree, Regression tree analysis, Spectroscopy, statistical classification, Training data, tree node, trees (mathematics), USA, vegetation mapping},
	pages = {492--501},
	file = {IEEE Xplore Abstract Record:/home/naudeber/Nextcloud/Onera/Timeless/Bibliographie/.zotero/storage/YD633IEG/1396322.html:text/html}
}